{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9719753c-7293-4597-b0f5-426519cfadcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data \n",
    "import datetime\n",
    "import csv\n",
    "import copy\n",
    "import dateutil.parser\n",
    "import sys\n",
    "sys.path.insert(1,'../')\n",
    "import Tools.data_processing as dp\n",
    "# In the future we should use multiple files for annotating as some anomaly types conflict: \n",
    "# Single_point/Plummeting/Skyrocketing/TurbSpike in one file \n",
    "# Titled/3Peak/ NormalPeak in another file \n",
    "# Peak (reguardless of anomlous status) \n",
    "# spikes_fname = '../Data/data_annotating/non_processed_annotated/_.csv'\n",
    "# anomalous_peaks_fname = '../Data/data_annotating/non_processed_annotated/_.csv'\n",
    "# normal_peaks_fname = '../Data/data_annotating/non_processed_annotated/_.csv'\n",
    "\n",
    "fname1 = '/Users/zachfogg/Desktop/DB-SRRW/Data/manual_annotating_data/annotated_data/turb_pp/turb_PP_Labeled_0k-100k-2.csv'\n",
    "fname2 = '/Users/zachfogg/Desktop/DB-SRRW/Data/manual_annotating_data/annotated_data/turb_pp/turb_PP_Labeled_100k-200k-2.csv'\n",
    "fname3 = '/Users/zachfogg/Desktop/DB-SRRW/Data/manual_annotating_data/annotated_data/turb_pp/turb_PP_Labeled_200k-300k-2.csv'\n",
    "# fname1 = '/Users/zachfogg/Desktop/newly_lab_turb_PPP/turb_PPP_0k-100K.csv'\n",
    "# fname2 = '/Users/zachfogg/Desktop/newly_lab_turb_PPP/turb_PPP_100k-200K.csv'\n",
    "# fname3 = '/Users/zachfogg/Desktop/newly_lab_turb_PPP/turb_PPP_200k-300K.csv'\n",
    "# out_file_fname = './data_annotating/processed_annotated/timeseries_0-100000k-annoated.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fc69bea3-4d7b-43bb-9074-55c80bab1e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data in fDOM, stage, turb \n",
    "turb_data = []\n",
    "stage_data= []\n",
    "fDOM_data = []\n",
    "with open(fname1, newline='') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter = ',')\n",
    "    next(csv_reader,None) # skip headers \n",
    "    count = 0\n",
    "    for row in csv_reader: \n",
    "        if count%3 == 0:\n",
    "            fDOM_data.append([row[1],row[2],row[3]])\n",
    "        elif count%3 == 1:\n",
    "            stage_data.append([row[1],row[2],row[3]])\n",
    "        elif count%3 == 2:\n",
    "            turb_data.append([row[1],row[2],row[3]])\n",
    "        count +=1 \n",
    "    csv_file.close()\n",
    "with open(fname2, newline='') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter = ',')\n",
    "    next(csv_reader,None) # skip headers \n",
    "    count = 0\n",
    "    for row in csv_reader: \n",
    "        if count%3 == 0:\n",
    "            fDOM_data.append([row[1],row[2],row[3]])\n",
    "        elif count%3 == 1:\n",
    "            stage_data.append([row[1],row[2],row[3]])\n",
    "        elif count%3 == 2:\n",
    "            turb_data.append([row[1],row[2],row[3]])\n",
    "        count +=1 \n",
    "    csv_file.close()\n",
    "with open(fname3, newline='') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter = ',')\n",
    "    next(csv_reader,None) # skip headers \n",
    "    count = 0\n",
    "    for row in csv_reader: \n",
    "        if count%3 == 0:\n",
    "            fDOM_data.append([row[1],row[2],row[3]])\n",
    "        elif count%3 == 1:\n",
    "            stage_data.append([row[1],row[2],row[3]])\n",
    "        elif count%3 == 2:\n",
    "            turb_data.append([row[1],row[2],row[3]])\n",
    "        count +=1 \n",
    "    csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b21f4ca6-c3b6-4b0e-b198-2eab7edb4908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229620\n",
      "229620\n",
      "229620\n"
     ]
    }
   ],
   "source": [
    "# Check data read\n",
    "print(len(fDOM_data))\n",
    "print(len(turb_data))\n",
    "print(len(stage_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9b5c08cd-f758-4aa5-adf4-652035193161",
   "metadata": {},
   "outputs": [],
   "source": [
    "turb_data_PPP = copy.deepcopy(turb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7aa9d740-fa01-49e2-b1eb-fa73e540a802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'left_base', 'nt_pp', 'right_base', 't_pp', 'sky_dr_st', 't_opp', 'PPP']\n"
     ]
    }
   ],
   "source": [
    "# Determine lable names \n",
    "discovered = []\n",
    "for row in turb_data:\n",
    "    val = row[2]\n",
    "    if val not in discovered:\n",
    "        discovered.append(val)\n",
    "print(discovered)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9dc899d1-1a01-45b7-927b-e6d71f6b39db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229620 229620\n"
     ]
    }
   ],
   "source": [
    "# Merge turb PPP and PP data\n",
    "print(len(turb_data), len(turb_data_PPP))\n",
    "for i,row in enumerate(turb_data_PPP):\n",
    "    if row[2] == 'PPP':\n",
    "        if turb_data[i][2] == 'nt_pp' or turb_data[i][2] == 'sky_dr_st' or turb_data[i][2] == 'PPP':\n",
    "            turb_data[i][2] = 'PPP'\n",
    "        else:\n",
    "            print(turb_data[i],'   ', turb_data_PPP[i])\n",
    "            break\n",
    "    elif row[2] == 'NPP' and turb_data[i][2] != 'nt_pp' and turb_data[i][2] != 'sky_dr_st':\n",
    "        print(turb_data[i],'   ', turb_data_PPP[i])\n",
    "    elif row[2] == 'PP' and turb_data[i][2] != 't_pp' :\n",
    "        print(turb_data[i],'   ', turb_data_PPP[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f57e19f3-8187-428b-b614-25817d2403a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "turb_data = [[row[0],row[1],row[2],i] for i,row in enumerate(turb_data)]\n",
    "fDOM_data = [[row[0],row[1],row[2],i] for i,row in enumerate(fDOM_data)]\n",
    "# Filter for entries that have a label \n",
    "fDOM_data_filtered = list(filter(lambda row: row[2] != '', fDOM_data))\n",
    "turb_data_filtered = list(filter(lambda row: row[2] != '', turb_data))\n",
    "\n",
    "# Convert ISO timestamp format to python datetime obj and then string\n",
    "fDOM_data_converted = [[dateutil.parser.isoparse(row[0][:-5]),row[1],row[2],row[3]] for row in fDOM_data_filtered]\n",
    "turb_data_converted = [[dateutil.parser.isoparse(row[0][:-5]),row[1],row[2],row[3]] for row in turb_data_filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136f288d-c01c-45aa-b330-ee9ae3261a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "352cff87-584b-46c8-8d49-af502d1a25d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map to convert label names\n",
    "mapLabels = {'t_pp' : 'PP',\n",
    "             'nt_pp': 'NPP',\n",
    "             'sky_dr_st' :'NPP',\n",
    "             'NPP' : 'NPP',\n",
    "             'PP' :'PP',\n",
    "             'PPP' :'PPP'} #sky_dr_st should be marked as nt_pp...they can not be caught with current PP rules but will be caught by turb skyrocketing peak\n",
    "peakLabels = ['t_pp','nt_pp','sky_dr_st','PP','PPP','NPP']\n",
    "# Write out turb pp lables: [timestamp, indx, label]\n",
    "out_f = '/Users/zachfogg/Desktop/DB-SRRW/Data/manual_annotating_data/processed_data/turb/julian_time/turb_pp_0k-300k-2_labeled'\n",
    "# Filter to just pp and npp \n",
    "turb_data_pp = list(filter(lambda row: row[2] in peakLabels, turb_data_converted))\n",
    "\n",
    "with open(out_f, 'w', newline = '') as f: \n",
    "    writer = csv.writer(f, delimiter = ',')\n",
    "    writer.writerow(['timestamp_of_peak', 'value_of_peak','label_of_peak','idx_of_peak'])\n",
    "    for row in turb_data_pp:\n",
    "        writer.writerow([dp.datetime_to_julian(row[0]), row[1], mapLabels[row[2]], row[3]])\n",
    "    f.close()\n",
    "    \n",
    "# Write out turb pp lables: [timestamp, indx, label]\n",
    "out_f = '/Users/zachfogg/Desktop/DB-SRRW/Data/manual_annotating_data/processed_data/turb/datetime/turb_pp_0k-300k-2_labeled'\n",
    "# Filter to just pp and npp \n",
    "with open(out_f, 'w', newline = '') as f: \n",
    "    writer = csv.writer(f, delimiter = ',')\n",
    "    writer.writerow(['timestamp_of_peak', 'value_of_peak','label_of_peak','idx_of_peak'])\n",
    "    for row in turb_data_pp:\n",
    "        writer.writerow([row[0],row[1],mapLabels[row[2]],row[3]])\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2743d352-99f5-4fde-b8df-61de2d0dffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in turb_data_converted:\n",
    "    if x[2] == 'comeback':  \n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c9a725d5-c18a-4b0a-adf6-837491ddd49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through fDOM and add each each anomalous data point to its corresponding lists \n",
    "tilted_peaks = []\n",
    "single_point_peaks = []\n",
    "three_peak_violations = []\n",
    "skyrocketing_peaks = []\n",
    "plummeting_peaks = []\n",
    "normal_peaks = []\n",
    "fluctuations = []\n",
    "\n",
    "tilted_peak_label = 'question'\n",
    "single_point_peaks_label = 'single_point'\n",
    "plummeting_peak_label = 'plummeting'\n",
    "skyrocketing_label = 'skyrocketing'\n",
    "normal_peak_label = 'normal_peak'\n",
    "fluctuation_label = 'fluctuation'\n",
    "three_peak_label = 'three_peak_vio'\n",
    "\n",
    "for row in fDOM_data_converted:\n",
    "    if row[2] == tilted_peak_label:\n",
    "        tilted_peaks.append(row)\n",
    "    elif row[2] == single_point_peaks_label:\n",
    "        single_point_peaks.append(row)\n",
    "    elif row[2] == plummeting_peak_label:\n",
    "        plummeting_peaks.append(row)\n",
    "    elif row[2] == skyrocketing_label:\n",
    "        skyrocketing_peaks.append(row)\n",
    "    elif row[2] == normal_peak_label:\n",
    "        normal_peaks.append(row)\n",
    "    elif row[2] == fluctuation_label:\n",
    "        fluctuations.append(row)\n",
    "    elif row[2] == three_peak_label:\n",
    "        three_peak_violations.append(row)\n",
    "\n",
    "\n",
    "turb_spike_anomalies = turb_data # turb data only has one anomaly type currently \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5687db7c-d393-43e9-8f5e-d943330eb1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d47fe841-ebbd-49f4-90ad-e21b2eb5aa9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "28\n",
      "20\n",
      "18\n",
      "22\n",
      "16\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(\"Tilted Peaks: {}\".format(len(tilted_peaks))))\n",
    "print(len(\"Single Point Peaks Peaks: {}\".format(len(single_point_peaks))))\n",
    "print(len(\"Plummeting Peaks: {}\".format(len(plummeting_peaks))))\n",
    "print(len(\"Normal Peaks: {}\".format(len(normal_peaks))))\n",
    "print(len(\"Skyrocketing Peaks: {}\".format(len(skyrocketing_peaks))))\n",
    "print(len(\"Local Fluct: {}\".format(len(fluctuations))))\n",
    "print(len(\"Three Peak Vio: {}\".format(len(three_peak_violations))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "96e99f16-2844-42cf-a0bd-f5b39ac03c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the start and end of each separate anomaly; append each [start, end, type] tuple to new lists \n",
    "\n",
    "# This determines how much time we use as a buffer to label starts and ends of event\n",
    "delta15 = datetime.timedelta(minutes=15)\n",
    "delta30 = datetime.timedelta(minutes=30)\n",
    "\"\"\"\n",
    "Take in list of data consisting of data points labeled to be part of an anomaly and \n",
    "determine the start and end point of each anomaly \n",
    "\n",
    "param: data - form: [datetime,value,type]\n",
    "return: list of anomalies in form: [start_datetime, end_datetime, type]\n",
    "\"\"\"\n",
    "def delineate_anomalies(data):\n",
    "    print(data[0][2])\n",
    "    anomalies = []\n",
    "    if len(data) < 3:\n",
    "        return anomalies \n",
    "    starts = []\n",
    "    ends = []\n",
    "    # starts of anomalies will not have a point x minutes before \n",
    "    # ends will not have a point x minutes after \n",
    "    starts.append(data[0][0])\n",
    "    for i in range(2,len(data)): # start at two as an event is at minumum three points and we have already considered 0 a start \n",
    "        # Here we allow index errors\n",
    "        try: \n",
    "            # If the previous point is more than 15 minutes behind the current point then we can \n",
    "            # assume that a non anomalous point separates them and that this point is a start \n",
    "            # a start cannot occur in the last 2 entries as this would not allow for a complete anomaly \n",
    "            if i<=(len(data)-3) and data[i-1][0] + delta15 < data[i][0] and data[i-2][0]+delta30 < data[i][0]:\n",
    "                starts.append(data[i][0])\n",
    "            elif data[i+1][0] - delta15 > data[i][0] and data[i+2][0] - delta30 > data[i][0]:\n",
    "                ends.append(data[i][0])\n",
    "        except IndexError: \n",
    "            # print('Error: i = {}, out of {} rows'.format(i,len(data)))\n",
    "            pass\n",
    "    ends.append(data[len(data)-1][0]) # We will get an error when calculating an end point at the end of the list so we append it here\n",
    "    # Verify Starts : Each start should have at least 2 more consecutive points \n",
    "    for i in range(len(data)):\n",
    "        if data[i][0] in starts:\n",
    "            # A start should not occur in the last 2 indices of the data so this should not produce errors\n",
    "            if not ((data[i][0] == data[i+1][0] - delta15) or (data[i][0] == data[i+2][0] - delta30)):\n",
    "                print('Error: Starts are not correct and do not have two perfect consecutive points...')\n",
    "                print('Index: {}, Start: {}'.format(i, data[i][0]))\n",
    "    \n",
    "    # There should be an equal number of starts and ends\n",
    "    if len(starts) != len(ends):\n",
    "        print('Error: Starts = {}, Ends = {}\\n'.format(len(starts), len(ends)))\n",
    "        print('Starts:')\n",
    "        for val in starts:\n",
    "            print(val)\n",
    "        print('\\nEnds:')\n",
    "        for val in ends: \n",
    "            print(val)\n",
    "        return anomalies\n",
    "    # If there are an equal number of starts and ends then the i entries in starts and ends should be from the same anomaly\n",
    "    for i in range(len(starts)):\n",
    "        anomalies.append([starts[i],ends[i],data[0][2]])\n",
    "    return anomalies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d6f25299-6cc4-4c17-b61f-c6570c0a771f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question\n",
      "single_point\n",
      "three_peak_vio\n",
      "skyrocketing\n",
      "plummeting\n",
      "normal_peak\n",
      "fluctuation\n"
     ]
    }
   ],
   "source": [
    "# For each anomaly type list: determine start and ends of each separate anomaly occurence and append to new list: \n",
    "processed_tilted_peaks = delineate_anomalies(tilted_peaks)\n",
    "processed_single_point_peaks = delineate_anomalies(single_point_peaks)\n",
    "processed_three_peak_violations = delineate_anomalies(three_peak_violations)\n",
    "processed_skyrocketing_peaks = delineate_anomalies(skyrocketing_peaks)\n",
    "processed_plummeting_peaks = delineate_anomalies(plummeting_peaks)\n",
    "processed_normal_peaks = delineate_anomalies(normal_peaks)\n",
    "processed_fluctuations = delineate_anomalies(fluctuations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "73b75ab7-e859-464b-abb8-b164aca357f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function takes outfile name, data to write, and peak type label, and writes data out to csv file \n",
    "def write_data_to_csv(out_file, data, peak_type):\n",
    "    with open(out_file, 'w',newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile, delimiter = ',')\n",
    "        csvwriter.writerow(['start_time','end_time','peak_type'])\n",
    "        for row in data:\n",
    "            csvwriter.writerow([str(row[0]), str(row[1]), peak_type])\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0a6989f9-8ed6-4f33-88ea-e4ee4ee2bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out data to txt files: [start_datetime, end_datetime, anomaly_type]\n",
    "base_path = '../Data/data_annotating/processed_annotated/10.1.12-7.13.20/'\n",
    "write_data_to_csv(base_path + 'plummetting_peaks.csv', processed_plummeting_peaks, 'plummeting_peak')\n",
    "write_data_to_csv(base_path + 'tilted_peaks.csv',  processed_tilted_peaks, 'tilted_peaks')\n",
    "write_data_to_csv(base_path + 'single_point_peaks.csv', processed_single_point_peaks, 'single_point_peaks')\n",
    "write_data_to_csv(base_path + 'three_peak_violations.csv',processed_three_peak_violations,'three_peak_violation')\n",
    "write_data_to_csv(base_path + 'skyrocketing_peaks.csv', processed_skyrocketing_peaks, 'skyrocketing_peak')\n",
    "write_data_to_csv(base_path + 'normal_peaks.csv', processed_normal_peaks, 'normal_peak')\n",
    "write_data_to_csv(base_path + 'local_fluctuations.csv', processed_fluctuations, 'local_fluctuations')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3b00eaf-8d42-45c6-a8c0-3a499cb0b5a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_skyrocketing_peaks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
