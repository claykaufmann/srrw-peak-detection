{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Peak Detection in fDOM\n",
    "\n",
    "This file combines all of the fDOM detection scripts into a singular classifier, that detects all peak types. On top of this, it also leverages the augmented data created previously.\n",
    "\n",
    "## Structure\n",
    "\n",
    "The core structure of the project is to have all individual classifiers running, and then when one detects a peak, it alerts the overall classifier \"manager\", which then takes note of the peak that a classifier has detected as an anomaly peak.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import progressbar\n",
    "\n",
    "import Tools.data_processing as dp\n",
    "import Tools.data_movement as dm\n",
    "from Tools.get_all_cands import get_all_cands_fDOM, get_all_truths_fDOM\n",
    "\n",
    "# import classifiers\n",
    "from Anomaly_Detection.Multiclass_Detection.fdom_classifiers.fDOM_PLP import (\n",
    "    fDOM_PLP_Classifier,\n",
    ")\n",
    "from Anomaly_Detection.Multiclass_Detection.fdom_classifiers.fDOM_FPT import (\n",
    "    fDOM_FPT_Classifier,\n",
    ")\n",
    "from Anomaly_Detection.Multiclass_Detection.fdom_classifiers.fDOM_FSK import (\n",
    "    fDOM_FSK_Classifier,\n",
    ")\n",
    "from Anomaly_Detection.Multiclass_Detection.fdom_classifiers.fDOM_PP import (\n",
    "    fDOM_PP_Classifier,\n",
    ")\n",
    "from Anomaly_Detection.Multiclass_Detection.fdom_classifiers.fDOM_SKP import (\n",
    "    fDOM_SKP_Classifier,\n",
    ")\n",
    "\n",
    "# disable warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.warn = warn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters and helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATIONS = 4000\n",
    "NUM_SPLITS = 5\n",
    "AUGMENT_DATA_BEGIN_TIMESTAMP = 2459096.9583333335\n",
    "USE_AUGMENTED_DATA = True\n",
    "CLASS_BALANCED_ONLY = True\n",
    "\n",
    "# for confusion matrix\n",
    "DATA_LABELS = [\"PLP\", \"SKP\", \"PP\", \"FPT\", \"FSK\", \"NAP\"]\n",
    "\n",
    "# Useful functions\n",
    "def get_prediction(plp_pred, skp_pred, pp_pred, fpt_pred, fsk_pred):\n",
    "    \"\"\"\n",
    "    take the top level prediction based on peak precendence\n",
    "    \"\"\"\n",
    "    if skp_pred == \"SKP\":\n",
    "        return skp_pred\n",
    "\n",
    "    elif pp_pred == \"PP\":\n",
    "        return pp_pred\n",
    "\n",
    "    elif plp_pred == \"PLP\":\n",
    "        return plp_pred\n",
    "\n",
    "    elif fpt_pred == \"FPT\":\n",
    "        return fpt_pred\n",
    "\n",
    "    elif fsk_pred == \"FSK\":\n",
    "        return fsk_pred\n",
    "\n",
    "    else:\n",
    "        return \"NAP\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filenames\n",
    "fdom_raw_data = \"Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv\"\n",
    "stage_raw_data = \"Data/converted_data/julian_format/stage_10.1.11-1.1.19.csv\"\n",
    "turb_raw_data = \"Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv\"\n",
    "\n",
    "fdom_labeled = \"Data/labeled_data/ground_truths/fDOM/fDOM_all_julian_0k-300k.csv\"\n",
    "\n",
    "fdom_raw_augmented = \"Data/augmented_data/fdom/unlabeled/unlabeled_fdom.csv\"\n",
    "fdom_labeled_augmented = \"Data/augmented_data/fdom/labeled/labeled_fdom_peaks.csv\"\n",
    "\n",
    "turb_augmented_raw_data = \"Data/augmented_data/fdom/unlabeled/unlabeled_turb.csv\"\n",
    "\n",
    "stage_augmented_data_fn = \"Data/augmented_data/fdom/unlabeled/unlabeled_stage.csv\"\n",
    "\n",
    "# to get fsk/fpt peaks in augmented data\n",
    "fdom_fpt_lookup_path = \"Data/augmented_data/fdom/fpt_lookup.csv\"\n",
    "fdom_fsk_lookup_path = \"Data/augmented_data/fdom/fsk_lookup.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "fDOM_data = dm.read_in_preprocessed_timeseries(fdom_raw_data)\n",
    "stage_data = dm.read_in_preprocessed_timeseries(stage_raw_data)\n",
    "turb_data = dm.read_in_preprocessed_timeseries(turb_raw_data)\n",
    "stage_data = dp.align_stage_to_fDOM(fDOM_data, stage_data)\n",
    "\n",
    "augmented_fdom_data = np.array(dm.read_in_timeseries(fdom_raw_augmented, True))\n",
    "augmented_turb_data = np.array(dm.read_in_timeseries(turb_augmented_raw_data, True))\n",
    "augmented_stage_data = np.array(dm.read_in_timeseries(stage_augmented_data_fn, True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Candidates and truths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get candidates from raw data\n",
    "cands = get_all_cands_fDOM(\n",
    "    fdom_raw_data,\n",
    "    fdom_labeled,\n",
    ")\n",
    "\n",
    "# get truths from raw data\n",
    "truths = get_all_truths_fDOM(fdom_labeled)\n",
    "\n",
    "# assert they are the same size\n",
    "assert truths.shape == cands.shape\n",
    "\n",
    "print(f\"Total number of original data candidates: {cands.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented Data\n",
    "\n",
    "Make sure that `USE_AUGMENTED_DATA` == `True`, for this data to take effect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_AUGMENTED_DATA:\n",
    "    # get candidates from augmented data\n",
    "    cands_augmented = get_all_cands_fDOM(\n",
    "        fdom_raw_augmented,\n",
    "        fdom_labeled_augmented,\n",
    "        True,\n",
    "        fdom_fpt_lookup_path,\n",
    "        fdom_fsk_lookup_path,\n",
    "    )\n",
    "\n",
    "    truths_augmented = get_all_truths_fDOM(fdom_labeled_augmented, True)\n",
    "\n",
    "    # align the missing augmented data (FPT, NFPT, FSK, NFSK, some others)\n",
    "    truths_augmented = truths_augmented[\n",
    "        truths_augmented[\"idx_of_peak\"].isin(cands_augmented[\"idx_of_peak\"])\n",
    "    ]\n",
    "\n",
    "    assert truths_augmented.shape == cands_augmented.shape\n",
    "\n",
    "    print(f\"Total number of augmented candidates: {cands_augmented.shape[0]}\")\n",
    "\n",
    "    # concatenate two candidates and truths into single list\n",
    "    cands = pd.concat([cands, cands_augmented])\n",
    "    truths = pd.concat([truths, truths_augmented])\n",
    "\n",
    "    # concat augmented raw data to normal raw data, for classifier preprocessing\n",
    "    turb_data_total = np.concatenate((turb_data, augmented_turb_data))\n",
    "    fdom_total = np.concatenate((fDOM_data, augmented_fdom_data))\n",
    "    stage_total = np.concatenate((stage_data, augmented_stage_data))\n",
    "\n",
    "# if we aren't using augmented data, make sure classifiers have correct data being passed into them for preprocessing\n",
    "else:\n",
    "    turb_data_total = turb_data\n",
    "    fdom_total = fDOM_data\n",
    "    stage_total = stage_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatively, use only augmented data for pure class balanced training/testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLASS_BALANCED_ONLY:\n",
    "    # get candidates from augmented data\n",
    "    cands = get_all_cands_fDOM(\n",
    "        fdom_raw_augmented,\n",
    "        fdom_labeled_augmented,\n",
    "        True,\n",
    "        fdom_fpt_lookup_path,\n",
    "        fdom_fsk_lookup_path,\n",
    "    )\n",
    "\n",
    "    truths = get_all_truths_fDOM(fdom_labeled_augmented, True)\n",
    "\n",
    "    # align the missing augmented data (FPT, NFPT, FSK, NFSK, some others)\n",
    "    truths = truths[truths[\"idx_of_peak\"].isin(cands[\"idx_of_peak\"])]\n",
    "\n",
    "    assert truths.shape == cands.shape\n",
    "\n",
    "    print(f\"Total number of candidates: {cands.shape[0]}\")\n",
    "\n",
    "    turb_data_total = augmented_turb_data\n",
    "    fdom_total = augmented_fdom_data\n",
    "    stage_total = augmented_stage_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert cands and truths into lists\n",
    "cands = cands.values.tolist()\n",
    "\n",
    "truths = truths.values.tolist()\n",
    "\n",
    "print(str(len(cands)) + \" candidates in provided data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cands = cands_augmented.values.tolist()\n",
    "train_truths = truths_augmented.values.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plp_classifer = fDOM_PLP_Classifier(\n",
    "    fdom_total,\n",
    "    turb_data_total,\n",
    "    fdom_raw_data,\n",
    "    fdom_labeled,\n",
    "    fdom_raw_augmented,\n",
    "    fdom_labeled_augmented,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skp_classifier = fDOM_SKP_Classifier(\n",
    "    fdom_total, fdom_raw_data, fdom_labeled, fdom_raw_augmented, fdom_labeled_augmented\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_classifier = fDOM_PP_Classifier(\n",
    "    fdom_total, stage_total, AUGMENT_DATA_BEGIN_TIMESTAMP\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpt_classifier = fDOM_FPT_Classifier(fdom_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsk_classifier = fDOM_FSK_Classifier(fdom_total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulated_test_metrics = {}\n",
    "\n",
    "accumulated_test_results = {}\n",
    "\n",
    "accumulated_best_params = {}\n",
    "\n",
    "accumulated_cfmxs = {}\n",
    "\n",
    "discovered_labels = []\n",
    "\n",
    "# split data\n",
    "tss = TimeSeriesSplit(NUM_SPLITS)\n",
    "\n",
    "overall_start = datetime.datetime.now()\n",
    "\n",
    "split = 1\n",
    "divide_by_zero_errs = 0\n",
    "\n",
    "for train_val_indices, test_indices in tss.split(cands):\n",
    "    X_train, y_train = [cands[i] for i in train_val_indices], [\n",
    "        truths[i] for i in train_val_indices\n",
    "    ]\n",
    "\n",
    "    X_test, y_test = [cands[i] for i in test_indices], [truths[i] for i in test_indices]\n",
    "\n",
    "    max_fold_metric = 0\n",
    "    max_result = None\n",
    "\n",
    "    # print out info for user\n",
    "    print(\"\\nSplit: \", split)\n",
    "    split_start = datetime.datetime.now()\n",
    "    num_pos_test = len(list(filter(lambda x: x[2] != \"NAP\", y_test)))\n",
    "    num_pos_train = len(list(filter(lambda x: x[2] != \"NAP\", y_train)))\n",
    "\n",
    "    print(f\"Num Pos in Test: {num_pos_test}\")\n",
    "    print(f\"Num Pos in Train: {num_pos_train}\")\n",
    "\n",
    "    if num_pos_test >= 1 and num_pos_train >= 1:\n",
    "        # instantiate the progress bar\n",
    "        pbar = progressbar.ProgressBar(max_value=ITERATIONS)\n",
    "\n",
    "        # main training loop\n",
    "        for iteration in range(ITERATIONS):\n",
    "            params = {}\n",
    "\n",
    "            # start the iteration for each classifier (resets predictions, generates params)\n",
    "            params[\"plp\"] = plp_classifer.start_iteration()\n",
    "            params[\"skp\"] = skp_classifier.start_iteration()\n",
    "            params[\"pp\"] = pp_classifier.start_iteration()\n",
    "            params[\"fpt\"] = fpt_classifier.start_iteration()\n",
    "            params[\"fsk\"] = fsk_classifier.start_iteration()\n",
    "\n",
    "            # empty predictions array\n",
    "            train_preds = []\n",
    "\n",
    "            plp_preds = plp_classifer.classify_samples(X_train)\n",
    "            skp_preds = skp_classifier.classify_samples(X_train)\n",
    "            pp_preds = pp_classifier.classify_samples(X_train)\n",
    "            fpt_preds = fpt_classifier.classify_samples(X_train)\n",
    "            fsk_preds = fsk_classifier.classify_samples(X_train)\n",
    "            for i, pred in enumerate(plp_preds):\n",
    "                train_pred = get_prediction(\n",
    "                    pred[1], skp_preds[i][1], pp_preds[i][1], fpt_preds[i][1], fsk_preds[i][1]\n",
    "                )\n",
    "                train_preds.append(train_pred)\n",
    "\n",
    "            ######## GET SCORES ########\n",
    "            bal_acc = balanced_accuracy_score(\n",
    "                [row[2] for row in y_train],\n",
    "                [row for row in train_preds],\n",
    "            )\n",
    "\n",
    "            acc = accuracy_score(\n",
    "                [row[2] for row in y_train],\n",
    "                [row for row in train_preds],\n",
    "            )\n",
    "\n",
    "            f1_train = f1_score(\n",
    "                [row[2] for row in y_train], [row for row in train_preds], average=None\n",
    "            )\n",
    "\n",
    "            # print out acc in 10 even splits\n",
    "            if iteration and iteration % int(ITERATIONS / 10) == 0:\n",
    "                print(\"Iteration {} acc: {} \".format(iteration, acc), end=\"\")\n",
    "\n",
    "            # check acc, if better than max fold, save it\n",
    "            if acc > max_fold_metric:\n",
    "                max_fold_metric = acc\n",
    "                max_result = copy.deepcopy(train_preds)\n",
    "\n",
    "                # save params of classifiers\n",
    "                plp_classifer.got_best_result()\n",
    "                skp_classifier.got_best_result()\n",
    "                pp_classifier.got_best_results()\n",
    "                fpt_classifier.got_best_results()\n",
    "                fsk_classifier.got_best_results()\n",
    "\n",
    "            # call end of iteration, as we are at the end of an iteration\n",
    "            # allows individual classifiers to calc results\n",
    "            plp_classifer.end_of_iteration(y_train)\n",
    "            skp_classifier.end_of_iteration(y_train)\n",
    "            pp_classifier.end_of_iteration(y_train)\n",
    "            fpt_classifier.end_of_iteration(y_train)\n",
    "            fsk_classifier.end_of_iteration(y_train)\n",
    "\n",
    "            # TODO make confusion matrix on training data\n",
    "            \n",
    "\n",
    "            # update the progressbar\n",
    "            pbar.update(iteration)\n",
    "\n",
    "        # perform peak testing\n",
    "        test_preds = []\n",
    "        plp_preds = plp_classifer.classify_samples(X_test, True)\n",
    "        skp_preds = skp_classifier.classify_samples(X_test, True)\n",
    "        pp_preds = pp_classifier.classify_samples(X_test, True)\n",
    "        fpt_preds = fpt_classifier.classify_samples(X_test, True)\n",
    "        fsk_preds = fsk_classifier.classify_samples(X_test, True)\n",
    "\n",
    "        for i, pred in enumerate(plp_preds):\n",
    "            test_pred = get_prediction(\n",
    "                pred[1], skp_preds[i][1], pp_preds[i][1], fpt_preds[i][1], fsk_preds[i][1]\n",
    "            )\n",
    "            test_preds.append(test_pred)\n",
    "\n",
    "        ######## GET SCORES ########\n",
    "        # get confusion matrix\n",
    "        cfmx = confusion_matrix(\n",
    "            [row[2] for row in y_test],\n",
    "            [row for row in test_preds],\n",
    "            labels=DATA_LABELS,\n",
    "        )\n",
    "\n",
    "        # get acc score\n",
    "        acc_score = accuracy_score(\n",
    "            [row[2] for row in y_test],\n",
    "            [row for row in test_preds],\n",
    "        )\n",
    "\n",
    "        bal_acc = balanced_accuracy_score(\n",
    "            [row[2] for row in y_test],\n",
    "            [row for row in test_preds],\n",
    "        )\n",
    "\n",
    "        f1 = f1_score(\n",
    "            [row[2] for row in y_test],\n",
    "            [row for row in test_preds],\n",
    "            average=\"weighted\",\n",
    "        )\n",
    "\n",
    "        precision = precision_score(\n",
    "            [row[2] for row in y_test],\n",
    "            [row for row in test_preds],\n",
    "            average=\"weighted\",\n",
    "        )\n",
    "\n",
    "        print(f\"\\nSplit {split} test Scores: F1: {f1}  BA: {bal_acc}  ACC: {acc_score}\")\n",
    "\n",
    "        accumulated_cfmxs[split] = copy.deepcopy(cfmx)\n",
    "\n",
    "        accumulated_test_metrics[split] = {\n",
    "            \"f1\": f1,\n",
    "            \"acc\": acc_score,\n",
    "            \"ba\": bal_acc,\n",
    "            \"precision\": precision,\n",
    "        }\n",
    "\n",
    "        accumulated_test_results[split] = copy.deepcopy(test_preds)\n",
    "\n",
    "        # save the best params\n",
    "        accumulated_best_params[split] = {\n",
    "            \"PLP\": copy.deepcopy(plp_classifer.best_params),\n",
    "            \"SKP\": copy.deepcopy(skp_classifier.best_params),\n",
    "            \"PP\": copy.deepcopy(pp_classifier.best_params),\n",
    "            \"FPT\": copy.deepcopy(fpt_classifier.best_params),\n",
    "            \"FSK\": copy.deepcopy(fsk_classifier.best_params)\n",
    "        }\n",
    "\n",
    "        # increment split\n",
    "        split += 1\n",
    "\n",
    "# print a newline char for better display\n",
    "print(\"\\n\")\n",
    "\n",
    "overall_end_time = datetime.datetime.now()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_f1 = 0\n",
    "mean_ba = 0\n",
    "mean_precision = 0\n",
    "mean_acc = 0\n",
    "\n",
    "for key in accumulated_test_metrics:\n",
    "    metrics = accumulated_test_metrics[key]\n",
    "\n",
    "    mean_f1 += metrics[\"f1\"]\n",
    "    mean_ba += metrics[\"ba\"]\n",
    "    mean_precision += metrics[\"precision\"]\n",
    "    mean_acc += metrics[\"acc\"]\n",
    "\n",
    "print(\"Mean Test F1: \", mean_f1 / len(accumulated_test_metrics))\n",
    "print(\"Mean Test BA: \", mean_ba / len(accumulated_test_metrics))\n",
    "print(\"Mean Test Acc: \", mean_acc / len(accumulated_test_metrics))\n",
    "print(\"Mean Test Precision: \", mean_precision / len(accumulated_test_metrics))\n",
    "\n",
    "for split in accumulated_best_params.keys():\n",
    "    print(f\"\\nParams from split {split}:\")\n",
    "    for peak in accumulated_best_params[split].keys():\n",
    "        print(f\"\\nFor peak type {peak}:\")\n",
    "\n",
    "        for param in accumulated_best_params[split][peak].keys():\n",
    "            print(f\"{param}, value: {accumulated_best_params[split][peak][param]}\")\n",
    "\n",
    "# print(\"Training time: \", overall_end_time - overall_start)\n",
    "\n",
    "mean_cfmx = np.zeros((len(DATA_LABELS), len(DATA_LABELS)))\n",
    "for key in accumulated_cfmxs.keys():\n",
    "    mean_cfmx += accumulated_cfmxs[key]\n",
    "\n",
    "mean_cfmx = mean_cfmx / len(accumulated_cfmxs)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(label=\"fDOM Peak Detection Ratio Confusion Matrix\")\n",
    "\n",
    "sn.set(font_scale=1.5)\n",
    "\n",
    "sn.heatmap(\n",
    "    pd.DataFrame(\n",
    "        mean_cfmx.astype(\"float\") / mean_cfmx.sum(axis=1)[:, np.newaxis],\n",
    "        index=DATA_LABELS,\n",
    "        columns=DATA_LABELS,\n",
    "    ),\n",
    "    annot=True,\n",
    "    annot_kws={\"size\": 16},\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Ground Truths\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(label=\"fDOM Peak Detection Normal Confusion Matrix\")\n",
    "\n",
    "sn.set(font_scale=1.5)\n",
    "\n",
    "sn.heatmap(\n",
    "    pd.DataFrame(\n",
    "        mean_cfmx,\n",
    "        index=DATA_LABELS,\n",
    "        columns=DATA_LABELS,\n",
    "    ),\n",
    "    annot=True,\n",
    "    annot_kws={\"size\": 16},\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Ground Truths\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"Anomaly_Detection/Multiclass_Detection/Experimental_Results/fdom/best_params.pkl\",\n",
    "    \"wb\",\n",
    ") as pck_file:\n",
    "    pickle.dump(accumulated_best_params, pck_file)\n",
    "    pck_file.close()\n",
    "\n",
    "# Pickle test results\n",
    "with open(\n",
    "    \"Anomaly_Detection/Multiclass_Detection/Experimental_Results/fdom/test_results.pkl\",\n",
    "    \"wb\",\n",
    ") as pck_file:\n",
    "    pickle.dump(accumulated_test_results, pck_file)\n",
    "    pck_file.close()\n",
    "\n",
    "# Pickle test metrics\n",
    "with open(\n",
    "    \"Anomaly_Detection/Multiclass_Detection/Experimental_Results/fdom/test_metrics.pkl\",\n",
    "    \"wb\",\n",
    ") as pck_file:\n",
    "    pickle.dump(accumulated_test_metrics, pck_file)\n",
    "    pck_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics from individual classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print accuracies\n",
    "print(\"PLP CLASSIFIER INFO:\")\n",
    "print(\"ACC: \" + str(plp_classifer.best_acc))\n",
    "print(\"f1: \" + str(plp_classifer.best_f1_score))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"SKP CLASSIFIER INFO:\")\n",
    "print(\"ACC: \" + str(skp_classifier.best_acc))\n",
    "print(\"f1: \" + str(skp_classifier.best_f1_score))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"PP CLASSIFIER INFO:\")\n",
    "print(\"ACC: \" + str(pp_classifier.best_acc))\n",
    "print(\"f1: \" + str(pp_classifier.best_f1_score))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"FPT CLASSIFIER INFO:\")\n",
    "print(\"ACC: \" + str(fpt_classifier.best_acc))\n",
    "print(\"f1: \" + str(fpt_classifier.best_f1_score))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"FSK CLASSIFIER INFO:\")\n",
    "print(\"ACC: \" + str(fsk_classifier.best_acc))\n",
    "print(\"f1: \" + str(fsk_classifier.best_f1_score))\n",
    "print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ed753961bdc37ee89b4275051722ceb8ec0b57b8793db9d189305c313070a7d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('srrw')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
