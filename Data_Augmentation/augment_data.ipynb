{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenting time-series data\n",
    "In this file, the data is augmented in order to create more of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, \"../\")\n",
    "import Tools.data_processing as dp\n",
    "import Tools.data_movement as dm\n",
    "import Tools.augmentation_helpers as augment\n",
    "import pandas as pd\n",
    "from Tools.get_candidates import (\n",
    "    get_cands_fDOM_NAP,\n",
    "    get_cands_fDOM_PLP,\n",
    "    get_cands_fDOM_PP,\n",
    "    get_cands_fDOM_SKP,\n",
    "    get_cands_fDOM_FPT,\n",
    "    get_cands_fDOM_FSK,\n",
    "    get_cands_turb_PP,\n",
    "    get_cands_turb_SKP,\n",
    "    get_cands_turb_FPT,\n",
    "    get_cands_turb_NAP,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants\n",
    "\n",
    "We define two constants for use with augmenting the data:\n",
    "\n",
    "1. `TIME_RANGE_INIT`: the number of points before and after the relative peak that we take data from\n",
    "2. `ITERATIONS`: the number of times we loop over the list of anomaly peaks, and augment them\n",
    "3. `STARTING_TIMESTAMP`: the timestamp to start all augmented data at. The default value is 15 minutes after the last data measurement from the original set up data given to the project devs. As of 2/16/22, this timestamp is correct.\n",
    "4. `LOWER_BOUND_AMPLITUDE_MULTIPLIER`: the lower bound of the amplitude augment multiplier\n",
    "5. `UPPER_BOUND_AMPLITUDE_MULTIPLIER`: the upper bound of the amplitude augment multiplier\n",
    "6. `SMOOTH_LOWER_BOUND`: lower bound for number of points to cover when adding smoothing data\n",
    "7. `SMOOTH_UPPER_BOUND`: upper bound for number of points to cover when adding smoothing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Helpful constants \"\"\"\n",
    "TIME_RANGE_INIT = 30  # the base time range for peaks, in number of data points(MUST BE 1 OR HIGHER), this is used as a fallback\n",
    "STARTING_TIMESTAMP = 2459096.9583333335\n",
    "SAMPLES = 500 # the number of samples to generate\n",
    "\n",
    "# for how much we change peaks by\n",
    "LOWER_BOUND_AMPLITUDE_MULTIPLIER = -0.1\n",
    "UPPER_BOUND_AMPLITUDE_MULTIPLIER = 0.1\n",
    "\n",
    "SMOOTH_LOWER_BOUND = 200  # the minimum amount of data points to cover when smoothing\n",
    "SMOOTH_UPPER_BOUND = 400  # the maximum amount of data points to cover when smoothing\n",
    "\n",
    "# flat level average vals (for data smoothing)\n",
    "FLAT_FDOM_VAL = 5\n",
    "FLAT_TURB_VAL = 10\n",
    "FLAT_STAGE_VAL = 0.1\n",
    "\n",
    "# used for balancing classes, add any necessary peak classes for your data here\n",
    "FDOM_PEAK_LABELS = ['PLP', 'SKP', 'PP', 'FPT', 'FSK', 'NAP']\n",
    "TURB_PEAK_LABELS = ['PP', 'SKP', 'FPT', 'NAP']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in data\n",
    "The knowledge-based approach uses the data in `Data/converted_data/julian_format/`, so that is where the data augmentation will go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fDOM_data_filename = (\n",
    "    \"../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv\"\n",
    ")\n",
    "fDOM_truths_filename = (\n",
    "    \"../Data/labeled_data/ground_truths/fDOM/fDOM_all_julian_0k-300k.csv\"\n",
    ")\n",
    "turb_data_filename = (\n",
    "    \"../Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv\"\n",
    ")\n",
    "turb_truths_filename = (\n",
    "    \"../Data/labeled_data/ground_truths/turb/turb_all_julian_0k-300k.csv\"\n",
    ")\n",
    "\n",
    "# Read in raw data\n",
    "fDOM_data = dm.read_in_preprocessed_timeseries(fDOM_data_filename)\n",
    "stage_data = dm.read_in_preprocessed_timeseries(\n",
    "    \"../Data/converted_data/julian_format/stage_10.1.11-1.1.19.csv\"\n",
    ")\n",
    "turb_data = dm.read_in_preprocessed_timeseries(turb_data_filename)\n",
    "\n",
    "# align stage to fDOM\n",
    "stage_data = dp.align_stage_to_fDOM(fDOM_data, stage_data)\n",
    "\n",
    "# read in labeled fDOM\n",
    "fDOM_labeled = pd.read_csv(fDOM_truths_filename)\n",
    "\n",
    "# read in labeled turb\n",
    "turb_labeled = pd.read_csv(turb_truths_filename)\n",
    "\n",
    "# Convert data into pandas dataframes for better indexing:\n",
    "fDOM_raw = pd.DataFrame(fDOM_data)\n",
    "fDOM_raw.columns = [\"timestamp\", \"value\"]\n",
    "\n",
    "turb_raw = pd.DataFrame(turb_data)\n",
    "turb_raw.columns = [\"timestamp\", \"value\"]\n",
    "\n",
    "stage_raw = pd.DataFrame(stage_data)\n",
    "stage_raw.columns = [\"timestamp\", \"value\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting Data\n",
    "We will augment data for each type of peak, and for each measurement.\n",
    "\n",
    "Starting with fDOM:\n",
    "1. PLP (plummeting peak)\n",
    "2. PP (phantom peak)\n",
    "3. SKP (skyrocketing peak)\n",
    "\n",
    "TODO: augment more peak types when they are labeled\n",
    "\n",
    "With turbidity:\n",
    "1. PP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting fDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "#                                                                                   #\n",
    "#                               DATAFRAME SETUP SECTION                             #\n",
    "#                                                                                   #\n",
    "#####################################################################################\n",
    "\n",
    "# new dataframes for augmented labeled/raw fDOM\n",
    "augmented_fDOM_raw = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "augmented_fDOM_labeled = pd.DataFrame(\n",
    "    columns=[\"timestamp_of_peak\", \"value_of_peak\", \"label_of_peak\", \"idx_of_peak\"]\n",
    ")\n",
    "\n",
    "# dataframe for stage to align with augmented fDOM\n",
    "augmented_stage_raw_fdom = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "\n",
    "# dataframes for turbidity to align with augmented fDOM\n",
    "augmented_turb_raw_fdom = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "\n",
    "\n",
    "# variable to keep the last entry in the dataframe for stage\n",
    "# defaults to the last entry that was in fdom/turb raw csv files, in julian format\n",
    "prev_added_entry = STARTING_TIMESTAMP\n",
    "\n",
    "# a list of peaks that don't align with the fDOM raw file that was aligned with stage\n",
    "# i believe its just peaks that don't align with stage in general for whatever reason\n",
    "missed_fDOM_peaks = 0\n",
    "\n",
    "# ~~~~~ Collect starting and ending points of each fDOM peak type ~~~~~\n",
    "# PP\n",
    "fdom_pp_index_lookup = get_cands_fDOM_PP(fDOM_data_filename, fDOM_truths_filename)\n",
    "\n",
    "## SKP\n",
    "fdom_skp_index_lookup = get_cands_fDOM_SKP(fDOM_data_filename, fDOM_truths_filename)\n",
    "\n",
    "## PLP\n",
    "fdom_plp_index_lookup = get_cands_fDOM_PLP(fDOM_data_filename, fDOM_truths_filename)\n",
    "\n",
    "# flat plateaus (FPT)\n",
    "fdom_fpt_index_lookup = get_cands_fDOM_FPT()\n",
    "\n",
    "# flat sinks\n",
    "fdom_fsk_index_lookup = get_cands_fDOM_FSK()\n",
    "\n",
    "# non anomaly peaks\n",
    "fdom_NAP_index_lookup = get_cands_fDOM_NAP(fDOM_data_filename, fDOM_truths_filename)\n",
    "\n",
    "# to balance out classes\n",
    "class_count = {}\n",
    "for label in FDOM_PEAK_LABELS:\n",
    "    class_count[label] = 0\n",
    "\n",
    "for sample in range(SAMPLES):\n",
    "    \"\"\"\n",
    "    gen a sample\n",
    "    \"\"\"\n",
    "\n",
    "    peaks = fDOM_labeled.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # find out which peak label we need augment next to keep classes balanced\n",
    "    next_label = augment.check_class_balance(class_count, FDOM_PEAK_LABELS)\n",
    "\n",
    "    # select a peak from peaks randomly\n",
    "    peak = peaks.sample()\n",
    "    peak_label = peak['label_of_peak'].tolist()[0]\n",
    "\n",
    "    # check its label, need to iterate until we get a correct one to keep classes balanced\n",
    "    while peak_label != next_label:\n",
    "        peak = peaks.sample()\n",
    "        peak_label = peak['label_of_peak'].tolist()[0]\n",
    "    \n",
    "    # update class count\n",
    "    class_count[peak_label] += 1\n",
    "\n",
    "    # now, we need to actually augment the peak\n",
    "    prev_dist, next_dist = TIME_RANGE_INIT, TIME_RANGE_INIT\n",
    "\n",
    "    # get lookup table for specific cand\n",
    "    if peak_label == \"PP\":\n",
    "        cands_df = fdom_pp_index_lookup\n",
    "    elif peak_label == \"PLP\":\n",
    "        cands_df = fdom_plp_index_lookup\n",
    "    elif peak_label == \"SKP\":\n",
    "        cands_df = fdom_skp_index_lookup\n",
    "    elif peak_label == \"FSK\":\n",
    "        cands_df = fdom_fsk_index_lookup\n",
    "    elif peak_label == \"FPT\":\n",
    "        cands_df = fdom_fpt_index_lookup\n",
    "    else:\n",
    "        cands_df = fdom_NAP_index_lookup\n",
    "\n",
    "    # get timestamp of peak\n",
    "    peak_timestamp = peak[\"timestamp_of_peak\"].tolist()[0]\n",
    "\n",
    "    # get the indices of the peak\n",
    "    peak_index_df = fDOM_raw[fDOM_raw[\"timestamp\"] == peak_timestamp]\n",
    "    stage_index_df = stage_raw[stage_raw[\"timestamp\"] == peak_timestamp]\n",
    "    turb_index_df = turb_raw[turb_raw[\"timestamp\"] == peak_timestamp]\n",
    "\n",
    "    if len(peak_index_df.index.tolist()) != 0:\n",
    "        peak_index = peak_index_df.index.tolist()[0]\n",
    "        stage_index = stage_index_df.index.tolist()[0]\n",
    "        turb_index = turb_index_df.index.tolist()[0]\n",
    "\n",
    "        # get the benginning and end of the peak\n",
    "        left, right = augment.get_ends_of_peak(cands_df, peak_index)\n",
    "        prev_dist = abs(peak_index - left)\n",
    "        next_dist = abs(peak_index - right)\n",
    "\n",
    "        # build the temp dataframes to concat to the main df\n",
    "        new_fdom_raw, new_stage, new_turb_raw = augment.build_temp_dataframes(\n",
    "            fDOM_raw,\n",
    "            stage_raw,\n",
    "            turb_raw,\n",
    "            prev_dist,\n",
    "            next_dist,\n",
    "            peak_index,\n",
    "            stage_index,\n",
    "            turb_index,\n",
    "        )\n",
    "\n",
    "        # actual data augmentation here\n",
    "        new_fdom_raw = augment.augment_data(\n",
    "            new_fdom_raw,\n",
    "            peak_index,\n",
    "            LOWER_BOUND_AMPLITUDE_MULTIPLIER,\n",
    "            UPPER_BOUND_AMPLITUDE_MULTIPLIER,\n",
    "        )\n",
    "\n",
    "        # SMOOTH DATA\n",
    "        # ensure that main augmented df has more than 1 row, else no data to smooth\n",
    "        if augmented_fDOM_raw.shape[0] > 1:\n",
    "            (\n",
    "                augmented_fDOM_raw,\n",
    "                augmented_stage_raw_fdom,\n",
    "                augmented_turb_raw_fdom,\n",
    "                prev_added_entry,\n",
    "            ) = augment.smooth_data(\n",
    "                augmented_fDOM_raw,\n",
    "                augmented_stage_raw_fdom,\n",
    "                augmented_turb_raw_fdom,\n",
    "                prev_added_entry,\n",
    "                SMOOTH_LOWER_BOUND,\n",
    "                SMOOTH_UPPER_BOUND,\n",
    "                FLAT_FDOM_VAL,\n",
    "                FLAT_TURB_VAL,\n",
    "                FLAT_STAGE_VAL,\n",
    "            )\n",
    "\n",
    "        # update the dataframes to set new indices and timestamps\n",
    "        (\n",
    "            new_label,\n",
    "            new_fdom_raw,\n",
    "            new_stage,\n",
    "            new_turb_raw,\n",
    "            prev_added_entry,\n",
    "        ) = augment.update_dataframes(\n",
    "            prev_added_entry,\n",
    "            new_fdom_raw,\n",
    "            peak_index,\n",
    "            prev_dist,\n",
    "            augmented_fDOM_raw,\n",
    "            new_stage,\n",
    "            new_turb_raw,\n",
    "            peak_label,\n",
    "        )\n",
    "\n",
    "        # concat rest of the peaks\n",
    "        (\n",
    "            augmented_fDOM_labeled,\n",
    "            augmented_fDOM_raw,\n",
    "            augmented_stage_raw_fdom,\n",
    "            augmented_turb_raw_fdom,\n",
    "        ) = augment.concat_dataframes(\n",
    "            augmented_fDOM_labeled,\n",
    "            augmented_fDOM_raw,\n",
    "            augmented_stage_raw_fdom,\n",
    "            augmented_turb_raw_fdom,\n",
    "            new_fdom_raw,\n",
    "            new_stage,\n",
    "            new_turb_raw,\n",
    "            new_label,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        missed_fDOM_peaks += 1\n",
    "\n",
    "# print the class count for debugging\n",
    "print('FINAL COUNTS OF AUGMENTED PEAKS BY CLASS')\n",
    "print('________________________________________')\n",
    "print(class_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment turbidity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "#                                                                                   #\n",
    "#                               DATAFRAME SETUP SECTION                             #\n",
    "#                                                                                   #\n",
    "#####################################################################################\n",
    "\n",
    "# new dataframes for augmented labeled and raw turb\n",
    "augmented_turb_raw = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "augmented_turb_labeled = pd.DataFrame(\n",
    "    columns=[\"timestamp_of_peak\", \"value_of_peak\", \"label_of_peak\", \"idx_of_peak\"]\n",
    ")\n",
    "\n",
    "# dataframe for stage to align with augmented turb\n",
    "augmented_stage_raw_turb = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "\n",
    "# dataframes for fDOM to align with augmented turb\n",
    "augmented_fdom_raw_turb = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "\n",
    "# var to keep last time entry for augmentation\n",
    "prev_added_entry = STARTING_TIMESTAMP\n",
    "\n",
    "# missed peaks\n",
    "missed_turb_peaks = 0\n",
    "\n",
    "# ~~~~~~ Collect starting and ending points of each fDOM peak type\n",
    "# PP\n",
    "turb_pp_index_lookup = get_cands_turb_PP(turb_data_filename, turb_truths_filename)\n",
    "\n",
    "## SKP\n",
    "turb_skp_index_lookup = get_cands_turb_SKP(turb_data_filename, turb_truths_filename)\n",
    "\n",
    "## FPT\n",
    "turb_fpt_index_lookup = get_cands_turb_FPT(turb_data_filename, turb_truths_filename)\n",
    "\n",
    "# non anomaly peaks\n",
    "turb_NAP_index_lookup = get_cands_turb_NAP(turb_data_filename, turb_truths_filename)\n",
    "\n",
    "# for class balancing\n",
    "class_count = {}\n",
    "for label in TURB_PEAK_LABELS:\n",
    "    class_count[label] = 0\n",
    "\n",
    "for sample in range(SAMPLES):\n",
    "    \"\"\"\n",
    "    augment a single sample\n",
    "    \"\"\"\n",
    "\n",
    "    peaks = turb_labeled.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # get next label to augment\n",
    "    next_label = augment.check_class_balance(class_count, TURB_PEAK_LABELS)\n",
    "\n",
    "    peak = peaks.sample()\n",
    "    peak_label = peak[\"label_of_peak\"].tolist()[0]\n",
    "\n",
    "    while peak_label != next_label:\n",
    "        peak = peaks.sample()\n",
    "        peak_label = peak[\"label_of_peak\"].tolist()[0]\n",
    "\n",
    "    class_count[peak_label] += 1\n",
    "\n",
    "    # now, we need to actually augment the peak\n",
    "    prev_dist, next_dist = TIME_RANGE_INIT, TIME_RANGE_INIT\n",
    "\n",
    "    # get lookup table for peak beginning and ends\n",
    "    if peak_label == \"SKP\":\n",
    "        cands_df = turb_skp_index_lookup\n",
    "    elif peak_label == \"PP\":\n",
    "        cands_df = turb_pp_index_lookup\n",
    "    elif peak_label == \"FPT\":\n",
    "        cands_df = turb_fpt_index_lookup\n",
    "    else:\n",
    "        cands_df = turb_NAP_index_lookup\n",
    "\n",
    "    # get timestamp of peak\n",
    "    peak_timestamp = peak[\"timestamp_of_peak\"].tolist()[0]\n",
    "\n",
    "    # get peak indices (NOTE THAT PEAK IS TURB)\n",
    "    peak_index_df = turb_raw[turb_raw[\"timestamp\"] == peak_timestamp]\n",
    "    stage_index_df = stage_raw[stage_raw[\"timestamp\"] == peak_timestamp]\n",
    "    fdom_index_df = fDOM_raw[fDOM_raw[\"timestamp\"] == peak_timestamp]\n",
    "\n",
    "    if len(peak_index_df.index.tolist()) != 0:\n",
    "        peak_index = peak_index_df.index.tolist()[0]\n",
    "        stage_index = stage_index_df.index.tolist()[0]\n",
    "        fdom_index = fdom_index_df.index.tolist()[0]\n",
    "\n",
    "        # get beginning and ending of peak indices from lookup cands df\n",
    "        left, right = augment.get_ends_of_peak(cands_df, peak_index)\n",
    "        prev_dist = int(abs(peak_index - left))\n",
    "        next_dist = int(abs(peak_index - right))\n",
    "\n",
    "        new_fdom_raw, new_stage, new_turb_raw = augment.build_temp_dataframes(\n",
    "            fDOM_raw,\n",
    "            stage_raw,\n",
    "            turb_raw,\n",
    "            prev_dist,\n",
    "            next_dist,\n",
    "            fdom_index,\n",
    "            stage_index,\n",
    "            peak_index,\n",
    "        )\n",
    "\n",
    "        # augment data\n",
    "        new_turb_raw = augment.augment_data(\n",
    "            new_turb_raw,\n",
    "            peak_index,\n",
    "            LOWER_BOUND_AMPLITUDE_MULTIPLIER,\n",
    "            UPPER_BOUND_AMPLITUDE_MULTIPLIER,\n",
    "        )\n",
    "\n",
    "        # SMOOTH DATA\n",
    "        if augmented_turb_raw.shape[0] > 1:\n",
    "            (\n",
    "                augmented_fdom_raw_turb,\n",
    "                augmented_stage_raw_turb,\n",
    "                augmented_turb_raw,\n",
    "                prev_added_entry,\n",
    "            ) = augment.smooth_data(\n",
    "                augmented_fdom_raw_turb,\n",
    "                augmented_stage_raw_turb,\n",
    "                augmented_turb_raw,\n",
    "                prev_added_entry,\n",
    "                SMOOTH_LOWER_BOUND,\n",
    "                SMOOTH_UPPER_BOUND,\n",
    "                FLAT_FDOM_VAL,\n",
    "                FLAT_TURB_VAL,\n",
    "                FLAT_STAGE_VAL,\n",
    "            )\n",
    "\n",
    "            # update dataframes\n",
    "            # UPDATE FRAMES\n",
    "        (\n",
    "            new_label,\n",
    "            new_fdom_raw,\n",
    "            new_stage,\n",
    "            new_turb_raw,\n",
    "            prev_added_entry,\n",
    "        ) = augment.update_dataframes(\n",
    "            prev_added_entry,\n",
    "            new_fdom_raw,\n",
    "            peak_index,\n",
    "            prev_dist,\n",
    "            augmented_fdom_raw_turb,\n",
    "            new_stage,\n",
    "            new_turb_raw,\n",
    "            peak_label,\n",
    "        )\n",
    "\n",
    "        # CONCAT FRAMES\n",
    "        (\n",
    "            augmented_turb_labeled,\n",
    "            augmented_fdom_raw_turb,\n",
    "            augmented_stage_raw_turb,\n",
    "            augmented_turb_raw,\n",
    "        ) = augment.concat_dataframes(\n",
    "            augmented_turb_labeled,\n",
    "            augmented_fdom_raw_turb,\n",
    "            augmented_stage_raw_turb,\n",
    "            augmented_turb_raw,\n",
    "            new_fdom_raw,\n",
    "            new_stage,\n",
    "            new_turb_raw,\n",
    "            new_label,\n",
    "        )\n",
    "    else:\n",
    "        missed_turb_peaks += 1\n",
    "\n",
    "# print the class count for debugging\n",
    "print('FINAL COUNTS OF AUGMENTED PEAKS BY CLASS')\n",
    "print('________________________________________')\n",
    "print(class_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move augmented data into csv files\n",
    "The following codeblock creates csv files for the augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Augmented Data Paths \"\"\"\n",
    "# NOTE: Before running this cell, be sure that the following paths exist, the folders must be there to allow the data to be written\n",
    "# trainset\n",
    "trainset_fdom_path = \"../Data/augmented_data/trainset_plotting/fdom/\"\n",
    "trainset_turb_path = \"../Data/augmented_data/trainset_plotting/turb/\"\n",
    "\n",
    "# unlabeled data\n",
    "unlabeled_fdom_path = \"../Data/augmented_data/fdom/unlabeled/\"\n",
    "unlabeled_turb_path = \"../Data/augmented_data/turb/unlabeled/\"\n",
    "\n",
    "# labeled data\n",
    "labeled_fdom_path = \"../Data/augmented_data/fdom/labeled/\"\n",
    "labeled_turb_path = \"../Data/augmented_data/turb/labeled/\"\n",
    "\n",
    "# write to normal julian csv\n",
    "augment.write_augmented_data_to_csv(\n",
    "    labeled_fdom_path,\n",
    "    unlabeled_fdom_path,\n",
    "    labeled_turb_path,\n",
    "    unlabeled_turb_path,\n",
    "    augmented_fDOM_labeled,\n",
    "    augmented_fDOM_raw,\n",
    "    augmented_turb_raw_fdom,\n",
    "    augmented_stage_raw_fdom,\n",
    "    augmented_turb_labeled,\n",
    "    augmented_turb_raw,\n",
    "    augmented_fdom_raw_turb,\n",
    "    augmented_stage_raw_turb,\n",
    ")\n",
    "\n",
    "# write to trainset\n",
    "augment.write_to_trainset_csv(\n",
    "    augmented_fDOM_raw,\n",
    "    augmented_turb_raw_fdom,\n",
    "    augmented_stage_raw_fdom,\n",
    "    trainset_fdom_path,\n",
    "    augmented_turb_raw,\n",
    "    augmented_fdom_raw_turb,\n",
    "    augmented_stage_raw_turb,\n",
    "    trainset_turb_path,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ed753961bdc37ee89b4275051722ceb8ec0b57b8793db9d189305c313070a7d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('srrw': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
