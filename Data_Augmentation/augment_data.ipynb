{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenting time-series data\n",
    "In this file, the data is augmented in order to create more of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.insert(1,'../')\n",
    "import Tools.data_processing as dp\n",
    "import Tools.data_movement as dm \n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "The following functions provide useful tools for the augmentation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/claykaufmann/Projects/srrw-anomaly-detection/Data_Augmentation/augment_data.ipynb Cell 4'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/claykaufmann/Projects/srrw-anomaly-detection/Data_Augmentation/augment_data.ipynb#ch0000003?line=0'>1</a>\u001b[0m \u001b[39m# Helper functions\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/claykaufmann/Projects/srrw-anomaly-detection/Data_Augmentation/augment_data.ipynb#ch0000003?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_data\u001b[39m(fdom_range: pd\u001b[39m.\u001b[39mDataFrame, stage_range: pd\u001b[39m.\u001b[39mDataFrame, turb_range: pd\u001b[39m.\u001b[39mDataFrame, peak_index: \u001b[39mint\u001b[39m, datatype: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m(pd\u001b[39m.\u001b[39mDataFrame, pd\u001b[39m.\u001b[39mDataFrame, pd\u001b[39m.\u001b[39mDataFrame, \u001b[39mint\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/claykaufmann/Projects/srrw-anomaly-detection/Data_Augmentation/augment_data.ipynb#ch0000003?line=2'>3</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/claykaufmann/Projects/srrw-anomaly-detection/Data_Augmentation/augment_data.ipynb#ch0000003?line=3'>4</a>\u001b[0m \u001b[39m    Makes changes to the current datapoints, by modifying peaks, and adding in values if needed\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/claykaufmann/Projects/srrw-anomaly-detection/Data_Augmentation/augment_data.ipynb#ch0000003?line=4'>5</a>\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/claykaufmann/Projects/srrw-anomaly-detection/Data_Augmentation/augment_data.ipynb#ch0000003?line=11'>12</a>\u001b[0m \u001b[39m    returns: the new data range to be appended to the data\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/claykaufmann/Projects/srrw-anomaly-detection/Data_Augmentation/augment_data.ipynb#ch0000003?line=12'>13</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/claykaufmann/Projects/srrw-anomaly-detection/Data_Augmentation/augment_data.ipynb#ch0000003?line=13'>14</a>\u001b[0m     \u001b[39m# make a copy of the modified data\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Helper functions\n",
    "def create_data(fdom_range: pd.DataFrame, stage_range: pd.DataFrame, turb_range: pd.DataFrame, peak_index: int, datatype: str) -> tuple(pd.DataFrame, pd.DataFrame, pd.DataFrame, int):\n",
    "    \"\"\"\n",
    "    Makes changes to the current datapoints, by modifying peaks, and adding in values if needed\n",
    "    \n",
    "    data: the range of datapoints by index in a dataframe\n",
    "\n",
    "    peak_index: the index of the actual peak we are modifying\n",
    "\n",
    "    datatype: \"fdom\" or \"turb\"\n",
    "\n",
    "    returns: the new data range to be appended to the data\n",
    "    \"\"\"\n",
    "    # make a copy of the modified data\n",
    "    new_fdom = copy.deepcopy(fdom_range)\n",
    "    new_stage = copy.deepcopy(stage_range)\n",
    "    new_turb = copy.deepcopy(turb_range)\n",
    "\n",
    "    # peak index can change when we add in x data\n",
    "    new_fdom_peak_index = peak_index\n",
    "\n",
    "    if datatype == \"fdom\":\n",
    "\n",
    "        # augment fDOM\n",
    "\n",
    "        # add entries to stage as needed\n",
    "\n",
    "        # add entries to turb as needed\n",
    "        pass\n",
    "\n",
    "    else:\n",
    "\n",
    "        # augment turb\n",
    "\n",
    "        # add entries to stage as needed\n",
    "\n",
    "        # add entries to fdom as needed\n",
    "        pass\n",
    "\n",
    "    return new_fdom, new_stage, new_turb, new_fdom_peak_index\n",
    "\n",
    "\n",
    "def next_time_entry(current_entry: float) -> float:\n",
    "    \"\"\"\n",
    "    This function returns the next time entry in julian time\n",
    "    \n",
    "    current_entry: a julina time float\n",
    "\n",
    "    return: julian time + 15 minutes from past julian time\n",
    "    \"\"\"\n",
    "\n",
    "    # convert julian to datetime\n",
    "    date_time_init = dp.julian_to_datetime(current_entry)\n",
    "\n",
    "    # find next date time (add 15 minutes)\n",
    "    next_entry = date_time_init + timedelta(minutes=15)\n",
    "\n",
    "    # convert date time to julian time\n",
    "    final_julian_time = dp.datetime_to_julian(next_entry)\n",
    "\n",
    "    # return julian time\n",
    "    return final_julian_time\n",
    "\n",
    "\n",
    "def reindex_augmented_data(data: pd.DataFrame, datatype: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reindex the augmented data so there are no overlaps\n",
    "\n",
    "    data: the data to reindex\n",
    "    datatype: fdom, turb, or stage\n",
    "\n",
    "    returns: reindexed data\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in data\n",
    "The knowledge-based approach uses the data in `Data/converted_data/julian_format/`, so that is where the data augmentation will go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in raw data\n",
    "fDOM_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv')\n",
    "stage_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/stage_10.1.11-1.1.19.csv')\n",
    "turb_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv')\n",
    "\n",
    "# align stage to fDOM\n",
    "stage_data = dp.align_stage_to_fDOM(fDOM_data, stage_data)\n",
    "\n",
    "# read in labeled fDOM\n",
    "fDOM_labeled = pd.read_csv(\"../Data/labeled_data/ground_truths/fDOM/fDOM_all_julian_0k-300k.csv\")\n",
    "\n",
    "turb_labeled = pd.read_csv(\"../Data/labeled_data/ground_truths/turb/turb_pp/julian_time/turb_pp_0k-300k_labeled.csv\")\n",
    "\n",
    "# New data folder:\n",
    "AUGMENT_DATA_PATH = '../Data/augmented_data/julian_format/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into pandas dataframes for better indexing:\n",
    "fDOM_raw = pd.DataFrame(fDOM_data)\n",
    "fDOM_raw.columns = [\"timestamp\", \"value\"]\n",
    "\n",
    "turb_raw = pd.DataFrame(turb_data)\n",
    "turb_raw.columns = [\"timestamp\", \"value\"]\n",
    "\n",
    "stage_raw = pd.DataFrame(stage_data)\n",
    "stage_raw.columns = [\"timestamp\", \"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9406\n",
      "2456140.479166667\n"
     ]
    }
   ],
   "source": [
    "# Visualize data\n",
    "# timestamp_of_peak = fDOM_labeled.loc[88, 'timestamp_of_peak']\n",
    "# index_df = fDOM_raw[fDOM_raw['timestamp'] == timestamp_of_peak]\n",
    "\n",
    "# if len(index_df.index.to_list()) != 0:\n",
    "#     index_of_peak = index_df.index.tolist()[0]\n",
    "\n",
    "#     print(index_of_peak)\n",
    "#     print(timestamp_of_peak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting Data\n",
    "We will augment data for each type of peak, and for each measurement.\n",
    "\n",
    "Starting with fDOM:\n",
    "1. PLP (plummeting peak)\n",
    "2. PP (phantom peak)\n",
    "3. SKP (skyrocketing peak)\n",
    "\n",
    "TODO: augment more peak types when they are labeled\n",
    "\n",
    "With turbidity:\n",
    "1. PP\n",
    "\n",
    "We start by creating our data frames to augment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Helpful constants \"\"\"\n",
    "TIME_RANGE_INIT = 5 # the base distance of points to add between the peaks\n",
    "\n",
    "\"\"\" Dataframes to be used by augmenter \"\"\"\n",
    "\n",
    "# labeled fDOM peaks\n",
    "fdom_anon_peaks = fDOM_labeled[fDOM_labeled['label_of_peak'] != 'NAP']\n",
    "fdom_anon_peaks = fdom_anon_peaks.reset_index() # reset index as values were removed\n",
    "\n",
    "# labeled turb peaks\n",
    "turb_anon_peaks = turb_labeled[turb_labeled['label_of_peak'] != \"NPP\"]\n",
    "turb_anon_peaks = turb_anon_peaks.reset_index()\n",
    "\n",
    "# new dataframes for augmented labeled/raw fDOM\n",
    "augmented_fDOM_raw = pd.DataFrame(columns=['timestamp', 'value'])\n",
    "augmented_fDOM_labeled = pd.DataFrame(columns=['timestamp_of_peak', 'value_of_peak', 'label_of_peak', 'idx_of_peak'])\n",
    "\n",
    "# dataframe for augmented stage \n",
    "augmented_stage_raw = pd.DataFrame(columns=['timestamp', 'value'])\n",
    "\n",
    "# dataframes for augmented raw/labeled turbidity\n",
    "augmented_turb_raw = pd.DataFrame(columns=['timestamp', 'value'])\n",
    "augmented_turb_labeled = pd.DataFrame(columns=['timestamp_of_peak', 'value_of_peak', 'label_of_peak', 'idx_of_peak'])\n",
    "\n",
    "# a list of peaks that don't align with the fDOM raw file that was aligned with stage\n",
    "# i believe its just peaks that don't align with stage in general for whatever reason\n",
    "missed_fDOM_peaks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting fDOM\n",
    "The next codeblocks augment fDOM data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         timestamp      value\n",
      "9665  2.456143e+06  33.203392\n",
      "9666  2.456143e+06  43.953086\n",
      "9667  2.456143e+06  40.934235\n",
      "9668  2.456143e+06  40.071279\n",
      "9669  2.456143e+06  44.637700\n",
      "9670  2.456143e+06  41.017220\n",
      "9671  2.456143e+06  47.057776\n",
      "9672  2.456143e+06  41.434796\n",
      "9673  2.456143e+06  49.846897\n",
      "9674  2.456143e+06  44.133289\n",
      "         timestamp     value\n",
      "9665  2.456143e+06  0.081429\n",
      "9666  2.456143e+06  0.081000\n",
      "9667  2.456143e+06  0.080571\n",
      "9668  2.456143e+06  0.080143\n",
      "9669  2.456143e+06  0.079714\n",
      "9670  2.456143e+06  0.079286\n",
      "9671  2.456143e+06  0.078857\n",
      "9672  2.456143e+06  0.078429\n",
      "9673  2.456143e+06  0.078000\n",
      "9674  2.456143e+06  0.078148\n",
      "         timestamp     value\n",
      "9665  2.456143e+06  0.851615\n",
      "9666  2.456143e+06  0.487554\n",
      "9667  2.456143e+06  0.525225\n",
      "9668  2.456143e+06  0.684105\n",
      "9669  2.456143e+06  0.801421\n",
      "9670  2.456143e+06  0.521043\n",
      "9671  2.456143e+06  0.910232\n",
      "9672  2.456143e+06  0.558703\n",
      "9673  2.456143e+06  0.659151\n",
      "9674  2.456143e+06  0.650588\n"
     ]
    }
   ],
   "source": [
    "# HACK: there are overlaps in time ranges, might not be an issue but it could be an issue\n",
    "# TODO: turn this into a callable function when it is finished\n",
    "\n",
    "\n",
    "# iterate over each peak\n",
    "for i, row in fdom_anon_peaks.iterrows():\n",
    "    # check to see if any overlap occurs between peaks\n",
    "    prev_dist = TIME_RANGE_INIT\n",
    "    next_dist = TIME_RANGE_INIT\n",
    "\n",
    "    if i == 0:\n",
    "        # we are at the first peak, check to see if there are 5 data points behind\n",
    "        # TODO: implement this\n",
    "        # note that its actually not needed\n",
    "        pass \n",
    "\n",
    "    elif i + 1 < fdom_anon_peaks.shape[0]:\n",
    "        # anywhere else in the middle, check for overlap\n",
    "        # FIXME: not currently checking for overlaps, we might not need to tho\n",
    "\n",
    "        # check next 5\n",
    "        if row['idx_of_peak'] + TIME_RANGE_INIT >= fdom_anon_peaks.loc[i + 1,'idx_of_peak'] - TIME_RANGE_INIT: # -5 becase we go back 5 peaks too\n",
    "            # change next_dist to whatever it needs to be \n",
    "            curr_dist_to_peak = abs(row['idx_of_peak'] - fdom_anon_peaks.loc[i + 1, 'idx_of_peak'] - TIME_RANGE_INIT)\n",
    "            next_dist = curr_dist_to_peak - 1\n",
    "\n",
    "        # check past 5\n",
    "        if row['idx_of_peak'] - TIME_RANGE_INIT <= fdom_anon_peaks.loc[i - 1,'idx_of_peak'] + TIME_RANGE_INIT:\n",
    "            curr_dist_to_peak = abs(row['idx_of_peak'] - fdom_anon_peaks.loc[i - 1, 'idx_of_peak'] - TIME_RANGE_INIT)\n",
    "            prev_dist = curr_dist_to_peak - 1\n",
    "\n",
    "    else:\n",
    "        # if no next peak, we are at the last peak, ensure there are still 5 data points to read\n",
    "        # TODO: implement this\n",
    "        pass\n",
    "\n",
    "\n",
    "    \"\"\"Get raw fDOM data points\"\"\"\n",
    "    timestamp_of_peak = fDOM_labeled.loc[i, 'timestamp_of_peak']\n",
    "\n",
    "    # get index dataframes of each type\n",
    "    # HACK: there has got to be a better way to do this\n",
    "    fdom_index_df = fDOM_raw[fDOM_raw['timestamp'] == timestamp_of_peak]\n",
    "    stage_index_df = stage_raw[stage_raw['timestamp'] == timestamp_of_peak]\n",
    "    turb_index_df = turb_raw[turb_raw['timestamp'] == timestamp_of_peak]\n",
    "\n",
    "    if len(fdom_index_df.index.to_list()) != 0:\n",
    "        # get indices of each data type from index df's \n",
    "        index_of_peak = fdom_index_df.index.tolist()[0]\n",
    "        stage_index = stage_index_df.index.tolist()[0]\n",
    "        turb_index = turb_index_df.index.tolist()[0]\n",
    "\n",
    "        # use this timestamp to make a dataframe of raw stuff\n",
    "        # get data from fDOM_raw file\n",
    "        fDOM_raw_time_range = pd.DataFrame(fDOM_raw.iloc[index_of_peak - prev_dist:index_of_peak + next_dist])\n",
    "\n",
    "        # get stage data range\n",
    "        stage_time_range = pd.DataFrame(stage_raw.iloc[stage_index - prev_dist:stage_index + next_dist])\n",
    "\n",
    "        # get turbidity data range\n",
    "        turb_time_range = pd.DataFrame(turb_raw.iloc[turb_index - prev_dist:turb_index + next_dist])\n",
    "\n",
    "        # get augmented data\n",
    "        fDOM_augmented, new_stage, new_turb, new_peak_index = create_data(fDOM_raw_time_range, stage_time_range, turb_time_range, index_of_peak, \"fdom\")\n",
    "\n",
    "        # append these to a new file, in format of the raw fDOM file\n",
    "        augmented_fDOM_labeled.append(fDOM_augmented)\n",
    "\n",
    "        # append these to a new file, in format of the stage/turb/fDOM file\n",
    "        augmented_stage_raw.append(new_stage)\n",
    "\n",
    "        # append these to a new file, in format of labeled data (so purely the peak, mark the index of the peak when starting)\n",
    "        #augmented_fDOM_labeled.append(changed_data)\n",
    "    \n",
    "    # TODO add this missed data into the overall data somehow\n",
    "    else:\n",
    "        # we missed some data points, append them to the missed data dataframe\n",
    "        missed_fDOM_peaks.append(timestamp_of_peak)\n",
    "\n",
    "print(fDOM_raw_time_range)\n",
    "print(stage_time_range)\n",
    "print(turb_time_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment turbidity data\n",
    "The following code blocks augment turbidity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Augment turbidity data by calling previously written function \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move augmented data into csv files\n",
    "The following codeblock creates csv files for the augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Augmented Data Paths \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this function\n",
    "def write_augmented_data_to_csv():\n",
    "    # call to_csv for each dataframe\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f14ca78382e4dc227f1e51b680612414c34712d66eddea25f1c0d0c728b236b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('anomaly-detection': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
