{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenting time-series data\n",
    "In this file, the data is augmented in order to create more of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, \"../\")\n",
    "import Tools.data_processing as dp\n",
    "import Tools.data_movement as dm\n",
    "import Tools.augmentation_helpers as augment\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import copy\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.signal import find_peaks\n",
    "from scipy import interpolate\n",
    "import numpy as np\n",
    "import scipy.ndimage as ndimage\n",
    "import csv\n",
    "from Tools.get_candidates import (\n",
    "    get_cands_fDOM_NAP,\n",
    "    get_cands_fDOM_PLP,\n",
    "    get_cands_fDOM_PP,\n",
    "    get_cands_fDOM_SKP,\n",
    "    get_cands_turb_PP,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants\n",
    "\n",
    "We define two constants for use with augmenting the data:\n",
    "\n",
    "1. `TIME_RANGE_INIT`: the number of points before and after the relative peak that we take data from\n",
    "2. `ITERATIONS`: the number of times we loop over the list of anomaly peaks, and augment them\n",
    "3. `STARTING_TIMESTAMP`: the timestamp to start all augmented data at. The default value is 15 minutes after the last data measurement from the original set up data given to the project devs. As of 2/16/22, this timestamp is correct.\n",
    "4. `LOWER_BOUND_AMPLITUDE_MULTIPLIER`: the lower bound of the amplitude augment multiplier\n",
    "5. `UPPER_BOUND_AMPLITUDE_MULTIPLIER`: the upper bound of the amplitude augment multiplier\n",
    "6. `SMOOTH_LOWER_BOUND`: lower bound for number of points to cover when adding smoothing data\n",
    "7. `SMOOTH_UPPER_BOUND`: upper bound for number of points to cover when adding smoothing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Helpful constants \"\"\"\n",
    "TIME_RANGE_INIT = 30  # the base time range for peaks, in number of data points(MUST BE 1 OR HIGHER), this is used as a fallback\n",
    "ITERATIONS = 1  # number of times to loop over dataset and augment\n",
    "STARTING_TIMESTAMP = 2459096.9583333335\n",
    "\n",
    "LOWER_BOUND_AMPLITUDE_MULTIPLIER = -0.1\n",
    "UPPER_BOUND_AMPLITUDE_MULTIPLIER = 0.1\n",
    "\n",
    "# when these values are higher (>40), the augmented data looks wonky\n",
    "SMOOTH_LOWER_BOUND = 200  # the minimum amount of data points to cover when smoothing\n",
    "SMOOTH_UPPER_BOUND = 400  # the maximum amount of data points to cover when smoothing\n",
    "\n",
    "# flat level average vals\n",
    "FLAT_FDOM_VAL = 15\n",
    "FLAT_TURB_VAL = 18\n",
    "FLAT_STAGE_VAL = 0.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in data\n",
    "The knowledge-based approach uses the data in `Data/converted_data/julian_format/`, so that is where the data augmentation will go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in raw data\n",
    "fDOM_data = dm.read_in_preprocessed_timeseries(\n",
    "    \"../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv\"\n",
    ")\n",
    "stage_data = dm.read_in_preprocessed_timeseries(\n",
    "    \"../Data/converted_data/julian_format/stage_10.1.11-1.1.19.csv\"\n",
    ")\n",
    "turb_data = dm.read_in_preprocessed_timeseries(\n",
    "    \"../Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv\"\n",
    ")\n",
    "\n",
    "# align stage to fDOM\n",
    "stage_data = dp.align_stage_to_fDOM(fDOM_data, stage_data)\n",
    "\n",
    "# read in labeled fDOM\n",
    "fDOM_labeled = pd.read_csv(\n",
    "    \"../Data/labeled_data/ground_truths/fDOM/fDOM_all_julian_0k-300k.csv\"\n",
    ")\n",
    "\n",
    "# read in labeled turb\n",
    "turb_labeled = pd.read_csv(\n",
    "    \"../Data/labeled_data/ground_truths/turb/turb_pp/julian_time/turb_pp_0k-300k_labeled.csv\"\n",
    ")\n",
    "\n",
    "# Convert data into pandas dataframes for better indexing:\n",
    "fDOM_raw = pd.DataFrame(fDOM_data)\n",
    "fDOM_raw.columns = [\"timestamp\", \"value\"]\n",
    "\n",
    "turb_raw = pd.DataFrame(turb_data)\n",
    "turb_raw.columns = [\"timestamp\", \"value\"]\n",
    "\n",
    "stage_raw = pd.DataFrame(stage_data)\n",
    "stage_raw.columns = [\"timestamp\", \"value\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualize data\n",
    "\"\"\"\n",
    "fig = plt.figure()\n",
    "x = fDOM_raw['timestamp']\n",
    "y = turb_raw['value']\n",
    "\n",
    "line_fdom = plt.Line2D(fDOM_raw['timestamp'], fDOM_raw['value'])\n",
    "line_turb = plt.Line2D(turb_raw['timestamp'], turb_raw['value'], color='red')\n",
    "line_stage = plt.Line2D(stage_raw['timestamp'], stage_raw['value'], color='orange')\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.add_line(line_fdom)\n",
    "ax.add_line(line_turb)\n",
    "ax.add_line(line_stage)\n",
    "ax.set_xlim(min(x), max(x))\n",
    "ax.set_ylim(min(y) - 10, max(y) + 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting Data\n",
    "We will augment data for each type of peak, and for each measurement.\n",
    "\n",
    "Starting with fDOM:\n",
    "1. PLP (plummeting peak)\n",
    "2. PP (phantom peak)\n",
    "3. SKP (skyrocketing peak)\n",
    "\n",
    "TODO: augment more peak types when they are labeled\n",
    "\n",
    "With turbidity:\n",
    "1. PP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting fDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "#                                                                                   #\n",
    "#                               DATAFRAME SETUP SECTION                             #\n",
    "#                                                                                   #\n",
    "#####################################################################################\n",
    "\n",
    "# new dataframes for augmented labeled/raw fDOM\n",
    "augmented_fDOM_raw = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "augmented_fDOM_labeled = pd.DataFrame(\n",
    "    columns=[\"timestamp_of_peak\", \"value_of_peak\", \"label_of_peak\", \"idx_of_peak\"]\n",
    ")\n",
    "\n",
    "# dataframe for augmented stage\n",
    "augmented_stage_raw_fdom = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "\n",
    "# dataframes for augmented raw/labeled turbidity\n",
    "augmented_turb_raw_fdom = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "augmented_turb_labeled_fdom = pd.DataFrame(\n",
    "    columns=[\"timestamp_of_peak\", \"value_of_peak\", \"label_of_peak\", \"idx_of_peak\"]\n",
    ")\n",
    "\n",
    "# variable to keep the last entry in the dataframe for stage\n",
    "# defaults to the last entry that was in fdom/turb raw csv files, in julian format\n",
    "prev_added_entry = STARTING_TIMESTAMP\n",
    "\n",
    "# a list of peaks that don't align with the fDOM raw file that was aligned with stage\n",
    "# i believe its just peaks that don't align with stage in general for whatever reason\n",
    "missed_fDOM_peaks = []\n",
    "\n",
    "# ~~~~~ Collect starting and ending points of each fDOM peak type ~~~~~\n",
    "# PP\n",
    "fdom_pp_index_lookup = get_cands_fDOM_PP()\n",
    "\n",
    "## SKP\n",
    "fdom_skp_index_lookup = get_cands_fDOM_SKP()\n",
    "\n",
    "## PLP\n",
    "fdom_plp_index_lookup = get_cands_fDOM_PLP()\n",
    "\n",
    "# non anomaly peaks\n",
    "fdom_NAP_index_lookup = get_cands_fDOM_NAP()\n",
    "\n",
    "#####################################################################################\n",
    "#                                                                                   #\n",
    "#                               AUGMENT DATA SECTION                                #\n",
    "#                                                                                   #\n",
    "#####################################################################################\n",
    "\n",
    "for iteration in range(ITERATIONS):\n",
    "    \"\"\"\n",
    "    Re-sample the fDOM labeled peaks to add variance to data\n",
    "    \"\"\"\n",
    "    # labeled fDOM peaks\n",
    "    fdom_anon_peaks = fDOM_labeled[fDOM_labeled[\"label_of_peak\"] != \"NAP\"]\n",
    "    # randomize the order, to add more variability to data\n",
    "    fdom_anon_peaks = fdom_anon_peaks.sample(frac=1).reset_index(\n",
    "        drop=True\n",
    "    )  # reset index as values were removed\n",
    "\n",
    "    # labeled non anomaly fDOM peaks\n",
    "    fdom_NAP = fDOM_labeled[fDOM_labeled[\"label_of_peak\"] == \"NAP\"]\n",
    "    fdom_NAP = fdom_NAP.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # iterate over each peak\n",
    "    for i, row in fdom_anon_peaks.iterrows():\n",
    "        # Get raw fDOM data points\n",
    "        label_of_peak = fdom_anon_peaks.loc[i, \"label_of_peak\"]\n",
    "\n",
    "        prev_dist = TIME_RANGE_INIT\n",
    "        next_dist = TIME_RANGE_INIT\n",
    "\n",
    "        prev_dist_NAP = TIME_RANGE_INIT\n",
    "        next_dist_NAP = TIME_RANGE_INIT\n",
    "\n",
    "        # set cands_df based on our peak label\n",
    "        if label_of_peak == \"PP\":\n",
    "            cands_df = fdom_pp_index_lookup\n",
    "        elif label_of_peak == \"PLP\":\n",
    "            cands_df = fdom_plp_index_lookup\n",
    "        elif label_of_peak == \"SKP\":\n",
    "            cands_df = fdom_skp_index_lookup\n",
    "\n",
    "        timestamp_of_peak = fdom_anon_peaks.loc[i, \"timestamp_of_peak\"]\n",
    "\n",
    "        NAP_index = fdom_NAP.loc[i, \"idx_of_peak\"]\n",
    "        timestamp_NAP = fDOM_raw.loc[NAP_index, \"timestamp\"]\n",
    "\n",
    "        # get index dataframes of each type\n",
    "        fdom_index_df = fDOM_raw[fDOM_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "        stage_index_df = stage_raw[stage_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "        turb_index_df = turb_raw[turb_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "\n",
    "        # get indices of non anomaly data\n",
    "        stage_index_df_NAP = stage_raw[stage_raw[\"timestamp\"] == timestamp_NAP]\n",
    "        turb_index_df_NAP = turb_raw[turb_raw[\"timestamp\"] == timestamp_NAP]\n",
    "\n",
    "        if len(fdom_index_df.index.to_list()) != 0:\n",
    "            # get indices of each data type from index df's\n",
    "            index_of_peak = fdom_index_df.index.tolist()[0]\n",
    "            stage_index = stage_index_df.index.tolist()[0]\n",
    "            turb_index = turb_index_df.index.tolist()[0]\n",
    "\n",
    "            # get indices of NAP (might break, try except to catch that)\n",
    "            stage_index_NAP = stage_index_df_NAP.index.tolist()[0]\n",
    "            turb_index_NAP = turb_index_df_NAP.index.tolist()[0]\n",
    "\n",
    "            # call get ends of peak to get beginning and ending of peaks\n",
    "            try:\n",
    "                left, right = augment.get_ends_of_peak(cands_df, index_of_peak)\n",
    "\n",
    "                prev_dist = abs(index_of_peak - left)\n",
    "                next_dist = abs(index_of_peak - right)\n",
    "\n",
    "            except IndexError as e:\n",
    "                # something went wrong, just set prev_dist and next dist to time range init\n",
    "                prev_dist = TIME_RANGE_INIT\n",
    "                next_dist = TIME_RANGE_INIT\n",
    "                print(e)\n",
    "\n",
    "            # try except for nap\n",
    "            try:\n",
    "                left_NAP, right_NAP = augment.get_ends_of_peak(\n",
    "                    fdom_NAP_index_lookup, NAP_index\n",
    "                )\n",
    "\n",
    "                prev_dist_NAP = abs(NAP_index - left_NAP)\n",
    "                next_dist_NAP = abs(NAP_index - right_NAP)\n",
    "\n",
    "            except IndexError as e:\n",
    "                prev_dist_NAP = TIME_RANGE_INIT\n",
    "                next_dist_NAP = TIME_RANGE_INIT\n",
    "                print(\"INDEX ERROR: \" + str(e))\n",
    "\n",
    "            # use this timestamp to make a dataframe of raw stuff\n",
    "            # get data from fDOM_raw file\n",
    "            # call build temp dataframes to build df's:\n",
    "            new_fdom_raw, new_stage, new_turb_raw = augment.build_temp_dataframes(\n",
    "                fDOM_raw,\n",
    "                stage_raw,\n",
    "                turb_raw,\n",
    "                prev_dist,\n",
    "                next_dist,\n",
    "                index_of_peak,\n",
    "                stage_index,\n",
    "                turb_index,\n",
    "            )\n",
    "\n",
    "            (\n",
    "                new_fdom_raw_NAP,\n",
    "                new_stage_NAP,\n",
    "                new_turb_raw_NAP,\n",
    "            ) = augment.build_temp_dataframes(\n",
    "                fDOM_raw,\n",
    "                stage_raw,\n",
    "                turb_raw,\n",
    "                prev_dist_NAP,\n",
    "                next_dist_NAP,\n",
    "                NAP_index,\n",
    "                stage_index_NAP,\n",
    "                turb_index_NAP,\n",
    "            )\n",
    "\n",
    "            #####################################################################################\n",
    "            #                                                                                   #\n",
    "            #                               ACTUAL CHANGES TO DATA                              #\n",
    "            #                                                                                   #\n",
    "            #####################################################################################\n",
    "\n",
    "            # decide whether to augment Non anomaly peak first, or anomaly peak first\n",
    "            which = random.randint(0, 1)\n",
    "\n",
    "            # if which is 0, augment NAP first\n",
    "            if which == 0:\n",
    "\n",
    "                # ~~~~~~~~~~~ AUGMENTING NAP ~~~~~~~~~\n",
    "                # augment data\n",
    "                new_fdom_raw_NAP = augment.augment_data(\n",
    "                    new_fdom_raw_NAP,\n",
    "                    NAP_index,\n",
    "                    LOWER_BOUND_AMPLITUDE_MULTIPLIER,\n",
    "                    UPPER_BOUND_AMPLITUDE_MULTIPLIER,\n",
    "                )\n",
    "\n",
    "                # SMOOTH DATA\n",
    "                # ensure that main augmented df has more than 1 row, else no data to smooth\n",
    "                if augmented_fDOM_raw.shape[0] > 1:\n",
    "                    (\n",
    "                        augmented_fDOM_raw,\n",
    "                        augmented_stage_raw_fdom,\n",
    "                        augmented_turb_raw_fdom,\n",
    "                        prev_added_entry,\n",
    "                    ) = augment.smooth_data(\n",
    "                        augmented_fDOM_raw,\n",
    "                        augmented_stage_raw_fdom,\n",
    "                        augmented_turb_raw_fdom,\n",
    "                        prev_added_entry,\n",
    "                        SMOOTH_LOWER_BOUND,\n",
    "                        SMOOTH_UPPER_BOUND,\n",
    "                        FLAT_FDOM_VAL,\n",
    "                        FLAT_TURB_VAL,\n",
    "                        FLAT_STAGE_VAL,\n",
    "                    )\n",
    "\n",
    "                (\n",
    "                    new_label_NAP,\n",
    "                    new_fdom_raw_NAP,\n",
    "                    new_stage_NAP,\n",
    "                    new_turb_raw_NAP,\n",
    "                    prev_added_entry,\n",
    "                ) = augment.update_dataframes(\n",
    "                    prev_added_entry,\n",
    "                    new_fdom_raw_NAP,\n",
    "                    NAP_index,\n",
    "                    prev_dist_NAP,\n",
    "                    augmented_fDOM_raw,\n",
    "                    new_stage_NAP,\n",
    "                    new_turb_raw_NAP,\n",
    "                    \"NAP\",  # NAP label is just NAP\n",
    "                )\n",
    "\n",
    "                # call concat dataframes to put the rest together\n",
    "                (\n",
    "                    augmented_fDOM_labeled,\n",
    "                    augmented_fDOM_raw,\n",
    "                    augmented_stage_raw_fdom,\n",
    "                    augmented_turb_raw_fdom,\n",
    "                ) = augment.concat_dataframes(\n",
    "                    augmented_fDOM_labeled,\n",
    "                    augmented_fDOM_raw,\n",
    "                    augmented_stage_raw_fdom,\n",
    "                    augmented_turb_raw_fdom,\n",
    "                    new_fdom_raw_NAP,\n",
    "                    new_stage_NAP,\n",
    "                    new_turb_raw_NAP,\n",
    "                    new_label_NAP,\n",
    "                )\n",
    "\n",
    "                ####### AUGMENT ANOMALY PEAK #######\n",
    "\n",
    "                # call augment data function\n",
    "                new_fdom_raw = augment.augment_data(\n",
    "                    new_fdom_raw,\n",
    "                    index_of_peak,\n",
    "                    LOWER_BOUND_AMPLITUDE_MULTIPLIER,\n",
    "                    UPPER_BOUND_AMPLITUDE_MULTIPLIER,\n",
    "                )\n",
    "\n",
    "                # SMOOTH DATA\n",
    "                (\n",
    "                    augmented_fDOM_raw,\n",
    "                    augmented_stage_raw_fdom,\n",
    "                    augmented_turb_raw_fdom,\n",
    "                    prev_added_entry,\n",
    "                ) = augment.smooth_data(\n",
    "                    augmented_fDOM_raw,\n",
    "                    augmented_stage_raw_fdom,\n",
    "                    augmented_turb_raw_fdom,\n",
    "                    prev_added_entry,\n",
    "                    SMOOTH_LOWER_BOUND,\n",
    "                    SMOOTH_UPPER_BOUND,\n",
    "                    FLAT_FDOM_VAL,\n",
    "                    FLAT_TURB_VAL,\n",
    "                    FLAT_STAGE_VAL,\n",
    "                )\n",
    "\n",
    "                # call update dataframes to set new indices and timestamps\n",
    "                (\n",
    "                    new_label,\n",
    "                    new_fdom_raw,\n",
    "                    new_stage,\n",
    "                    new_turb_raw,\n",
    "                    prev_added_entry,\n",
    "                ) = augment.update_dataframes(\n",
    "                    prev_added_entry,\n",
    "                    new_fdom_raw,\n",
    "                    index_of_peak,\n",
    "                    prev_dist,\n",
    "                    augmented_fDOM_raw,\n",
    "                    new_stage,\n",
    "                    new_turb_raw,\n",
    "                    label_of_peak,\n",
    "                )\n",
    "\n",
    "                # concat the rest of the peaks\n",
    "                (\n",
    "                    augmented_fDOM_labeled,\n",
    "                    augmented_fDOM_raw,\n",
    "                    augmented_stage_raw_fdom,\n",
    "                    augmented_turb_raw_fdom,\n",
    "                ) = augment.concat_dataframes(\n",
    "                    augmented_fDOM_labeled,\n",
    "                    augmented_fDOM_raw,\n",
    "                    augmented_stage_raw_fdom,\n",
    "                    augmented_turb_raw_fdom,\n",
    "                    new_fdom_raw,\n",
    "                    new_stage,\n",
    "                    new_turb_raw,\n",
    "                    new_label,\n",
    "                )\n",
    "\n",
    "            # else, augment anomaly first\n",
    "            else:\n",
    "\n",
    "                ####### AUGMENT ANOMALY PEAK #######\n",
    "                # call augment data function\n",
    "                new_fdom_raw = augment.augment_data(\n",
    "                    new_fdom_raw,\n",
    "                    index_of_peak,\n",
    "                    LOWER_BOUND_AMPLITUDE_MULTIPLIER,\n",
    "                    UPPER_BOUND_AMPLITUDE_MULTIPLIER,\n",
    "                )\n",
    "\n",
    "                # SMOOTH DATA\n",
    "                # ensure that main augmented df has more than 1 row, else no data to smooth\n",
    "                if augmented_fDOM_raw.shape[0] > 1:\n",
    "                    (\n",
    "                        augmented_fDOM_raw,\n",
    "                        augmented_stage_raw_fdom,\n",
    "                        augmented_turb_raw_fdom,\n",
    "                        prev_added_entry,\n",
    "                    ) = augment.smooth_data(\n",
    "                        augmented_fDOM_raw,\n",
    "                        augmented_stage_raw_fdom,\n",
    "                        augmented_turb_raw_fdom,\n",
    "                        prev_added_entry,\n",
    "                        SMOOTH_LOWER_BOUND,\n",
    "                        SMOOTH_UPPER_BOUND,\n",
    "                        FLAT_FDOM_VAL,\n",
    "                        FLAT_TURB_VAL,\n",
    "                        FLAT_STAGE_VAL,\n",
    "                    )\n",
    "\n",
    "                # call update dataframes to set new indices and timestamps\n",
    "                (\n",
    "                    new_label,\n",
    "                    new_fdom_raw,\n",
    "                    new_stage,\n",
    "                    new_turb_raw,\n",
    "                    prev_added_entry,\n",
    "                ) = augment.update_dataframes(\n",
    "                    prev_added_entry,\n",
    "                    new_fdom_raw,\n",
    "                    index_of_peak,\n",
    "                    prev_dist,\n",
    "                    augmented_fDOM_raw,\n",
    "                    new_stage,\n",
    "                    new_turb_raw,\n",
    "                    label_of_peak,\n",
    "                )\n",
    "\n",
    "                # concat the rest of the peaks\n",
    "                (\n",
    "                    augmented_fDOM_labeled,\n",
    "                    augmented_fDOM_raw,\n",
    "                    augmented_stage_raw_fdom,\n",
    "                    augmented_turb_raw_fdom,\n",
    "                ) = augment.concat_dataframes(\n",
    "                    augmented_fDOM_labeled,\n",
    "                    augmented_fDOM_raw,\n",
    "                    augmented_stage_raw_fdom,\n",
    "                    augmented_turb_raw_fdom,\n",
    "                    new_fdom_raw,\n",
    "                    new_stage,\n",
    "                    new_turb_raw,\n",
    "                    new_label,\n",
    "                )\n",
    "\n",
    "                # ~~~~~~~~~~~ AUGMENTING NAP ~~~~~~~~~\n",
    "                # augment data\n",
    "                new_fdom_raw_NAP = augment.augment_data(\n",
    "                    new_fdom_raw_NAP,\n",
    "                    NAP_index,\n",
    "                    LOWER_BOUND_AMPLITUDE_MULTIPLIER,\n",
    "                    UPPER_BOUND_AMPLITUDE_MULTIPLIER,\n",
    "                )\n",
    "\n",
    "                # SMOOTH DATA\n",
    "                (\n",
    "                    augmented_fDOM_raw,\n",
    "                    augmented_stage_raw_fdom,\n",
    "                    augmented_turb_raw_fdom,\n",
    "                    prev_added_entry,\n",
    "                ) = augment.smooth_data(\n",
    "                    augmented_fDOM_raw,\n",
    "                    augmented_stage_raw_fdom,\n",
    "                    augmented_turb_raw_fdom,\n",
    "                    prev_added_entry,\n",
    "                    SMOOTH_LOWER_BOUND,\n",
    "                    SMOOTH_UPPER_BOUND,\n",
    "                    FLAT_FDOM_VAL,\n",
    "                    FLAT_TURB_VAL,\n",
    "                    FLAT_STAGE_VAL,\n",
    "                )\n",
    "\n",
    "                (\n",
    "                    new_label_NAP,\n",
    "                    new_fdom_raw_NAP,\n",
    "                    new_stage_NAP,\n",
    "                    new_turb_raw_NAP,\n",
    "                    prev_added_entry,\n",
    "                ) = augment.update_dataframes(\n",
    "                    prev_added_entry,\n",
    "                    new_fdom_raw_NAP,\n",
    "                    NAP_index,\n",
    "                    prev_dist_NAP,\n",
    "                    augmented_fDOM_raw,\n",
    "                    new_stage_NAP,\n",
    "                    new_turb_raw_NAP,\n",
    "                    \"NAP\",  # NAP label is just NAP\n",
    "                )\n",
    "\n",
    "                # call concat dataframes to put the rest together\n",
    "                (\n",
    "                    augmented_fDOM_labeled,\n",
    "                    augmented_fDOM_raw,\n",
    "                    augmented_stage_raw_fdom,\n",
    "                    augmented_turb_raw_fdom,\n",
    "                ) = augment.concat_dataframes(\n",
    "                    augmented_fDOM_labeled,\n",
    "                    augmented_fDOM_raw,\n",
    "                    augmented_stage_raw_fdom,\n",
    "                    augmented_turb_raw_fdom,\n",
    "                    new_fdom_raw_NAP,\n",
    "                    new_stage_NAP,\n",
    "                    new_turb_raw_NAP,\n",
    "                    new_label_NAP,\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            # we missed some data points, append them to the missed data dataframe\n",
    "            missed_fDOM_peaks.append(timestamp_of_peak)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment turbidity data\n",
    "The following code blocks augment turbidity data.\n",
    "NOTE: This is currently nonfunctional, and needs to be rewritten using the new functions written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "#                                                                                   #\n",
    "#                               DATAFRAME SETUP SECTION                             #\n",
    "#                                                                                   #\n",
    "#####################################################################################\n",
    "\n",
    "# TODO: REDO THIS USING NEW FUNCTIONS\n",
    "# FIXME: This is currently broken, the peak value is wrong\n",
    "\n",
    "# labeled turb\n",
    "augmented_turb_labeled = pd.DataFrame(\n",
    "    columns=[\"timestamp_of_peak\", \"value_of_peak\", \"label_of_peak\", \"idx_of_peak\"]\n",
    ")\n",
    "\n",
    "# raw turb\n",
    "augmented_turb_raw = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "\n",
    "# raw fdom\n",
    "augmented_fDOM_raw_turb = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "\n",
    "# raw stage\n",
    "augmented_stage_raw_turb = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "\n",
    "# variable to keep the last entry in the dataframe for stage\n",
    "# defaults to the last entry that was in fdom/turb raw csv files, in julian format\n",
    "prev_added_entry = STARTING_TIMESTAMP\n",
    "\n",
    "# a list of peaks that don't align with the fDOM raw file that was aligned with stage\n",
    "# i believe its just peaks that don't align with stage in general for whatever reason\n",
    "missed_turb_peaks = []\n",
    "\n",
    "# ~~~~~ Collect starting and ending points of each turb peak type ~~~~~~\n",
    "turb_pp_index_lookup = get_cands_turb_PP()\n",
    "\n",
    "\n",
    "new_peak_val = 5\n",
    "\n",
    "#####################################################################################\n",
    "#                                                                                   #\n",
    "#                               AUGMENT DATA SECTION                                #\n",
    "#                                                                                   #\n",
    "#####################################################################################\n",
    "\n",
    "\"\"\" Augment turbidity data by calling previously written function \"\"\"\n",
    "for iteration in range(ITERATIONS):\n",
    "    \"\"\" Resample turb labeled peaks at each iteration for more variance \"\"\"\n",
    "    # labeled turb peaks\n",
    "    turb_anon_peaks = turb_labeled[turb_labeled[\"label_of_peak\"] != \"NPP\"]\n",
    "    turb_anon_peaks = turb_anon_peaks.sample(frac=1).reset_index()\n",
    "\n",
    "    # iterate over each peak\n",
    "    for i, row in turb_anon_peaks.iterrows():\n",
    "\n",
    "        \"\"\"Get raw turb data points\"\"\"\n",
    "        timestamp_of_peak = turb_anon_peaks.loc[i, \"timestamp_of_peak\"]\n",
    "        label_of_peak = turb_anon_peaks.loc[i, \"label_of_peak\"]\n",
    "\n",
    "        # get index dataframes of each type\n",
    "        fdom_index_df = fDOM_raw[fDOM_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "        stage_index_df = stage_raw[stage_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "        turb_index_df = turb_raw[turb_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "\n",
    "        if len(turb_index_df.index.to_list()) != 0:\n",
    "            # get indices of each data type from index df's\n",
    "            index_of_peak = turb_index_df.index.tolist()[0]\n",
    "            stage_index = stage_index_df.index.tolist()[0]\n",
    "            fdom_index = fdom_index_df.index.tolist()[0]\n",
    "\n",
    "            # check to see if any overlap occurs between peaks\n",
    "            prev_dist = TIME_RANGE_INIT\n",
    "            next_dist = TIME_RANGE_INIT\n",
    "\n",
    "            if label_of_peak == \"PP\":\n",
    "                cands_df = turb_pp_index_lookup\n",
    "\n",
    "            try:\n",
    "                left, right = augment.get_ends_of_peak(cands_df, index_of_peak)\n",
    "\n",
    "                prev_dist = abs(index_of_peak - left)\n",
    "                next_dist = abs(index_of_peak - right)\n",
    "\n",
    "            except IndexError:\n",
    "                # index wrong, reset prev dist\n",
    "                print(\"INDEX ERROR!\")\n",
    "                prev_dist = TIME_RANGE_INIT\n",
    "                next_dist = TIME_RANGE_INIT \n",
    "\n",
    "            # use this timestamp to make a dataframe of raw stuff\n",
    "            # get data from fDOM_raw file\n",
    "            fDOM_raw_time_range = pd.DataFrame(\n",
    "                fDOM_raw.iloc[fdom_index - prev_dist : fdom_index + next_dist + 1]\n",
    "            )\n",
    "\n",
    "            # get stage data range\n",
    "            stage_time_range = pd.DataFrame(\n",
    "                stage_raw.iloc[stage_index - prev_dist : stage_index + next_dist + 1]\n",
    "            )\n",
    "\n",
    "            # get turbidity data range\n",
    "            turb_time_range = pd.DataFrame(\n",
    "                turb_raw.iloc[index_of_peak - prev_dist : index_of_peak + next_dist + 1]\n",
    "            )\n",
    "\n",
    "            # make a copy of the modified data\n",
    "            new_fdom_raw = copy.deepcopy(fDOM_raw_time_range)\n",
    "            new_stage = copy.deepcopy(stage_time_range)\n",
    "            new_turb_raw = copy.deepcopy(turb_time_range)\n",
    "\n",
    "            # peak index can change when we add in x data\n",
    "            new_fdom_peak_index = -1\n",
    "            new_peak_timestamp = -1\n",
    "            \n",
    "            #####################################################################################\n",
    "            #                                                                                   #\n",
    "            #                               ACTUAL CHANGES TO DATA                              #\n",
    "            #                                                                                   #\n",
    "            #####################################################################################\n",
    "\n",
    "            # gen random num to decide whether to widen or heighten\n",
    "            random.seed() \n",
    "            widen_or_heighten = random.randint(0, 1)\n",
    "\n",
    "            # if 0, widen the peak\n",
    "            if widen_or_heighten == 0:\n",
    "               # get current peak val\n",
    "                main_peak_val = new_turb_raw.loc[index_of_peak, \"value\"]\n",
    "\n",
    "                # gen a random number between 0.01 and 0.2 to increase height by for previous and next peak\n",
    "                multiplier = random.uniform(0.001, 0.2)\n",
    "                \n",
    "                # new peak vals\n",
    "                val_before_peak = new_turb_raw.loc[index_of_peak - 1, \"value\"] * (1 + multiplier)\n",
    "                val_after_peak = new_turb_raw.loc[index_of_peak + 1, \"value\"] * (1 + multiplier)\n",
    "\n",
    "                # ensure new value does not go over main peak val\n",
    "                fail_count = 0\n",
    "                can_widen = True\n",
    "                while val_before_peak > main_peak_val and val_after_peak > main_peak_val:\n",
    "                    fail_count += 1 \n",
    "                    if fail_count > 20:\n",
    "                        # not possible to fix peak, break and forget about it\n",
    "                        can_widen = False\n",
    "                        break \n",
    "                    \n",
    "                    # re gen num\n",
    "                    multiplier = random.uniform(0.001, 0.2)\n",
    "                    val_before_peak = new_turb_raw.loc[index_of_peak - 1, \"value\"] * (1 + multiplier)\n",
    "                    val_after_peak = new_turb_raw.loc[index_of_peak + 1, \"value\"] * (1 + multiplier) \n",
    "                \n",
    "                if can_widen:\n",
    "                    # set new vals\n",
    "                    new_turb_raw.loc[index_of_peak - 1, \"value\"] = val_before_peak\n",
    "                    new_turb_raw.loc[index_of_peak + 1, \"value\"] = val_after_peak \n",
    "            \n",
    "            # else, lets heighten the peak\n",
    "            else:\n",
    "                random_val = random.uniform(LOWER_BOUND_AMPLITUDE_MULTIPLIER, UPPER_BOUND_AMPLITUDE_MULTIPLIER)\n",
    "                new_peak_val = new_turb_raw.loc[index_of_peak, \"value\"] * (1 + random_val)\n",
    "                new_turb_raw.loc[index_of_peak, \"value\"] = new_peak_val\n",
    "\n",
    "            # get the next possible timestamp\n",
    "            new_time_entry = augment.next_time_entry(prev_added_entry)\n",
    "\n",
    "            # update all timestamps for augmented data\n",
    "            for i, row in new_turb_raw.iterrows():\n",
    "                # if timestamps equal, we have the relative peak\n",
    "                if new_turb_raw.loc[i, \"timestamp\"] == timestamp_of_peak:\n",
    "                    # register index here\n",
    "                    new_turb_peak_index = augment.get_last_augment_index(augmented_turb_raw)\n",
    "                    new_peak_timestamp = new_time_entry\n",
    "\n",
    "                # update timestamps\n",
    "                new_fdom_raw.loc[i, \"timestamp\"] = new_time_entry\n",
    "                new_stage.loc[i, \"timestamp\"] = new_time_entry\n",
    "                new_turb_raw.loc[i, \"timestamp\"] = new_time_entry\n",
    "\n",
    "                # get next time stamp\n",
    "                new_time_entry = augment.next_time_entry(new_time_entry)\n",
    "\n",
    "            # add entries to turb\n",
    "            augmented_turb_raw = pd.concat(\n",
    "                [augmented_turb_raw, new_turb_raw], ignore_index=True\n",
    "            )\n",
    "\n",
    "            new_label = pd.DataFrame(\n",
    "                [\n",
    "                    [\n",
    "                        new_peak_timestamp,\n",
    "                        new_peak_val,\n",
    "                        label_of_peak,\n",
    "                        new_turb_peak_index,\n",
    "                    ]\n",
    "                ],\n",
    "                columns=[\n",
    "                    \"timestamp_of_peak\",\n",
    "                    \"value_of_peak\",\n",
    "                    \"label_of_peak\",\n",
    "                    \"idx_of_peak\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            # add entries to labeled turb\n",
    "            augmented_turb_labeled = pd.concat([augmented_turb_labeled, new_label])\n",
    "\n",
    "            # add entries to stage\n",
    "            augmented_stage_raw_turb = pd.concat(\n",
    "                [augmented_stage_raw_turb, new_stage], ignore_index=True\n",
    "            )\n",
    "\n",
    "            # add entries into raw fDOM\n",
    "            augmented_fDOM_raw_turb = pd.concat(\n",
    "                [augmented_fDOM_raw_turb, new_fdom_raw], ignore_index=True\n",
    "            )\n",
    "\n",
    "            # update prev time entry\n",
    "            prev_added_entry = new_time_entry\n",
    "\n",
    "        else:\n",
    "            # we missed some data points, append them to the missed data dataframe\n",
    "            missed_fDOM_peaks.append(timestamp_of_peak)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move augmented data into csv files\n",
    "The following codeblock creates csv files for the augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Augmented Data Paths \"\"\"\n",
    "# trainset\n",
    "trainset_fdom_path = \"../Data/augmented_data/trainset_plotting/fdom/\"\n",
    "trainset_turb_path = \"../Data/augmented_data/trainset_plotting/turb/\"\n",
    "\n",
    "# unlabeled data\n",
    "unlabeled_fdom_path = \"../Data/augmented_data/fdom/unlabeled/\"\n",
    "unlabeled_turb_path = \"../Data/augmented_data/turb/unlabeled/\"\n",
    "\n",
    "# labeled data\n",
    "labeled_fdom_path = \"../Data/augmented_data/fdom/labeled/\"\n",
    "labeled_turb_path = \"../Data/augmented_data/turb/labeled/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_augmented_data_to_csv():\n",
    "    # call to_csv for each dataframe\n",
    "    # for each dataframe, we also drop the index\n",
    "\n",
    "    # write fDOM augmented data\n",
    "    augmented_fDOM_labeled.to_csv(labeled_fdom_path + 'labeled_fdom_peaks.csv', index=False)\n",
    "    augmented_fDOM_raw.to_csv(unlabeled_fdom_path + 'unlabeled_fdom.csv', index=False)\n",
    "    augmented_turb_raw_fdom.to_csv(unlabeled_fdom_path + 'unlabeled_turb.csv', index=False)\n",
    "    augmented_stage_raw_fdom.to_csv(unlabeled_fdom_path + 'unlabeled_stage.csv', index=False)\n",
    "\n",
    "    # write turb augmented data\n",
    "    augmented_turb_labeled.to_csv(labeled_turb_path + 'labeled_turb_peaks.csv', index=False)\n",
    "    augmented_turb_raw.to_csv(unlabeled_turb_path + 'unlabeled_turb.csv', index=False)\n",
    "    augmented_fDOM_raw_turb.to_csv(unlabeled_turb_path + 'unlabeled_fdom.csv', index=False)\n",
    "    augmented_stage_raw_turb.to_csv(unlabeled_turb_path + 'unlabeled_stage.csv', index=False)\n",
    "\n",
    "\n",
    "def convert_df_julian_to_datetime(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts a dataframe julian timestamp to datetime ISO 8601 format\n",
    "\n",
    "    df: a dataframe\n",
    "\n",
    "    return: changed dataframe\n",
    "    \"\"\"\n",
    "    # iterate over dataframe, replacing timestamp vals\n",
    "    for i, row in df.iterrows():\n",
    "        df.loc[i, \"timestamp\"] = dp.julian_to_datetime(df.loc[i, \"timestamp\"]).isoformat()\n",
    "        \n",
    "        # add stupid 0.00Z to fit trainset format\n",
    "        df.loc[i, \"timestamp\"] = df.loc[i, \"timestamp\"] + \".000Z\"\n",
    "\n",
    "    return df\n",
    "\n",
    "def write_to_trainset_csv():\n",
    "    # TODO: add peak labels in\n",
    "\n",
    "    # start by creating a dataframe that has the correct columns\n",
    "    trainset_fdom_df = pd.DataFrame(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "    trainset_turb_df = pd.DataFrame(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "\n",
    "    # ~~~~~~~ fDOM section ~~~~~~~\n",
    "    # start by just adding the fDOM data into the series, need to replace all timestamps\n",
    "    fdom_trainset_raw = copy.deepcopy(augmented_fDOM_raw)\n",
    "    fdom_turb_trainset_raw = copy.deepcopy(augmented_turb_raw_fdom)\n",
    "    fdom_stage_trainset_raw = copy.deepcopy(augmented_stage_raw_fdom)\n",
    "    \n",
    "    # convert timestamps to julian\n",
    "    fdom_trainset_raw = convert_df_julian_to_datetime(fdom_trainset_raw)\n",
    "    fdom_turb_trainset_raw = convert_df_julian_to_datetime(fdom_turb_trainset_raw)\n",
    "    fdom_stage_trainset_raw = convert_df_julian_to_datetime(fdom_stage_trainset_raw)\n",
    "\n",
    "    # add in new values\n",
    "    fdom_trainset_raw[\"series\"] = \"fDOM\"\n",
    "    fdom_trainset_raw[\"label\"] = \"\"\n",
    "\n",
    "    fdom_turb_trainset_raw[\"series\"] = \"turb\"\n",
    "    fdom_turb_trainset_raw[\"label\"] = \"\"\n",
    "\n",
    "    fdom_stage_trainset_raw[\"series\"] = \"stage\"\n",
    "    fdom_stage_trainset_raw[\"label\"] = \"\"\n",
    "\n",
    "    # reorder columns\n",
    "    fdom_trainset_raw = fdom_trainset_raw.reindex(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "    fdom_turb_trainset_raw = fdom_turb_trainset_raw.reindex(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "    fdom_stage_trainset_raw = fdom_stage_trainset_raw.reindex(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "\n",
    "    # concat into single dataframe\n",
    "    trainset_fdom_df = pd.concat([fdom_trainset_raw, fdom_turb_trainset_raw, fdom_stage_trainset_raw])\n",
    "\n",
    "    # sort together\n",
    "    trainset_fdom_df = trainset_fdom_df.sort_values(by=['timestamp'], kind='stable')\n",
    "\n",
    "    # export to csv\n",
    "    trainset_fdom_df.to_csv(trainset_fdom_path + \"fdom_augmented.csv\", index=False)\n",
    "\n",
    "    # ~~~~~~~ turbidity section ~~~~~~~\n",
    "\n",
    "    # create new dataframes\n",
    "    turb_trainset_raw = copy.deepcopy(augmented_turb_raw)\n",
    "    turb_fdom_trainset_raw = copy.deepcopy(augmented_fDOM_raw_turb)\n",
    "    turb_stage_trainset_raw = copy.deepcopy(augmented_stage_raw_turb)\n",
    "\n",
    "    # convert timestamps\n",
    "    turb_trainset_raw = convert_df_julian_to_datetime(turb_trainset_raw)\n",
    "    turb_fdom_trainset_raw = convert_df_julian_to_datetime(turb_fdom_trainset_raw)\n",
    "    turb_stage_trainset_raw = convert_df_julian_to_datetime(turb_stage_trainset_raw)\n",
    "\n",
    "    # add in new values\n",
    "    turb_fdom_trainset_raw[\"series\"] = \"fDOM\"\n",
    "    turb_fdom_trainset_raw[\"label\"] = \"\"\n",
    "\n",
    "    turb_trainset_raw[\"series\"] = \"turb\"\n",
    "    turb_trainset_raw[\"label\"] = \"\"\n",
    "\n",
    "    turb_stage_trainset_raw[\"series\"] = \"stage\"\n",
    "    turb_stage_trainset_raw[\"label\"] = \"\"\n",
    "\n",
    "    # reorder columns\n",
    "    turb_fdom_trainset_raw = turb_fdom_trainset_raw.reindex(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "    turb_trainset_raw = turb_trainset_raw.reindex(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "    turb_stage_trainset_raw = turb_stage_trainset_raw.reindex(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "\n",
    "    # concat into single dataframe\n",
    "    trainset_turb_df = pd.concat([turb_fdom_trainset_raw, turb_trainset_raw, turb_stage_trainset_raw])\n",
    "\n",
    "    # sort together\n",
    "    trainset_turb_df = trainset_turb_df.sort_values(by=['timestamp'], kind='stable')\n",
    "\n",
    "    # export to csv\n",
    "    trainset_turb_df.to_csv(trainset_turb_path + \"turb_augmented.csv\", index=False)\n",
    "\n",
    "# write_augmented_data_to_csv()\n",
    "write_to_trainset_csv()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ed753961bdc37ee89b4275051722ceb8ec0b57b8793db9d189305c313070a7d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('srrw': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
