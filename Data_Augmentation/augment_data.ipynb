{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenting time-series data\n",
    "In this file, the data is augmented in order to create more of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.insert(1,'../')\n",
    "import Tools.data_processing as dp\n",
    "import Tools.data_movement as dm \n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import copy\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.signal import find_peaks\n",
    "import numpy as np\n",
    "import scipy.ndimage as ndimage\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "The following functions provide useful tools for the augmentation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_time_entry(current_entry: float) -> float:\n",
    "    \"\"\"\n",
    "    This function returns the next time entry in julian time\n",
    "\n",
    "    current_entry: a julina time float\n",
    "\n",
    "    return: julian time + 15 minutes from past julian time\n",
    "    \"\"\"\n",
    "\n",
    "    # convert julian to datetime\n",
    "    date_time_init = dp.julian_to_datetime(current_entry)\n",
    "\n",
    "    # find next date time (add 15 minutes)\n",
    "    next_entry = date_time_init + timedelta(minutes=15)\n",
    "\n",
    "    # convert date time to julian time\n",
    "    final_julian_time = dp.datetime_to_julian(next_entry)\n",
    "\n",
    "    # return julian time\n",
    "    return final_julian_time\n",
    "\n",
    "\n",
    "def reindex_augmented_data(data: pd.DataFrame, datatype: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reindex the augmented data so there are no overlaps\n",
    "\n",
    "    data: the data to reindex\n",
    "    datatype: fdom, turb, or stage\n",
    "\n",
    "    returns: reindexed data\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_last_augment_index(dataframe) -> int:\n",
    "    \"\"\"\n",
    "    Collects the last index of the augmented time series\n",
    "    \"\"\"\n",
    "    return dataframe.shape[0]\n",
    "\n",
    "\n",
    "# from auxuilary functions file\n",
    "def get_candidates(data: np.ndarray, params: dict):\n",
    "\n",
    "    \"\"\"\n",
    "    Return all peaks that should be scanned for out or order peaks(oop)\n",
    "    We don't want to return skyrocketing peaks/local fluctuations - although they are oop,\n",
    "    they will be caught by their respective algorithms\n",
    "\n",
    "    data   : timeseries to scan for peaks\n",
    "    return : peaks indentified with given hyperparameters and properties of those peaks\n",
    "    \"\"\"\n",
    "    peaks, props = find_peaks(\n",
    "        data[:, 1],\n",
    "        height=(None, None),\n",
    "        threshold=(None, None),\n",
    "        distance=params[\"dist\"],\n",
    "        prominence=params[\"prom\"],\n",
    "        width=params[\"width\"],\n",
    "        wlen=params[\"wlen\"],\n",
    "        rel_height=params[\"rel_h\"],\n",
    "    )\n",
    "    return peaks, props\n",
    "\n",
    "\n",
    "def get_ends_of_peak(cands_df: pd.DataFrame, peak_index):\n",
    "    \"\"\"\n",
    "    get left and right ends of a peak from the respective dataframes\n",
    "\n",
    "    peak_dataset: fdom or turb\n",
    "\n",
    "    peak_type: PP, SKP, PLP, etc.\n",
    "    \"\"\"\n",
    "    # use cands_df to return left and right base of peak index\n",
    "    new_cands = copy.deepcopy(cands_df)\n",
    "    new_cands = new_cands.loc[new_cands[\"idx_of_peak\"] == peak_index]\n",
    "\n",
    "    left_base = new_cands[\"left_base\"]\n",
    "    right_base = new_cands[\"right_base\"]\n",
    "\n",
    "    left_base = left_base.to_list()\n",
    "    left_base = left_base[0]\n",
    "\n",
    "    right_base = right_base.to_list()\n",
    "    right_base = right_base[0]\n",
    "\n",
    "    # return left and right\n",
    "    return left_base, right_base\n",
    "\n",
    "\n",
    "def isInRange(indx, remove_ranges):\n",
    "    for rng in remove_ranges:\n",
    "        if rng[0] <= indx and indx <= rng[1]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def low_pass_filter(data, window_len):\n",
    "    \"\"\"\n",
    "    (2 * window_len) + 1 is the size of the window that determines the values that\n",
    "    influence the current measurement (middle of window)\n",
    "    \"\"\"\n",
    "    kernel = np.lib.pad(np.linspace(1, 3, window_len), (0, window_len - 1), \"reflect\")\n",
    "    kernel = np.divide(kernel, np.sum(kernel))\n",
    "    return ndimage.convolve(data, kernel)\n",
    "\n",
    "\n",
    "def get_cands_fdom_pp():\n",
    "    # pass fDOM data through low pass filter\n",
    "    fDOM_data = dm.read_in_preprocessed_timeseries(\n",
    "        \"../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv\"\n",
    "    )\n",
    "    smoothed_signal = low_pass_filter(fDOM_data[:, 1], 7)\n",
    "    fDOM_data = np.column_stack((fDOM_data[:, 0], smoothed_signal))\n",
    "\n",
    "    candidate_params = {\n",
    "        \"prom\": [3, None],\n",
    "        \"width\": [None, None],\n",
    "        \"wlen\": 200,\n",
    "        \"dist\": 1,\n",
    "        \"rel_h\": 0.6,\n",
    "    }\n",
    "\n",
    "    remove_ranges = [[17816, 17849], [108170, 108200], [111364, 111381]]\n",
    "\n",
    "    peaks, props = get_candidates(fDOM_data, candidate_params)\n",
    "\n",
    "    cands = [\n",
    "        [\n",
    "            peak,\n",
    "            math.floor(props[\"left_ips\"][i]),\n",
    "            math.ceil(props[\"right_ips\"][i]),\n",
    "            props[\"prominences\"][i],\n",
    "        ]\n",
    "        for i, peak in enumerate(peaks)\n",
    "    ]\n",
    "\n",
    "    # Remove erroneously detected peaks\n",
    "    temp = []\n",
    "    for peak in cands:\n",
    "        if not (isInRange(peak[0], remove_ranges)):\n",
    "            temp.append(peak)\n",
    "    cands = copy.deepcopy(temp)\n",
    "\n",
    "    df_cands_pp = pd.DataFrame(cands)\n",
    "\n",
    "    # now load in ground truths, and drop all things in cands that are not anomaly peaks\n",
    "    # Import ground truth values\n",
    "    truth_fname = \"../Data/labeled_data/ground_truths/fDOM/fDOM_PP/julian_time/fDOM_PP_0k-300k.csv\"\n",
    "\n",
    "    truths = pd.read_csv(truth_fname)\n",
    "\n",
    "    # drop all NPP indices\n",
    "    truths = truths[truths[\"label_of_peak\"] != \"NPP\"]\n",
    "\n",
    "    # drop all rows in cnads that are not in truths\n",
    "    df_cands_pp = df_cands_pp[df_cands_pp[0].isin(truths[\"idx_of_peak\"])]\n",
    "\n",
    "    # reindex frame\n",
    "    df_cands_pp = df_cands_pp.rename(\n",
    "        columns={0: \"idx_of_peak\", 1: \"left_base\", 2: \"right_base\", 3: \"amplitude\"}\n",
    "    )\n",
    "\n",
    "    # return frame\n",
    "    return df_cands_pp\n",
    "\n",
    "\n",
    "def get_cands_fdom_skp():\n",
    "    # load in fdom data\n",
    "    fDOM_data = dm.read_in_preprocessed_timeseries(\n",
    "        \"../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv\"\n",
    "    )\n",
    "\n",
    "    # find peaks\n",
    "    prominence_range = [5, None]\n",
    "    width_range = [None, None]\n",
    "    wlen = 100\n",
    "    distance = 1\n",
    "    rel_height = 0.6\n",
    "\n",
    "    # Get list of all peaks that could possibly be plummeting peaks\n",
    "    peaks, props = find_peaks(\n",
    "        fDOM_data[:, 1],\n",
    "        height=(None, None),\n",
    "        threshold=(None, None),\n",
    "        distance=distance,\n",
    "        prominence=prominence_range,\n",
    "        width=width_range,\n",
    "        wlen=wlen,\n",
    "        rel_height=rel_height,\n",
    "    )\n",
    "\n",
    "    # Form candidate set from returned information\n",
    "    cands = [\n",
    "        [\n",
    "            peak,\n",
    "            math.floor(props[\"left_ips\"][i]),\n",
    "            math.ceil(props[\"right_ips\"][i]),\n",
    "            props[\"prominences\"][i],\n",
    "        ]\n",
    "        for i, peak in enumerate(peaks)\n",
    "    ]\n",
    "\n",
    "    cands_df = pd.DataFrame(cands)\n",
    "\n",
    "    # import truths\n",
    "    truth_fname = \"../Data/labeled_data/ground_truths/fDOM/fDOM_SKP/julian_time/fDOM_SKP_0k-300k.csv\"\n",
    "\n",
    "    truths = pd.read_csv(truth_fname)\n",
    "\n",
    "    # drop all NSKP indices\n",
    "    truths = truths[truths[\"label_of_peak\"] != \"NSKP\"]\n",
    "\n",
    "    # drop all rows in cands that are not in truths\n",
    "    cands_df = cands_df[cands_df[0].isin(truths[\"idx_of_peak\"])]\n",
    "\n",
    "    # rename cols\n",
    "    cands_df = cands_df.rename(\n",
    "        columns={0: \"idx_of_peak\", 1: \"left_base\", 2: \"right_base\", 3: \"amplitude\"}\n",
    "    )\n",
    "\n",
    "    return cands_df\n",
    "\n",
    "\n",
    "def get_cands_fdom_plp():\n",
    "    # load data\n",
    "    fDOM_data = dm.read_in_preprocessed_timeseries(\n",
    "        \"../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv\"\n",
    "    )\n",
    "\n",
    "    # flip fdom\n",
    "    flipped_fDOM = dp.flip_timeseries(copy.deepcopy(fDOM_data))\n",
    "\n",
    "    # find peaks\n",
    "\n",
    "    # Get fDOM plummeting peak candidate set using scipy find_peaks()\n",
    "    prominence_range = [3, None]  # peaks must have at least prominence 3\n",
    "    width_range = [None, 10]  # peaks cannot have a base width of more than 5\n",
    "    wlen = 100\n",
    "    distance = 1\n",
    "    rel_height = 0.6\n",
    "\n",
    "    # Get list of all peaks that could possibly be plummeting peaks\n",
    "    peaks, props = find_peaks(\n",
    "        flipped_fDOM[:, 1],\n",
    "        height=(None, None),\n",
    "        threshold=(None, None),\n",
    "        distance=distance,\n",
    "        prominence=prominence_range,\n",
    "        width=width_range,\n",
    "        wlen=wlen,\n",
    "        rel_height=rel_height,\n",
    "    )\n",
    "\n",
    "    # Form candidate set from returned information\n",
    "    cands = [\n",
    "        [\n",
    "            peak,\n",
    "            math.floor(props[\"left_ips\"][i]),\n",
    "            math.ceil(props[\"right_ips\"][i]),\n",
    "            props[\"prominences\"][i],\n",
    "        ]\n",
    "        for i, peak in enumerate(peaks)\n",
    "    ]\n",
    "\n",
    "    cands_df = pd.DataFrame(cands)\n",
    "\n",
    "    # get truths\n",
    "    truth_fname = \"../Data/labeled_data/ground_truths/fDOM/fDOM_PLP/julian_time/fDOM_PLP_0k-300k.csv\"\n",
    "    truths = pd.read_csv(truth_fname)\n",
    "\n",
    "    # drop all NPLP indices\n",
    "    # TODO: if we want to allow non anomaly peaks, remove this line\n",
    "    truths = truths[truths[\"label_of_peak\"] != \"NPLP\"]\n",
    "\n",
    "    # drop all rows in cands not in truths\n",
    "    cands_df = cands_df[cands_df[0].isin(truths[\"idx_of_peak\"])]\n",
    "\n",
    "    # rename cols\n",
    "    cands_df = cands_df.rename(\n",
    "        columns={0: \"idx_of_peak\", 1: \"left_base\", 2: \"right_base\", 3: \"amplitude\"}\n",
    "    )\n",
    "\n",
    "    # return data\n",
    "    return cands_df\n",
    "\n",
    "def get_cands_fdom_nap():\n",
    "    \"\"\"\n",
    "    get candidates from non anomaly peak data in fdom\n",
    "    \"\"\"\n",
    "    # get all peaks from other data types, drop dupes, go from there\n",
    "\n",
    "    # get peaks from NPP:\n",
    "    # pass fDOM data through low pass filter\n",
    "    fDOM_data = dm.read_in_preprocessed_timeseries(\n",
    "        \"../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv\"\n",
    "    )\n",
    "    smoothed_signal = low_pass_filter(fDOM_data[:, 1], 7)\n",
    "    fDOM_data = np.column_stack((fDOM_data[:, 0], smoothed_signal))\n",
    "\n",
    "    candidate_params = {\n",
    "        \"prom\": [3, None],\n",
    "        \"width\": [None, None],\n",
    "        \"wlen\": 200,\n",
    "        \"dist\": 1,\n",
    "        \"rel_h\": 0.6,\n",
    "    }\n",
    "\n",
    "    remove_ranges = [[17816, 17849], [108170, 108200], [111364, 111381]]\n",
    "\n",
    "    peaks, props = get_candidates(fDOM_data, candidate_params)\n",
    "\n",
    "    cands = [\n",
    "        [\n",
    "            peak,\n",
    "            math.floor(props[\"left_ips\"][i]),\n",
    "            math.ceil(props[\"right_ips\"][i]),\n",
    "            props[\"prominences\"][i],\n",
    "        ]\n",
    "        for i, peak in enumerate(peaks)\n",
    "    ]\n",
    "\n",
    "    # Remove erroneously detected peaks\n",
    "    temp = []\n",
    "    for peak in cands:\n",
    "        if not (isInRange(peak[0], remove_ranges)):\n",
    "            temp.append(peak)\n",
    "    cands = copy.deepcopy(temp)\n",
    "\n",
    "    cands_npp = pd.DataFrame(cands)\n",
    "\n",
    "    # get peaks from NPLP:\n",
    "    # load data\n",
    "    fDOM_data = dm.read_in_preprocessed_timeseries(\n",
    "        \"../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv\"\n",
    "    )\n",
    "\n",
    "    # flip fdom\n",
    "    flipped_fDOM = dp.flip_timeseries(copy.deepcopy(fDOM_data))\n",
    "\n",
    "    # find peaks\n",
    "\n",
    "    # Get fDOM plummeting peak candidate set using scipy find_peaks()\n",
    "    prominence_range = [3, None]  # peaks must have at least prominence 3\n",
    "    width_range = [None, 10]  # peaks cannot have a base width of more than 5\n",
    "    wlen = 100\n",
    "    distance = 1\n",
    "    rel_height = 0.6\n",
    "\n",
    "    # Get list of all peaks that could possibly be plummeting peaks\n",
    "    peaks, props = find_peaks(\n",
    "        flipped_fDOM[:, 1],\n",
    "        height=(None, None),\n",
    "        threshold=(None, None),\n",
    "        distance=distance,\n",
    "        prominence=prominence_range,\n",
    "        width=width_range,\n",
    "        wlen=wlen,\n",
    "        rel_height=rel_height,\n",
    "    )\n",
    "\n",
    "    # Form candidate set from returned information\n",
    "    cands = [\n",
    "        [\n",
    "            peak,\n",
    "            math.floor(props[\"left_ips\"][i]),\n",
    "            math.ceil(props[\"right_ips\"][i]),\n",
    "            props[\"prominences\"][i],\n",
    "        ]\n",
    "        for i, peak in enumerate(peaks)\n",
    "    ]\n",
    "\n",
    "    cands_nplp = pd.DataFrame(cands)\n",
    "\n",
    "    # get peaks from NSKP:\n",
    "    # load in fdom data\n",
    "    fDOM_data = dm.read_in_preprocessed_timeseries(\n",
    "        \"../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv\"\n",
    "    )\n",
    "\n",
    "    # find peaks\n",
    "    prominence_range = [5, None]\n",
    "    width_range = [None, None]\n",
    "    wlen = 100\n",
    "    distance = 1\n",
    "    rel_height = 0.6\n",
    "\n",
    "    # Get list of all peaks that could possibly be plummeting peaks\n",
    "    peaks, props = find_peaks(\n",
    "        fDOM_data[:, 1],\n",
    "        height=(None, None),\n",
    "        threshold=(None, None),\n",
    "        distance=distance,\n",
    "        prominence=prominence_range,\n",
    "        width=width_range,\n",
    "        wlen=wlen,\n",
    "        rel_height=rel_height,\n",
    "    )\n",
    "\n",
    "    # Form candidate set from returned information\n",
    "    cands = [\n",
    "        [\n",
    "            peak,\n",
    "            math.floor(props[\"left_ips\"][i]),\n",
    "            math.ceil(props[\"right_ips\"][i]),\n",
    "            props[\"prominences\"][i],\n",
    "        ]\n",
    "        for i, peak in enumerate(peaks)\n",
    "    ]\n",
    "\n",
    "    cands_nskp = pd.DataFrame(cands)\n",
    "\n",
    "    # concat dataframes\n",
    "    cands_df = pd.concat([cands_nskp, cands_npp, cands_nplp])\n",
    "    cands_df = cands_df.sort_values(by=[0], kind='stable')\n",
    "    #cands_df = cands_df[~cands_df.index.duplicated(keep='first')]\n",
    "    cands_df = cands_df.reset_index(drop=True)\n",
    "\n",
    "    # import ground truths\n",
    "    truth_fname = \"../Data/labeled_data/ground_truths/fDOM/fDOM_all_julian_0k-300k.csv\"\n",
    "    truths = pd.read_csv(truth_fname)\n",
    "\n",
    "    truths = truths[truths[\"label_of_peak\"] == \"NAP\"]\n",
    "\n",
    "    cands_df = cands_df[cands_df[0].isin(truths[\"idx_of_peak\"])]\n",
    "\n",
    "    # rename cols\n",
    "    cands_df = cands_df.rename(\n",
    "        columns={0: \"idx_of_peak\", 1: \"left_base\", 2: \"right_base\", 3: \"amplitude\"}\n",
    "    )\n",
    "    \n",
    "    return cands_df\n",
    "\n",
    "def get_cands_turb_pp():\n",
    "    # load data\n",
    "    turb_data = dm.read_in_preprocessed_timeseries(\n",
    "        \"../Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv\"\n",
    "    )\n",
    "\n",
    "    turb_cand_params = {\n",
    "        \"prom\": [6, None],\n",
    "        \"width\": [None, None],\n",
    "        \"wlen\": 200,\n",
    "        \"dist\": 1,\n",
    "        \"rel_h\": 0.6,\n",
    "    }\n",
    "\n",
    "    turb_peaks, turb_props = get_candidates(turb_data, turb_cand_params)\n",
    "\n",
    "    turb_peaks, turb_props = dp.delete_missing_data_peaks(\n",
    "        turb_data, turb_peaks, turb_props, \"../Data/misc/flat_plat_ranges.txt\"\n",
    "    )\n",
    "\n",
    "    turb_cand = [\n",
    "        [\n",
    "            peak,\n",
    "            math.floor(turb_props[\"left_ips\"][i]),\n",
    "            math.ceil(turb_props[\"right_ips\"][i]),\n",
    "            turb_props[\"prominences\"][i],\n",
    "        ]\n",
    "        for i, peak in enumerate(turb_peaks)\n",
    "    ]\n",
    "\n",
    "    # convert to dataframe\n",
    "    cands_df = pd.DataFrame(turb_cand)\n",
    "\n",
    "    # load ground truths\n",
    "    truth_fname = \"../Data/labeled_data/ground_truths/turb/turb_pp/julian_time/turb_pp_0k-300k_labeled\"\n",
    "    truths = pd.read_csv(truth_fname)\n",
    "\n",
    "    truths = truths[truths[\"label_of_peak\"] != \"NPP\"]\n",
    "\n",
    "    # drop all rows in cands that are not in truths\n",
    "    cands_df = cands_df[cands_df[0].isin(truths[\"idx_of_peak\"])]\n",
    "\n",
    "    # rename cols\n",
    "    cands_df = cands_df.rename(\n",
    "        columns={0: \"idx_of_peak\", 1: \"left_base\", 2: \"right_base\", 3: \"amplitude\"}\n",
    "    )\n",
    "\n",
    "    return cands_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in data\n",
    "The knowledge-based approach uses the data in `Data/converted_data/julian_format/`, so that is where the data augmentation will go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in raw data\n",
    "fDOM_data = dm.read_in_preprocessed_timeseries(\n",
    "    \"../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv\"\n",
    ")\n",
    "stage_data = dm.read_in_preprocessed_timeseries(\n",
    "    \"../Data/converted_data/julian_format/stage_10.1.11-1.1.19.csv\"\n",
    ")\n",
    "turb_data = dm.read_in_preprocessed_timeseries(\n",
    "    \"../Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv\"\n",
    ")\n",
    "\n",
    "# align stage to fDOM\n",
    "stage_data = dp.align_stage_to_fDOM(fDOM_data, stage_data)\n",
    "\n",
    "# read in labeled fDOM\n",
    "fDOM_labeled = pd.read_csv(\n",
    "    \"../Data/labeled_data/ground_truths/fDOM/fDOM_all_julian_0k-300k.csv\"\n",
    ")\n",
    "\n",
    "# read in labeled turb\n",
    "turb_labeled = pd.read_csv(\n",
    "    \"../Data/labeled_data/ground_truths/turb/turb_pp/julian_time/turb_pp_0k-300k_labeled.csv\"\n",
    ")\n",
    "\n",
    "# New data folder:\n",
    "AUGMENT_DATA_PATH = \"../Data/augmented_data/julian_format/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into pandas dataframes for better indexing:\n",
    "fDOM_raw = pd.DataFrame(fDOM_data)\n",
    "fDOM_raw.columns = [\"timestamp\", \"value\"]\n",
    "\n",
    "turb_raw = pd.DataFrame(turb_data)\n",
    "turb_raw.columns = [\"timestamp\", \"value\"]\n",
    "\n",
    "stage_raw = pd.DataFrame(stage_data)\n",
    "stage_raw.columns = [\"timestamp\", \"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualize data\n",
    "\"\"\"\n",
    "fig = plt.figure()\n",
    "x = fDOM_raw['timestamp']\n",
    "y = turb_raw['value']\n",
    "\n",
    "line_fdom = plt.Line2D(fDOM_raw['timestamp'], fDOM_raw['value'])\n",
    "line_turb = plt.Line2D(turb_raw['timestamp'], turb_raw['value'], color='red')\n",
    "line_stage = plt.Line2D(stage_raw['timestamp'], stage_raw['value'], color='orange')\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.add_line(line_fdom)\n",
    "ax.add_line(line_turb)\n",
    "ax.add_line(line_stage)\n",
    "ax.set_xlim(min(x), max(x))\n",
    "ax.set_ylim(min(y) - 10, max(y) + 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting Data\n",
    "We will augment data for each type of peak, and for each measurement.\n",
    "\n",
    "Starting with fDOM:\n",
    "1. PLP (plummeting peak)\n",
    "2. PP (phantom peak)\n",
    "3. SKP (skyrocketing peak)\n",
    "\n",
    "TODO: augment more peak types when they are labeled\n",
    "\n",
    "With turbidity:\n",
    "1. PP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants\n",
    "We define two constants for use with augmenting the data:\n",
    "1. `TIME_RANGE_INIT`: the number of points before and after the relative peak that we take data from\n",
    "2. `ITERATIONS`: the number of times we loop over the list of anomaly peaks, and augment them\n",
    "3. `STARTING_TIMESTAMP`: the timestamp to start all augmented data at. The default value is 15 minutes after the last data measurement from the original set up data given to the project devs. As of 2/16/22, this timestamp is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Helpful constants \"\"\"\n",
    "TIME_RANGE_INIT = 30  # the base time range for peaks, in number of data points(MUST BE 1 OR HIGHER), this is used as a fallback\n",
    "ITERATIONS = 1 # number of times to loop over dataset and augment\n",
    "STARTING_TIMESTAMP = 2459096.9583333335\n",
    "LOWER_BOUND_AMPLITUDE_MULTIPLIER = -0.1\n",
    "UPPER_BOUND_AMPLITUDE_MULTIPLIER = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting fDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "#                                                                                   #\n",
    "#                               DATAFRAME SETUP SECTION                             #\n",
    "#                                                                                   #\n",
    "#####################################################################################\n",
    "\n",
    "# new dataframes for augmented labeled/raw fDOM\n",
    "augmented_fDOM_raw = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "augmented_fDOM_labeled = pd.DataFrame(\n",
    "    columns=[\"timestamp_of_peak\", \"value_of_peak\", \"label_of_peak\", \"idx_of_peak\"]\n",
    ")\n",
    "\n",
    "# dataframe for augmented stage\n",
    "augmented_stage_raw_fdom = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "\n",
    "# dataframes for augmented raw/labeled turbidity\n",
    "augmented_turb_raw_fdom = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "augmented_turb_labeled_fdom = pd.DataFrame(\n",
    "    columns=[\"timestamp_of_peak\", \"value_of_peak\", \"label_of_peak\", \"idx_of_peak\"]\n",
    ")\n",
    "\n",
    "# use scipy find peaks to get detected peaks\n",
    "# need to iterate over every labeled peak, and get the beginning and end from a specific list somehow\n",
    "\n",
    "# variable to keep the last entry in the dataframe for stage\n",
    "# defaults to the last entry that was in fdom/turb raw csv files, in julian format\n",
    "prev_added_entry = STARTING_TIMESTAMP\n",
    "\n",
    "# a list of peaks that don't align with the fDOM raw file that was aligned with stage\n",
    "# i believe its just peaks that don't align with stage in general for whatever reason\n",
    "missed_fDOM_peaks = []\n",
    "\n",
    "# ~~~~~ Collect starting and ending points of each fDOM peak type ~~~~~\n",
    "# PP\n",
    "fdom_pp_index_lookup = get_cands_fdom_pp()\n",
    "\n",
    "## SKP\n",
    "fdom_skp_index_lookup = get_cands_fdom_skp()\n",
    "\n",
    "## PLP\n",
    "fdom_plp_index_lookup = get_cands_fdom_plp()\n",
    "\n",
    "# non anomaly peaks\n",
    "fdom_NAP_index_lookup = get_cands_fdom_nap()\n",
    "\n",
    "#####################################################################################\n",
    "#                                                                                   #\n",
    "#                               AUGMENT DATA SECTION                                #\n",
    "#                                                                                   #\n",
    "#####################################################################################\n",
    "\n",
    "for iteration in range(ITERATIONS):\n",
    "    \"\"\"\n",
    "    Re-sample the fDOM labeled peaks to add variance to data\n",
    "    \"\"\"\n",
    "    # labeled fDOM peaks\n",
    "    fdom_anon_peaks = fDOM_labeled[fDOM_labeled[\"label_of_peak\"] != \"NAP\"]\n",
    "    # randomize the order, to add more\n",
    "    fdom_anon_peaks = fdom_anon_peaks.sample(frac=1).reset_index(\n",
    "        drop=True\n",
    "    )  # reset index as values were removed\n",
    "\n",
    "    # labeled non anomaly fDOM peaks\n",
    "    fdom_NAP = fDOM_labeled[fDOM_labeled[\"label_of_peak\"] == \"NAP\"]\n",
    "    fdom_NAP = fdom_NAP.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # iterate over each peak\n",
    "    for i, row in fdom_anon_peaks.iterrows():\n",
    "        print(\"anom peaks iterations: \" + str(i))\n",
    "\n",
    "        # check to see if any overlap occurs between peaks\n",
    "\n",
    "        # Get raw fDOM data points\n",
    "        label_of_peak = fdom_anon_peaks.loc[i, \"label_of_peak\"]\n",
    "\n",
    "        prev_dist = TIME_RANGE_INIT\n",
    "        next_dist = TIME_RANGE_INIT\n",
    "\n",
    "        prev_dist_NAP = TIME_RANGE_INIT\n",
    "        next_dist_NAP = TIME_RANGE_INIT\n",
    "\n",
    "        # set cands_df based on our peak label\n",
    "        if label_of_peak == \"PP\":\n",
    "            cands_df = fdom_pp_index_lookup\n",
    "        elif label_of_peak == \"PLP\":\n",
    "            cands_df = fdom_plp_index_lookup\n",
    "        elif label_of_peak == \"SKP\":\n",
    "            cands_df = fdom_skp_index_lookup\n",
    "\n",
    "        timestamp_of_peak = fdom_anon_peaks.loc[i, \"timestamp_of_peak\"]\n",
    "        # timestamp_NAP = fdom_NAP.loc[i, \"timestamp_of_peak\"]\n",
    "\n",
    "        NAP_index = fdom_NAP.loc[i, \"idx_of_peak\"]\n",
    "        timestamp_NAP = fDOM_raw.loc[NAP_index, \"timestamp\"]\n",
    "\n",
    "        # get index dataframes of each type\n",
    "        fdom_index_df = fDOM_raw[fDOM_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "        stage_index_df = stage_raw[stage_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "        turb_index_df = turb_raw[turb_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "\n",
    "        # get indices of non anomaly data\n",
    "        stage_index_df_NAP = stage_raw[stage_raw[\"timestamp\"] == timestamp_NAP]\n",
    "        turb_index_df_NAP = turb_raw[turb_raw[\"timestamp\"] == timestamp_NAP]\n",
    "\n",
    "        if len(fdom_index_df.index.to_list()) != 0:\n",
    "            # get indices of each data type from index df's\n",
    "            index_of_peak = fdom_index_df.index.tolist()[0]\n",
    "            stage_index = stage_index_df.index.tolist()[0]\n",
    "            turb_index = turb_index_df.index.tolist()[0]\n",
    "\n",
    "            # get indices of NAP (might break, try except to catch that)\n",
    "            stage_index_NAP = stage_index_df_NAP.index.tolist()[0]\n",
    "            turb_index_NAP = turb_index_df_NAP.index.tolist()[0]\n",
    "\n",
    "            # call get ends of peak to get beginning and ending of peaks\n",
    "            try:\n",
    "                left, right = get_ends_of_peak(cands_df, index_of_peak)\n",
    "\n",
    "                prev_dist = abs(index_of_peak - left)\n",
    "                next_dist = abs(index_of_peak - right)\n",
    "\n",
    "            except IndexError as e:\n",
    "                # something went wrong, just set prev_dist and next dist to time range init\n",
    "                prev_dist = TIME_RANGE_INIT\n",
    "                next_dist = TIME_RANGE_INIT\n",
    "                print(e)\n",
    "\n",
    "            # try except for nap\n",
    "            try:\n",
    "                left_NAP, right_NAP = get_ends_of_peak(fdom_NAP_index_lookup, NAP_index)\n",
    "                print(\"LEFT NAP: \" + str(left_NAP))\n",
    "\n",
    "                prev_dist_NAP = abs(NAP_index - left_NAP)\n",
    "                next_dist_NAP = abs(NAP_index - right_NAP)\n",
    "\n",
    "            except IndexError as e:\n",
    "                prev_dist_NAP = TIME_RANGE_INIT\n",
    "                next_dist_NAP = TIME_RANGE_INIT\n",
    "                print(e)\n",
    "\n",
    "            # use this timestamp to make a dataframe of raw stuff\n",
    "            # get data from fDOM_raw file\n",
    "            fDOM_raw_time_range = pd.DataFrame(\n",
    "                fDOM_raw.iloc[index_of_peak - prev_dist : index_of_peak + next_dist + 1]\n",
    "            )\n",
    "\n",
    "            # get stage data range\n",
    "            stage_time_range = pd.DataFrame(\n",
    "                stage_raw.iloc[stage_index - prev_dist : stage_index + next_dist + 1]\n",
    "            )\n",
    "\n",
    "            # get turbidity data range\n",
    "            turb_time_range = pd.DataFrame(\n",
    "                turb_raw.iloc[turb_index - prev_dist : turb_index + next_dist + 1]\n",
    "            )\n",
    "\n",
    "            # ~~~~~~~~~ make NAP data ranges ~~~~~~~~\n",
    "            print(prev_dist_NAP)\n",
    "            print(next_dist_NAP)\n",
    "\n",
    "            fDOM_raw_time_range_NAP = pd.DataFrame(\n",
    "                fDOM_raw.iloc[NAP_index - prev_dist_NAP : NAP_index + next_dist_NAP + 1]\n",
    "            )\n",
    "\n",
    "            # get stage data range\n",
    "            stage_time_range_NAP = pd.DataFrame(\n",
    "                stage_raw.iloc[\n",
    "                    stage_index_NAP\n",
    "                    - prev_dist_NAP : stage_index_NAP\n",
    "                    + next_dist_NAP\n",
    "                    + 1\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # get turbidity data range\n",
    "            turb_time_range_NAP = pd.DataFrame(\n",
    "                turb_raw.iloc[\n",
    "                    turb_index_NAP - prev_dist_NAP : turb_index_NAP + next_dist_NAP + 1\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            new_fdom_raw_NAP = copy.deepcopy(fDOM_raw_time_range_NAP)\n",
    "            new_stage_NAP = copy.deepcopy(stage_time_range_NAP)\n",
    "            new_turb_raw_NAP = copy.deepcopy(turb_time_range_NAP)\n",
    "\n",
    "            # make a copy of the modified data\n",
    "            new_fdom_raw = copy.deepcopy(fDOM_raw_time_range)\n",
    "            new_stage = copy.deepcopy(stage_time_range)\n",
    "            new_turb_raw = copy.deepcopy(turb_time_range)\n",
    "\n",
    "            # peak index can change when we add in x data\n",
    "            new_fdom_peak_index = -1\n",
    "            new_peak_timestamp = -1\n",
    "\n",
    "            #####################################################################################\n",
    "            #                                                                                   #\n",
    "            #                               ACTUAL CHANGES TO DATA                              #\n",
    "            #                                                                                   #\n",
    "            #####################################################################################\n",
    "\n",
    "            ####### AUGMENT ANOMALY PEAK #######\n",
    "            # gen a random number 0 or 1, if 0, widen, if 1, increase peak val\n",
    "            # set seed\n",
    "            random.seed()\n",
    "            widen_or_heighten = random.randint(0, 1)\n",
    "\n",
    "            # if 0, we widen\n",
    "            if widen_or_heighten == 0:\n",
    "                # TODO: ensure this worked...\n",
    "\n",
    "                # get current peak val\n",
    "                main_peak_val = new_fdom_raw.loc[index_of_peak, \"value\"]\n",
    "\n",
    "                # gen a random number between 0.01 and 0.2 to increase height by for previous and next peak\n",
    "                multiplier = random.uniform(0.001, 0.2)\n",
    "\n",
    "                # new peak vals\n",
    "                val_before_peak = new_fdom_raw.loc[index_of_peak - 1, \"value\"] * (\n",
    "                    1 + multiplier\n",
    "                )\n",
    "                val_after_peak = new_fdom_raw.loc[index_of_peak + 1, \"value\"] * (\n",
    "                    1 + multiplier\n",
    "                )\n",
    "\n",
    "                # ensure new value does not go over main peak val\n",
    "                fail_count = 0\n",
    "                can_widen = True\n",
    "                while (\n",
    "                    val_before_peak > main_peak_val and val_after_peak > main_peak_val\n",
    "                ):\n",
    "                    fail_count += 1\n",
    "                    if fail_count > 20:\n",
    "                        # not possible to fix peak, break and forget about it\n",
    "                        can_widen = False\n",
    "                        break\n",
    "\n",
    "                    # re gen num\n",
    "                    multiplier = random.uniform(0.001, 0.2)\n",
    "                    val_before_peak = new_fdom_raw.loc[index_of_peak - 1, \"value\"] * (\n",
    "                        1 + multiplier\n",
    "                    )\n",
    "                    val_after_peak = new_fdom_raw.loc[index_of_peak + 1, \"value\"] * (\n",
    "                        1 + multiplier\n",
    "                    )\n",
    "\n",
    "                if can_widen:\n",
    "                    # set new vals\n",
    "                    new_fdom_raw.loc[index_of_peak - 1, \"value\"] = val_before_peak\n",
    "                    new_fdom_raw.loc[index_of_peak + 1, \"value\"] = val_after_peak\n",
    "\n",
    "                # if we cant widen, lets just increase main peak amp\n",
    "                else:\n",
    "                    random_val = random.uniform(\n",
    "                        LOWER_BOUND_AMPLITUDE_MULTIPLIER,\n",
    "                        UPPER_BOUND_AMPLITUDE_MULTIPLIER,\n",
    "                    )\n",
    "                    new_peak_val = new_fdom_raw.loc[index_of_peak, \"value\"] * (\n",
    "                        1 + random_val\n",
    "                    )\n",
    "                    new_fdom_raw.loc[index_of_peak, \"value\"] = new_peak_val\n",
    "\n",
    "            # else we heighten\n",
    "            else:\n",
    "                # gen a random number to multiply amplitude by\n",
    "                random_val = random.uniform(\n",
    "                    LOWER_BOUND_AMPLITUDE_MULTIPLIER,\n",
    "                    UPPER_BOUND_AMPLITUDE_MULTIPLIER,\n",
    "                )\n",
    "                new_peak_val = new_fdom_raw.loc[index_of_peak, \"value\"] * (\n",
    "                    1 + random_val\n",
    "                )\n",
    "                new_fdom_raw.loc[index_of_peak, \"value\"] = new_peak_val\n",
    "\n",
    "            # get the next possible timestamp\n",
    "            new_time_entry = next_time_entry(prev_added_entry)\n",
    "\n",
    "            # update all timestamps for augmented data\n",
    "            for i, row in new_fdom_raw.iterrows():\n",
    "                # if timestamps equal, we have the relative peak\n",
    "                if new_fdom_raw.loc[i, \"timestamp\"] == timestamp_of_peak:\n",
    "                    # register index here\n",
    "                    new_fdom_peak_index = get_last_augment_index(augmented_fDOM_raw)\n",
    "                    new_peak_timestamp = new_time_entry\n",
    "                    new_peak_val = new_fdom_raw.loc[i, \"value\"]\n",
    "\n",
    "                # update timestamps\n",
    "                new_fdom_raw.loc[i, \"timestamp\"] = new_time_entry\n",
    "                new_stage.loc[i, \"timestamp\"] = new_time_entry\n",
    "                new_turb_raw.loc[i, \"timestamp\"] = new_time_entry\n",
    "\n",
    "                # get next time stamp\n",
    "                new_time_entry = next_time_entry(new_time_entry)\n",
    "\n",
    "            # add entries into raw fDOM\n",
    "            augmented_fDOM_raw = pd.concat(\n",
    "                [augmented_fDOM_raw, new_fdom_raw], ignore_index=True\n",
    "            )\n",
    "\n",
    "            new_label = pd.DataFrame(\n",
    "                [\n",
    "                    [\n",
    "                        new_peak_timestamp,\n",
    "                        new_peak_val,\n",
    "                        label_of_peak,\n",
    "                        new_fdom_peak_index,\n",
    "                    ]\n",
    "                ],\n",
    "                columns=[\n",
    "                    \"timestamp_of_peak\",\n",
    "                    \"value_of_peak\",\n",
    "                    \"label_of_peak\",\n",
    "                    \"idx_of_peak\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            # add entries to labeled fDOM\n",
    "            augmented_fDOM_labeled = pd.concat([augmented_fDOM_labeled, new_label])\n",
    "\n",
    "            # add entries to stage\n",
    "            augmented_stage_raw_fdom = pd.concat(\n",
    "                [augmented_stage_raw_fdom, new_stage], ignore_index=True\n",
    "            )\n",
    "\n",
    "            # add entries to turb\n",
    "            augmented_turb_raw_fdom = pd.concat(\n",
    "                [augmented_turb_raw_fdom, new_turb_raw], ignore_index=True\n",
    "            )\n",
    "\n",
    "            # update prev time entry\n",
    "            prev_added_entry = new_time_entry\n",
    "\n",
    "            # ~~~~~~~~~~~ AUGMENTING NAP ~~~~~~~~~\n",
    "            random.seed()\n",
    "            widen_or_heighten = random.randint(0, 1)\n",
    "\n",
    "            if widen_or_heighten == 0:\n",
    "                # add noise to the set\n",
    "                mu, sigma = 0, 0.15\n",
    "                noise = np.random.normal(mu, sigma, new_fdom_raw_NAP.shape[0])\n",
    "                print(new_fdom_raw_NAP)\n",
    "\n",
    "                new_fdom_raw_NAP.loc[:, \"value\"] = (\n",
    "                    new_fdom_raw_NAP.loc[:, \"value\"] + noise\n",
    "                )\n",
    "            else:\n",
    "                # increase main peak amplitude\n",
    "                random_val = random.uniform(\n",
    "                    LOWER_BOUND_AMPLITUDE_MULTIPLIER,\n",
    "                    UPPER_BOUND_AMPLITUDE_MULTIPLIER,\n",
    "                )\n",
    "\n",
    "                new_peak_val = new_fdom_raw_NAP.loc[NAP_index, \"value\"] * (\n",
    "                    1 + random_val\n",
    "                )\n",
    "                new_fdom_raw_NAP.loc[NAP_index, \"value\"] = new_peak_val\n",
    "\n",
    "            new_time_entry = next_time_entry(prev_added_entry)\n",
    "\n",
    "            # add temp index col for accessing old index vals\n",
    "            new_fdom_raw_NAP[\"tmp\"] = new_fdom_raw_NAP.index\n",
    "            for i, row in new_fdom_raw_NAP.iterrows():\n",
    "                if new_fdom_raw_NAP.loc[i, \"tmp\"] == NAP_index:\n",
    "                    new_peak_index = get_last_augment_index(augmented_fDOM_raw)\n",
    "                    new_peak_timestamp = new_time_entry\n",
    "                    new_peak_val = new_fdom_raw_NAP.loc[i, \"value\"]\n",
    "\n",
    "                new_fdom_raw_NAP.loc[i, \"timestamp\"] = new_time_entry\n",
    "                new_stage_NAP.loc[i, \"timestamp\"] = new_time_entry\n",
    "                new_turb_raw_NAP.loc[i, \"timestamp\"] = new_time_entry\n",
    "\n",
    "                new_time_entry = next_time_entry(new_time_entry)\n",
    "\n",
    "            # drop temp col\n",
    "            del new_fdom_raw_NAP[\"tmp\"]\n",
    "\n",
    "            augmented_fDOM_raw = pd.concat(\n",
    "                [augmented_fDOM_raw, new_fdom_raw_NAP], ignore_index=True\n",
    "            )\n",
    "\n",
    "            label_of_peak = \"NAP\"\n",
    "            new_peak_index = new_peak_index\n",
    "\n",
    "            new_label = pd.DataFrame(\n",
    "                [[new_peak_timestamp, new_peak_val, label_of_peak, new_peak_index]],\n",
    "                columns=[\n",
    "                    \"timestamp_of_peak\",\n",
    "                    \"value_of_peak\",\n",
    "                    \"label_of_peak\",\n",
    "                    \"idx_of_peak\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            # append the data to the main dataframes\n",
    "            augmented_fDOM_labeled = pd.concat([augmented_fDOM_labeled, new_label])\n",
    "\n",
    "            augmented_stage_raw_fdom = pd.concat(\n",
    "                [augmented_stage_raw_fdom, new_stage_NAP], ignore_index=True\n",
    "            )\n",
    "\n",
    "            # add entries to turb\n",
    "            augmented_turb_raw_fdom = pd.concat(\n",
    "                [augmented_turb_raw_fdom, new_turb_raw_NAP], ignore_index=True\n",
    "            )\n",
    "\n",
    "            # update prev time entry\n",
    "            prev_added_entry = new_time_entry\n",
    "\n",
    "        else:\n",
    "            # we missed some data points, append them to the missed data dataframe\n",
    "            missed_fDOM_peaks.append(timestamp_of_peak)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell allows you to print out the augmented dataframes in full\n",
    "\"\"\"\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "print(\"Labeled Peaks Augmented\")\n",
    "print(augmented_fDOM_labeled)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Raw fDOM Augmented\")\n",
    "print(augmented_fDOM_raw)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Raw Stage Augmented\")\n",
    "print(augmented_stage_raw_fdom)\n",
    "print(\"\\n\")\n",
    "\n",
    "# print(\"Raw Turbidity Augmented\")\n",
    "# print(augmented_turb_raw_fdom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualize data with matplotlib\n",
    "\"\"\"\n",
    "fig = plt.figure()\n",
    "x = augmented_turb_raw_fdom['timestamp']\n",
    "y = augmented_turb_raw_fdom['value']\n",
    "\n",
    "line_fdom = plt.Line2D(augmented_fDOM_raw['timestamp'], augmented_fDOM_raw['value'])\n",
    "line_turb = plt.Line2D(augmented_turb_raw_fdom['timestamp'], augmented_turb_raw_fdom['value'], color='red')\n",
    "line_stage = plt.Line2D(augmented_stage_raw_fdom['timestamp'], augmented_stage_raw_fdom['value'], color='orange')\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.add_line(line_fdom)\n",
    "ax.add_line(line_turb)\n",
    "ax.add_line(line_stage)\n",
    "ax.set_xlim(min(x), max(x))\n",
    "ax.set_ylim(min(y) - 10, max(y) + 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment turbidity data\n",
    "The following code blocks augment turbidity data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "#                                                                                   #\n",
    "#                               DATAFRAME SETUP SECTION                             #\n",
    "#                                                                                   #\n",
    "#####################################################################################\n",
    "\n",
    "# labeled turb\n",
    "augmented_turb_labeled = pd.DataFrame(\n",
    "    columns=[\"timestamp_of_peak\", \"value_of_peak\", \"label_of_peak\", \"idx_of_peak\"]\n",
    ")\n",
    "\n",
    "# raw turb\n",
    "augmented_turb_raw = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "\n",
    "# raw fdom\n",
    "augmented_fDOM_raw_turb = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "\n",
    "# raw stage\n",
    "augmented_stage_raw_turb = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "\n",
    "# variable to keep the last entry in the dataframe for stage\n",
    "# defaults to the last entry that was in fdom/turb raw csv files, in julian format\n",
    "prev_added_entry = STARTING_TIMESTAMP\n",
    "\n",
    "# a list of peaks that don't align with the fDOM raw file that was aligned with stage\n",
    "# i believe its just peaks that don't align with stage in general for whatever reason\n",
    "missed_turb_peaks = []\n",
    "\n",
    "# ~~~~~ Collect starting and ending points of each turb peak type ~~~~~~\n",
    "turb_pp_index_lookup = get_cands_turb_pp()\n",
    "\n",
    "#####################################################################################\n",
    "#                                                                                   #\n",
    "#                               AUGMENT DATA SECTION                                #\n",
    "#                                                                                   #\n",
    "#####################################################################################\n",
    "\n",
    "\"\"\" Augment turbidity data by calling previously written function \"\"\"\n",
    "for iteration in range(ITERATIONS):\n",
    "    \"\"\" Resample turb labeled peaks at each iteration for more variance \"\"\"\n",
    "    # labeled turb peaks\n",
    "    turb_anon_peaks = turb_labeled[turb_labeled[\"label_of_peak\"] != \"NPP\"]\n",
    "    turb_anon_peaks = turb_anon_peaks.sample(frac=1).reset_index()\n",
    "\n",
    "    # iterate over each peak\n",
    "    for i, row in turb_anon_peaks.iterrows():\n",
    "\n",
    "        \"\"\"Get raw turb data points\"\"\"\n",
    "        timestamp_of_peak = turb_anon_peaks.loc[i, \"timestamp_of_peak\"]\n",
    "        label_of_peak = turb_anon_peaks.loc[i, \"label_of_peak\"]\n",
    "\n",
    "        # get index dataframes of each type\n",
    "        fdom_index_df = fDOM_raw[fDOM_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "        stage_index_df = stage_raw[stage_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "        turb_index_df = turb_raw[turb_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "\n",
    "        if len(turb_index_df.index.to_list()) != 0:\n",
    "            # get indices of each data type from index df's\n",
    "            index_of_peak = turb_index_df.index.tolist()[0]\n",
    "            stage_index = stage_index_df.index.tolist()[0]\n",
    "            fdom_index = fdom_index_df.index.tolist()[0]\n",
    "\n",
    "            # check to see if any overlap occurs between peaks\n",
    "            prev_dist = TIME_RANGE_INIT\n",
    "            next_dist = TIME_RANGE_INIT\n",
    "\n",
    "            if label_of_peak == \"PP\":\n",
    "                cands_df = turb_pp_index_lookup\n",
    "\n",
    "            try:\n",
    "                left, right = get_ends_of_peak(cands_df, index_of_peak)\n",
    "\n",
    "                prev_dist = abs(index_of_peak - left)\n",
    "                next_dist = abs(index_of_peak - right)\n",
    "\n",
    "            except IndexError:\n",
    "                # index wrong, reset prev dist\n",
    "                print(\"INDEX ERROR!\")\n",
    "                prev_dist = TIME_RANGE_INIT\n",
    "                next_dist = TIME_RANGE_INIT \n",
    "\n",
    "            # use this timestamp to make a dataframe of raw stuff\n",
    "            # get data from fDOM_raw file\n",
    "            fDOM_raw_time_range = pd.DataFrame(\n",
    "                fDOM_raw.iloc[fdom_index - prev_dist : fdom_index + next_dist + 1]\n",
    "            )\n",
    "\n",
    "            # get stage data range\n",
    "            stage_time_range = pd.DataFrame(\n",
    "                stage_raw.iloc[stage_index - prev_dist : stage_index + next_dist + 1]\n",
    "            )\n",
    "\n",
    "            # get turbidity data range\n",
    "            turb_time_range = pd.DataFrame(\n",
    "                turb_raw.iloc[index_of_peak - prev_dist : index_of_peak + next_dist + 1]\n",
    "            )\n",
    "\n",
    "            # make a copy of the modified data\n",
    "            new_fdom_raw = copy.deepcopy(fDOM_raw_time_range)\n",
    "            new_stage = copy.deepcopy(stage_time_range)\n",
    "            new_turb_raw = copy.deepcopy(turb_time_range)\n",
    "\n",
    "            # peak index can change when we add in x data\n",
    "            new_fdom_peak_index = -1\n",
    "            new_peak_timestamp = -1\n",
    "            \n",
    "            #####################################################################################\n",
    "            #                                                                                   #\n",
    "            #                               ACTUAL CHANGES TO DATA                              #\n",
    "            #                                                                                   #\n",
    "            #####################################################################################\n",
    "\n",
    "            # gen random num to decide whether to widen or heighten\n",
    "            random.seed() \n",
    "            widen_or_heighten = random.randint(0, 1)\n",
    "\n",
    "            # if 0, widen the peak\n",
    "            if widen_or_heighten == 0:\n",
    "               # get current peak val\n",
    "                main_peak_val = new_turb_raw.loc[index_of_peak, \"value\"]\n",
    "\n",
    "                # gen a random number between 0.01 and 0.2 to increase height by for previous and next peak\n",
    "                multiplier = random.uniform(0.001, 0.2)\n",
    "                \n",
    "                # new peak vals\n",
    "                val_before_peak = new_turb_raw.loc[index_of_peak - 1, \"value\"] * (1 + multiplier)\n",
    "                val_after_peak = new_turb_raw.loc[index_of_peak + 1, \"value\"] * (1 + multiplier)\n",
    "\n",
    "                # ensure new value does not go over main peak val\n",
    "                fail_count = 0\n",
    "                can_widen = True\n",
    "                while val_before_peak > main_peak_val and val_after_peak > main_peak_val:\n",
    "                    fail_count += 1 \n",
    "                    if fail_count > 20:\n",
    "                        # not possible to fix peak, break and forget about it\n",
    "                        can_widen = False\n",
    "                        break \n",
    "                    \n",
    "                    # re gen num\n",
    "                    multiplier = random.uniform(0.001, 0.2)\n",
    "                    val_before_peak = new_turb_raw.loc[index_of_peak - 1, \"value\"] * (1 + multiplier)\n",
    "                    val_after_peak = new_turb_raw.loc[index_of_peak + 1, \"value\"] * (1 + multiplier) \n",
    "                \n",
    "                if can_widen:\n",
    "                    # set new vals\n",
    "                    new_turb_raw.loc[index_of_peak - 1, \"value\"] = val_before_peak\n",
    "                    new_turb_raw.loc[index_of_peak + 1, \"value\"] = val_after_peak \n",
    "            \n",
    "            # else, lets heighten the peak\n",
    "            else:\n",
    "                random_val = random.uniform(LOWER_BOUND_AMPLITUDE_MULTIPLIER, UPPER_BOUND_AMPLITUDE_MULTIPLIER)\n",
    "                new_peak_val = new_turb_raw.loc[index_of_peak, \"value\"] * (1 + random_val)\n",
    "                new_turb_raw.loc[index_of_peak, \"value\"] = new_peak_val\n",
    "\n",
    "            # get the next possible timestamp\n",
    "            new_time_entry = next_time_entry(prev_added_entry)\n",
    "\n",
    "            # update all timestamps for augmented data\n",
    "            for i, row in new_turb_raw.iterrows():\n",
    "                # if timestamps equal, we have the relative peak\n",
    "                if new_turb_raw.loc[i, \"timestamp\"] == timestamp_of_peak:\n",
    "                    # register index here\n",
    "                    new_turb_peak_index = get_last_augment_index(augmented_turb_raw)\n",
    "                    new_peak_timestamp = new_time_entry\n",
    "\n",
    "                # update timestamps\n",
    "                new_fdom_raw.loc[i, \"timestamp\"] = new_time_entry\n",
    "                new_stage.loc[i, \"timestamp\"] = new_time_entry\n",
    "                new_turb_raw.loc[i, \"timestamp\"] = new_time_entry\n",
    "\n",
    "                # get next time stamp\n",
    "                new_time_entry = next_time_entry(new_time_entry)\n",
    "\n",
    "            # add entries to turb\n",
    "            augmented_turb_raw = pd.concat(\n",
    "                [augmented_turb_raw, new_turb_raw], ignore_index=True\n",
    "            )\n",
    "\n",
    "            new_label = pd.DataFrame(\n",
    "                [\n",
    "                    [\n",
    "                        new_peak_timestamp,\n",
    "                        new_peak_val,\n",
    "                        label_of_peak,\n",
    "                        new_turb_peak_index,\n",
    "                    ]\n",
    "                ],\n",
    "                columns=[\n",
    "                    \"timestamp_of_peak\",\n",
    "                    \"value_of_peak\",\n",
    "                    \"label_of_peak\",\n",
    "                    \"idx_of_peak\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            # add entries to labeled turb\n",
    "            augmented_turb_labeled = pd.concat([augmented_turb_labeled, new_label])\n",
    "\n",
    "            # add entries to stage\n",
    "            augmented_stage_raw_turb = pd.concat(\n",
    "                [augmented_stage_raw_turb, new_stage], ignore_index=True\n",
    "            )\n",
    "\n",
    "            # add entries into raw fDOM\n",
    "            augmented_fDOM_raw_turb = pd.concat(\n",
    "                [augmented_fDOM_raw_turb, new_fdom_raw], ignore_index=True\n",
    "            )\n",
    "\n",
    "            # update prev time entry\n",
    "            prev_added_entry = new_time_entry\n",
    "\n",
    "        else:\n",
    "            # we missed some data points, append them to the missed data dataframe\n",
    "            missed_fDOM_peaks.append(timestamp_of_peak)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell allows you to print out the augmented dataframes in full\n",
    "\"\"\"\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.width', 1000)\n",
    "# pd.set_option('display.colheader_justify', 'center')\n",
    "# pd.set_option('display.precision', 3)\n",
    "\n",
    "# print(\"Labeled Peaks Augmented\")\n",
    "# print(augmented_turb_labeled)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(\"Raw fDOM Augmented\")\n",
    "# print(augmented_fDOM_raw_turb)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(\"Raw Stage Augmented\")\n",
    "# print(augmented_stage_raw_turb)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(\"Raw Turbidity Augmented\")\n",
    "# print(augmented_turb_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualize data with matplotlib\n",
    "\"\"\"\n",
    "fig = plt.figure()\n",
    "x = augmented_turb_raw['timestamp']\n",
    "y = augmented_turb_raw['value']\n",
    "\n",
    "line_fdom = plt.Line2D(augmented_fDOM_raw_turb['timestamp'], augmented_fDOM_raw_turb['value'])\n",
    "line_turb = plt.Line2D(augmented_turb_raw['timestamp'], augmented_turb_raw['value'], color='red')\n",
    "line_stage = plt.Line2D(augmented_stage_raw_turb['timestamp'], augmented_stage_raw_turb['value'], color='orange')\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.add_line(line_fdom)\n",
    "ax.add_line(line_turb)\n",
    "ax.add_line(line_stage)\n",
    "ax.set_xlim(min(x), max(x))\n",
    "ax.set_ylim(min(y) - 10, max(y) + 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move augmented data into csv files\n",
    "The following codeblock creates csv files for the augmented data.\n",
    "\n",
    "### NOTE ON DATA:\n",
    "Due to the random sampling used when augmenting fDOM and turbidity,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Augmented Data Paths \"\"\"\n",
    "# trainset\n",
    "trainset_fdom_path = \"../Data/augmented_data/trainset_plotting/fdom/\"\n",
    "trainset_turb_path = \"../Data/augmented_data/trainset_plotting/turb/\"\n",
    "\n",
    "# unlabeled data\n",
    "unlabeled_fdom_path = \"../Data/augmented_data/fdom/unlabeled/\"\n",
    "unlabeled_turb_path = \"../Data/augmented_data/turb/unlabeled/\"\n",
    "\n",
    "# labeled data\n",
    "labeled_fdom_path = \"../Data/augmented_data/fdom/labeled/\"\n",
    "labeled_turb_path = \"../Data/augmented_data/turb/labeled/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_augmented_data_to_csv():\n",
    "    # call to_csv for each dataframe\n",
    "    # for each dataframe, we also drop the index\n",
    "\n",
    "    # write fDOM augmented data\n",
    "    augmented_fDOM_labeled.to_csv(labeled_fdom_path + 'labeled_fdom_peaks.csv', index=False)\n",
    "    augmented_fDOM_raw.to_csv(unlabeled_fdom_path + 'unlabeled_fdom.csv', index=False)\n",
    "    augmented_turb_raw_fdom.to_csv(unlabeled_fdom_path + 'unlabeled_turb.csv', index=False)\n",
    "    augmented_stage_raw_fdom.to_csv(unlabeled_fdom_path + 'unlabeled_stage.csv', index=False)\n",
    "\n",
    "    # write turb augmented data\n",
    "    augmented_turb_labeled.to_csv(labeled_turb_path + 'labeled_turb_peaks.csv', index=False)\n",
    "    augmented_turb_raw.to_csv(unlabeled_turb_path + 'unlabeled_turb.csv', index=False)\n",
    "    augmented_fDOM_raw_turb.to_csv(unlabeled_turb_path + 'unlabeled_fdom.csv', index=False)\n",
    "    augmented_stage_raw_turb.to_csv(unlabeled_turb_path + 'unlabeled_stage.csv', index=False)\n",
    "\n",
    "\n",
    "def convert_df_julian_to_datetime(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts a dataframe julian timestamp to datetime ISO 8601 format\n",
    "\n",
    "    df: a dataframe\n",
    "\n",
    "    return: changed dataframe\n",
    "    \"\"\"\n",
    "    # iterate over dataframe, replacing timestamp vals\n",
    "    for i, row in df.iterrows():\n",
    "        df.loc[i, \"timestamp\"] = dp.julian_to_datetime(df.loc[i, \"timestamp\"]).isoformat()\n",
    "        \n",
    "        # add stupid 0.00Z to fit trainset format\n",
    "        df.loc[i, \"timestamp\"] = df.loc[i, \"timestamp\"] + \".000Z\"\n",
    "\n",
    "    return df\n",
    "\n",
    "def write_to_trainset_csv():\n",
    "    # TODO: add peak labels in\n",
    "\n",
    "    # start by creating a dataframe that has the correct columns\n",
    "    trainset_fdom_df = pd.DataFrame(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "    trainset_turb_df = pd.DataFrame(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "\n",
    "    # ~~~~~~~ fDOM section ~~~~~~~\n",
    "    # start by just adding the fDOM data into the series, need to replace all timestamps\n",
    "    fdom_trainset_raw = copy.deepcopy(augmented_fDOM_raw)\n",
    "    fdom_turb_trainset_raw = copy.deepcopy(augmented_turb_raw_fdom)\n",
    "    fdom_stage_trainset_raw = copy.deepcopy(augmented_stage_raw_fdom)\n",
    "    \n",
    "    # convert timestamps to julian\n",
    "    fdom_trainset_raw = convert_df_julian_to_datetime(fdom_trainset_raw)\n",
    "    fdom_turb_trainset_raw = convert_df_julian_to_datetime(fdom_turb_trainset_raw)\n",
    "    fdom_stage_trainset_raw = convert_df_julian_to_datetime(fdom_stage_trainset_raw)\n",
    "\n",
    "    # add in new values\n",
    "    fdom_trainset_raw[\"series\"] = \"fDOM\"\n",
    "    fdom_trainset_raw[\"label\"] = \"\"\n",
    "\n",
    "    fdom_turb_trainset_raw[\"series\"] = \"turb\"\n",
    "    fdom_turb_trainset_raw[\"label\"] = \"\"\n",
    "\n",
    "    fdom_stage_trainset_raw[\"series\"] = \"stage\"\n",
    "    fdom_stage_trainset_raw[\"label\"] = \"\"\n",
    "\n",
    "    # reorder columns\n",
    "    fdom_trainset_raw = fdom_trainset_raw.reindex(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "    fdom_turb_trainset_raw = fdom_turb_trainset_raw.reindex(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "    fdom_stage_trainset_raw = fdom_stage_trainset_raw.reindex(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "\n",
    "    # concat into single dataframe\n",
    "    trainset_fdom_df = pd.concat([fdom_trainset_raw, fdom_turb_trainset_raw, fdom_stage_trainset_raw])\n",
    "\n",
    "    # sort together\n",
    "    trainset_fdom_df = trainset_fdom_df.sort_values(by=['timestamp'], kind='stable')\n",
    "\n",
    "    # export to csv\n",
    "    trainset_fdom_df.to_csv(trainset_fdom_path + \"fdom_augmented.csv\", index=False)\n",
    "\n",
    "    # ~~~~~~~ turbidity section ~~~~~~~\n",
    "\n",
    "    # create new dataframes\n",
    "    turb_trainset_raw = copy.deepcopy(augmented_turb_raw)\n",
    "    turb_fdom_trainset_raw = copy.deepcopy(augmented_fDOM_raw_turb)\n",
    "    turb_stage_trainset_raw = copy.deepcopy(augmented_stage_raw_turb)\n",
    "\n",
    "    # convert timestamps\n",
    "    turb_trainset_raw = convert_df_julian_to_datetime(turb_trainset_raw)\n",
    "    turb_fdom_trainset_raw = convert_df_julian_to_datetime(turb_fdom_trainset_raw)\n",
    "    turb_stage_trainset_raw = convert_df_julian_to_datetime(turb_stage_trainset_raw)\n",
    "\n",
    "    # add in new values\n",
    "    turb_fdom_trainset_raw[\"series\"] = \"fDOM\"\n",
    "    turb_fdom_trainset_raw[\"label\"] = \"\"\n",
    "\n",
    "    turb_trainset_raw[\"series\"] = \"turb\"\n",
    "    turb_trainset_raw[\"label\"] = \"\"\n",
    "\n",
    "    turb_stage_trainset_raw[\"series\"] = \"stage\"\n",
    "    turb_stage_trainset_raw[\"label\"] = \"\"\n",
    "\n",
    "    # reorder columns\n",
    "    turb_fdom_trainset_raw = turb_fdom_trainset_raw.reindex(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "    turb_trainset_raw = turb_trainset_raw.reindex(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "    turb_stage_trainset_raw = turb_stage_trainset_raw.reindex(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "\n",
    "    # concat into single dataframe\n",
    "    trainset_turb_df = pd.concat([turb_fdom_trainset_raw, turb_trainset_raw, turb_stage_trainset_raw])\n",
    "\n",
    "    # sort together\n",
    "    trainset_turb_df = trainset_turb_df.sort_values(by=['timestamp'], kind='stable')\n",
    "\n",
    "    # export to csv\n",
    "    trainset_turb_df.to_csv(trainset_turb_path + \"turb_augmented.csv\", index=False)\n",
    "\n",
    "#write_augmented_data_to_csv()\n",
    "write_to_trainset_csv()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f14ca78382e4dc227f1e51b680612414c34712d66eddea25f1c0d0c728b236b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('anomaly-detection': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
