{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenting time-series data\n",
    "In this file, the data is augmented in order to create more of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, \"../\")\n",
    "import Tools.data_processing as dp\n",
    "import Tools.data_movement as dm\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import copy\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.signal import find_peaks\n",
    "import numpy as np\n",
    "import scipy.ndimage as ndimage\n",
    "import csv\n",
    "from Tools.get_candidates import (\n",
    "    get_cands_fDOM_NAP,\n",
    "    get_cands_fDOM_PLP,\n",
    "    get_cands_fDOM_PP,\n",
    "    get_cands_fDOM_SKP,\n",
    "    get_cands_turb_PP,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants\n",
    "We define two constants for use with augmenting the data:\n",
    "1. `TIME_RANGE_INIT`: the number of points before and after the relative peak that we take data from\n",
    "2. `ITERATIONS`: the number of times we loop over the list of anomaly peaks, and augment them\n",
    "3. `STARTING_TIMESTAMP`: the timestamp to start all augmented data at. The default value is 15 minutes after the last data measurement from the original set up data given to the project devs. As of 2/16/22, this timestamp is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Helpful constants \"\"\"\n",
    "TIME_RANGE_INIT = 30  # the base time range for peaks, in number of data points(MUST BE 1 OR HIGHER), this is used as a fallback\n",
    "ITERATIONS = 1 # number of times to loop over dataset and augment\n",
    "STARTING_TIMESTAMP = 2459096.9583333335\n",
    "LOWER_BOUND_AMPLITUDE_MULTIPLIER = -0.1\n",
    "UPPER_BOUND_AMPLITUDE_MULTIPLIER = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "The following functions provide useful tools for the augmentation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_time_entry(current_entry: float) -> float:\n",
    "    \"\"\"\n",
    "    This function returns the next time entry in julian time\n",
    "\n",
    "    current_entry: a julina time float\n",
    "\n",
    "    return: julian time + 15 minutes from past julian time\n",
    "    \"\"\"\n",
    "\n",
    "    # convert julian to datetime\n",
    "    date_time_init = dp.julian_to_datetime(current_entry)\n",
    "\n",
    "    # find next date time (add 15 minutes)\n",
    "    next_entry = date_time_init + timedelta(minutes=15)\n",
    "\n",
    "    # convert date time to julian time\n",
    "    final_julian_time = dp.datetime_to_julian(next_entry)\n",
    "\n",
    "    # return julian time\n",
    "    return final_julian_time\n",
    "\n",
    "\n",
    "def get_last_augment_index(dataframe) -> int:\n",
    "    \"\"\"\n",
    "    Collects the last index of the augmented time series\n",
    "\n",
    "    dataframe: the dataframe of index to check\n",
    "\n",
    "    returns: the last index\n",
    "    \"\"\"\n",
    "    return dataframe.shape[0]\n",
    "\n",
    "\n",
    "def get_ends_of_peak(cands_df: pd.DataFrame, peak_index) -> tuple():\n",
    "    \"\"\"\n",
    "    get left and right ends of a peak from the respective dataframes\n",
    "\n",
    "    cands_df: the candidate dataframe\n",
    "\n",
    "    peak_index: the index of the peak\n",
    "\n",
    "    returns: the left and right base of the peak segment\n",
    "    \"\"\"\n",
    "    # use cands_df to return left and right base of peak index\n",
    "    new_cands = copy.deepcopy(cands_df)\n",
    "    new_cands = new_cands.loc[new_cands[\"idx_of_peak\"] == peak_index]\n",
    "\n",
    "    left_base = new_cands[\"left_base\"]\n",
    "    right_base = new_cands[\"right_base\"]\n",
    "\n",
    "    left_base = left_base.to_list()\n",
    "    left_base = left_base[0]\n",
    "\n",
    "    right_base = right_base.to_list()\n",
    "    right_base = right_base[0]\n",
    "\n",
    "    # return left and right\n",
    "    return left_base, right_base\n",
    "\n",
    "\n",
    "# TODO: Refactor this function to work for fdom and turb\n",
    "# might actually work if correct indices are just passed in...\n",
    "def build_temp_dataframes(\n",
    "    fdom, stage, turb, prev, next, fdom_idx, stage_idx, turb_idx\n",
    ") -> tuple():\n",
    "    \"\"\"\n",
    "    build the temporary dataframes for the peak segment\n",
    "\n",
    "    fdom: the fdom main dataframe\n",
    "\n",
    "    stage: the stage dataframe\n",
    "\n",
    "    turb: the turb dataframe\n",
    "\n",
    "    prev: how far back the peak goes (index wise)\n",
    "\n",
    "    next: how far forward the peak goes (index wise)\n",
    "\n",
    "    fdom_idx: the peak index for fdom\n",
    "\n",
    "    stage_idx: the relevant stage index\n",
    "\n",
    "    turb_idx: the relevant turb index\n",
    "\n",
    "    returns: the new time segments for each dataframe\n",
    "    \"\"\"\n",
    "    fDOM_raw_time_range = pd.DataFrame(fdom.iloc[fdom_idx - prev : fdom_idx + next + 1])\n",
    "\n",
    "    # get stage data range\n",
    "    stage_time_range = pd.DataFrame(stage.iloc[stage_idx - prev : stage_idx + next + 1])\n",
    "\n",
    "    # get turbidity data range\n",
    "    turb_time_range = pd.DataFrame(turb.iloc[turb_idx - prev : turb_idx + next + 1])\n",
    "\n",
    "    new_fdom = copy.deepcopy(fDOM_raw_time_range)\n",
    "    new_stage = copy.deepcopy(stage_time_range)\n",
    "    new_turb_raw = copy.deepcopy(turb_time_range)\n",
    "\n",
    "    return new_fdom, new_stage, new_turb_raw\n",
    "\n",
    "\n",
    "def widen_augment(df, peak_idx) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    widen the peaks by increasing values across the peak by a set noise val\n",
    "\n",
    "    df: the dataframe to augment\n",
    "\n",
    "    peak_idx: the index of the peak\n",
    "\n",
    "    returns: The augmented dataframe\n",
    "    \"\"\"\n",
    "    mu, sigma = 0.001, 0.1\n",
    "    noise = np.random.normal(mu, sigma, df.shape[0])\n",
    "\n",
    "    df.loc[:, \"value\"] = df.loc[:, \"value\"] + noise\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def heighten_augment(df, peak_idx) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    heighten the main peak ampltiude\n",
    "\n",
    "    df: the dataframe to augment\n",
    "\n",
    "    peak_index = the index of the peak\n",
    "\n",
    "    returns: the augmented dataframe\n",
    "    \"\"\"\n",
    "    # gen a random number to multiply amplitude by\n",
    "    random_val = random.uniform(\n",
    "        LOWER_BOUND_AMPLITUDE_MULTIPLIER,\n",
    "        UPPER_BOUND_AMPLITUDE_MULTIPLIER,\n",
    "    )\n",
    "    new_peak_val = df.loc[peak_idx, \"value\"] * (1 + random_val)\n",
    "    df.loc[peak_idx, \"value\"] = new_peak_val\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def augment_data(df, peak_index):\n",
    "    \"\"\"\n",
    "    augment the given dataframe\n",
    "    decides whether to heighten or widen the peak\n",
    "\n",
    "    df: dataframe to augment\n",
    "\n",
    "    peak_index: index of the peak\n",
    "\n",
    "    returns: the augmented dataframe\n",
    "    \"\"\"\n",
    "    # gen random number\n",
    "    random.seed()\n",
    "    widen_or_heighten = random.randint(0, 1)\n",
    "\n",
    "    # if 0, widen peak, else heighten\n",
    "    if widen_or_heighten == 0:\n",
    "        df = widen_augment(df, peak_index)\n",
    "\n",
    "    else:\n",
    "        df = heighten_augment(df, peak_index)\n",
    "\n",
    "    # return augmented dataframe\n",
    "    return df\n",
    "\n",
    "\n",
    "def concat_dataframes(\n",
    "    main_df_labeled,\n",
    "    main_fdom,\n",
    "    main_stage,\n",
    "    main_turb,\n",
    "    new_fdom,\n",
    "    new_stage,\n",
    "    new_turb,\n",
    "    new_label,\n",
    "):\n",
    "    \"\"\"\n",
    "    Concatenate the augmented dataframes\n",
    "\n",
    "    main_df_labeled: the labeled dataframe that holds all augmented data\n",
    "\n",
    "    main_fdom: the main fdom dataframe being concatenated\n",
    "\n",
    "    main_stage: the main stage df being concatenated\n",
    "\n",
    "    main_turb: the main turb df being concatenated\n",
    "\n",
    "    new fdom, stage, turb: the small peak range df's holding newly augmented data\n",
    "\n",
    "    new_label: the label for the relevant data that was augmented (either fdom or turb)\n",
    "\n",
    "    returns: the main augmented dataframes\n",
    "    \"\"\"\n",
    "    main_df_labeled = pd.concat([main_df_labeled, new_label])\n",
    "    main_fdom = pd.concat([main_fdom, new_fdom], ignore_index=True)\n",
    "    main_stage = pd.concat([main_stage, new_stage], ignore_index=True)\n",
    "    main_turb = pd.concat([main_turb, new_turb], ignore_index=True)\n",
    "\n",
    "    return main_df_labeled, main_fdom, main_stage, main_turb\n",
    "\n",
    "\n",
    "def update_dataframes(\n",
    "    prev_time_entry,\n",
    "    df,\n",
    "    peak_index,\n",
    "    prev_dist,\n",
    "    main_augment_df,\n",
    "    stage_df,\n",
    "    non_augment_df,\n",
    "    label_of_peak,\n",
    "):\n",
    "    \"\"\"\n",
    "    set updated timestamps, peak values, etc.\n",
    "\n",
    "    prev_time_entry is the past time entry\n",
    "\n",
    "    df is the main dataframe being actually augmented (fdom or turb)\n",
    "\n",
    "    peak_index is the index of the augmented peak\n",
    "\n",
    "    prev_dist is how far back from the peak the main df being augmented goes\n",
    "\n",
    "    main_augment_df is the overall df that holds the augmented data\n",
    "\n",
    "    stage_df is the stage section for df\n",
    "\n",
    "    non_augment_df is either the turb section or fdom section, depending on if fdom or turb is being augmented\n",
    "\n",
    "    label_of_peak is the peak label\n",
    "\n",
    "    returns: the new label, and the augmented dataframes with the updated timestamps\n",
    "    \"\"\"\n",
    "    # set new time entry to be the prev entry passed into this function\n",
    "    new_time_entry = prev_time_entry\n",
    "\n",
    "    # add temp col to access indices\n",
    "    df[\"tmp\"] = df.index\n",
    "    for i, row in df.iterrows():\n",
    "        # get new timestamp\n",
    "        new_time_entry = next_time_entry(new_time_entry)\n",
    "\n",
    "        # check timestamps\n",
    "        if df.loc[i, \"tmp\"] == peak_index:\n",
    "            # get the new index of the peak\n",
    "            # this is for the new label for the labeled augmented data\n",
    "            new_peak_index = get_last_augment_index(main_augment_df) + prev_dist\n",
    "            new_peak_timestamp = new_time_entry\n",
    "            new_peak_val = df.loc[i, \"value\"]\n",
    "\n",
    "        # update timestamps of df's\n",
    "        df.loc[i, \"timestamp\"] = new_time_entry\n",
    "        stage_df.loc[i, \"timestamp\"] = new_time_entry\n",
    "        non_augment_df.loc[i, \"timestamp\"] = new_time_entry\n",
    "\n",
    "    # delete the extra label\n",
    "    del df[\"tmp\"]\n",
    "\n",
    "    # create new label for labeled data\n",
    "    new_label = pd.DataFrame(\n",
    "        [[new_peak_timestamp, new_peak_val, label_of_peak, new_peak_index]],\n",
    "        columns=[\"timestamp_of_peak\", \"value_of_peak\", \"label_of_peak\", \"idx_of_peak\"],\n",
    "    )\n",
    "\n",
    "    # return the label, and return all new dataframes\n",
    "    return new_label, df, stage_df, non_augment_df, new_time_entry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in data\n",
    "The knowledge-based approach uses the data in `Data/converted_data/julian_format/`, so that is where the data augmentation will go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in raw data\n",
    "fDOM_data = dm.read_in_preprocessed_timeseries(\n",
    "    \"../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv\"\n",
    ")\n",
    "stage_data = dm.read_in_preprocessed_timeseries(\n",
    "    \"../Data/converted_data/julian_format/stage_10.1.11-1.1.19.csv\"\n",
    ")\n",
    "turb_data = dm.read_in_preprocessed_timeseries(\n",
    "    \"../Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv\"\n",
    ")\n",
    "\n",
    "# align stage to fDOM\n",
    "stage_data = dp.align_stage_to_fDOM(fDOM_data, stage_data)\n",
    "\n",
    "# read in labeled fDOM\n",
    "fDOM_labeled = pd.read_csv(\n",
    "    \"../Data/labeled_data/ground_truths/fDOM/fDOM_all_julian_0k-300k.csv\"\n",
    ")\n",
    "\n",
    "# read in labeled turb\n",
    "turb_labeled = pd.read_csv(\n",
    "    \"../Data/labeled_data/ground_truths/turb/turb_pp/julian_time/turb_pp_0k-300k_labeled.csv\"\n",
    ")\n",
    "\n",
    "# New data folder:\n",
    "AUGMENT_DATA_PATH = \"../Data/augmented_data/julian_format/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into pandas dataframes for better indexing:\n",
    "fDOM_raw = pd.DataFrame(fDOM_data)\n",
    "fDOM_raw.columns = [\"timestamp\", \"value\"]\n",
    "\n",
    "turb_raw = pd.DataFrame(turb_data)\n",
    "turb_raw.columns = [\"timestamp\", \"value\"]\n",
    "\n",
    "stage_raw = pd.DataFrame(stage_data)\n",
    "stage_raw.columns = [\"timestamp\", \"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualize data\n",
    "\"\"\"\n",
    "fig = plt.figure()\n",
    "x = fDOM_raw['timestamp']\n",
    "y = turb_raw['value']\n",
    "\n",
    "line_fdom = plt.Line2D(fDOM_raw['timestamp'], fDOM_raw['value'])\n",
    "line_turb = plt.Line2D(turb_raw['timestamp'], turb_raw['value'], color='red')\n",
    "line_stage = plt.Line2D(stage_raw['timestamp'], stage_raw['value'], color='orange')\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.add_line(line_fdom)\n",
    "ax.add_line(line_turb)\n",
    "ax.add_line(line_stage)\n",
    "ax.set_xlim(min(x), max(x))\n",
    "ax.set_ylim(min(y) - 10, max(y) + 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting Data\n",
    "We will augment data for each type of peak, and for each measurement.\n",
    "\n",
    "Starting with fDOM:\n",
    "1. PLP (plummeting peak)\n",
    "2. PP (phantom peak)\n",
    "3. SKP (skyrocketing peak)\n",
    "\n",
    "TODO: augment more peak types when they are labeled\n",
    "\n",
    "With turbidity:\n",
    "1. PP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting fDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "#                                                                                   #\n",
    "#                               DATAFRAME SETUP SECTION                             #\n",
    "#                                                                                   #\n",
    "#####################################################################################\n",
    "\n",
    "# new dataframes for augmented labeled/raw fDOM\n",
    "augmented_fDOM_raw = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "augmented_fDOM_labeled = pd.DataFrame(\n",
    "    columns=[\"timestamp_of_peak\", \"value_of_peak\", \"label_of_peak\", \"idx_of_peak\"]\n",
    ")\n",
    "\n",
    "# dataframe for augmented stage\n",
    "augmented_stage_raw_fdom = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "\n",
    "# dataframes for augmented raw/labeled turbidity\n",
    "augmented_turb_raw_fdom = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "augmented_turb_labeled_fdom = pd.DataFrame(\n",
    "    columns=[\"timestamp_of_peak\", \"value_of_peak\", \"label_of_peak\", \"idx_of_peak\"]\n",
    ")\n",
    "\n",
    "# use scipy find peaks to get detected peaks\n",
    "# need to iterate over every labeled peak, and get the beginning and end from a specific list somehow\n",
    "\n",
    "# variable to keep the last entry in the dataframe for stage\n",
    "# defaults to the last entry that was in fdom/turb raw csv files, in julian format\n",
    "prev_added_entry = STARTING_TIMESTAMP\n",
    "\n",
    "# a list of peaks that don't align with the fDOM raw file that was aligned with stage\n",
    "# i believe its just peaks that don't align with stage in general for whatever reason\n",
    "missed_fDOM_peaks = []\n",
    "\n",
    "# ~~~~~ Collect starting and ending points of each fDOM peak type ~~~~~\n",
    "# PP\n",
    "fdom_pp_index_lookup = get_cands_fDOM_PP()\n",
    "\n",
    "## SKP\n",
    "fdom_skp_index_lookup = get_cands_fDOM_SKP()\n",
    "\n",
    "## PLP\n",
    "fdom_plp_index_lookup = get_cands_fDOM_PLP()\n",
    "\n",
    "# non anomaly peaks\n",
    "fdom_NAP_index_lookup = get_cands_fDOM_NAP()\n",
    "\n",
    "#####################################################################################\n",
    "#                                                                                   #\n",
    "#                               AUGMENT DATA SECTION                                #\n",
    "#                                                                                   #\n",
    "#####################################################################################\n",
    "\n",
    "for iteration in range(ITERATIONS):\n",
    "    \"\"\"\n",
    "    Re-sample the fDOM labeled peaks to add variance to data\n",
    "    \"\"\"\n",
    "    # labeled fDOM peaks\n",
    "    fdom_anon_peaks = fDOM_labeled[fDOM_labeled[\"label_of_peak\"] != \"NAP\"]\n",
    "    # randomize the order, to add more\n",
    "    fdom_anon_peaks = fdom_anon_peaks.sample(frac=1).reset_index(\n",
    "        drop=True\n",
    "    )  # reset index as values were removed\n",
    "\n",
    "    # labeled non anomaly fDOM peaks\n",
    "    fdom_NAP = fDOM_labeled[fDOM_labeled[\"label_of_peak\"] == \"NAP\"]\n",
    "    fdom_NAP = fdom_NAP.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # iterate over each peak\n",
    "    for i, row in fdom_anon_peaks.iterrows():\n",
    "        # Get raw fDOM data points\n",
    "        label_of_peak = fdom_anon_peaks.loc[i, \"label_of_peak\"]\n",
    "\n",
    "        prev_dist = TIME_RANGE_INIT\n",
    "        next_dist = TIME_RANGE_INIT\n",
    "\n",
    "        prev_dist_NAP = TIME_RANGE_INIT\n",
    "        next_dist_NAP = TIME_RANGE_INIT\n",
    "\n",
    "        # set cands_df based on our peak label\n",
    "        if label_of_peak == \"PP\":\n",
    "            cands_df = fdom_pp_index_lookup\n",
    "        elif label_of_peak == \"PLP\":\n",
    "            cands_df = fdom_plp_index_lookup\n",
    "        elif label_of_peak == \"SKP\":\n",
    "            cands_df = fdom_skp_index_lookup\n",
    "\n",
    "        timestamp_of_peak = fdom_anon_peaks.loc[i, \"timestamp_of_peak\"]\n",
    "\n",
    "        NAP_index = fdom_NAP.loc[i, \"idx_of_peak\"]\n",
    "        timestamp_NAP = fDOM_raw.loc[NAP_index, \"timestamp\"]\n",
    "\n",
    "        # get index dataframes of each type\n",
    "        fdom_index_df = fDOM_raw[fDOM_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "        stage_index_df = stage_raw[stage_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "        turb_index_df = turb_raw[turb_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "\n",
    "        # get indices of non anomaly data\n",
    "        stage_index_df_NAP = stage_raw[stage_raw[\"timestamp\"] == timestamp_NAP]\n",
    "        turb_index_df_NAP = turb_raw[turb_raw[\"timestamp\"] == timestamp_NAP]\n",
    "\n",
    "        if len(fdom_index_df.index.to_list()) != 0:\n",
    "            # get indices of each data type from index df's\n",
    "            index_of_peak = fdom_index_df.index.tolist()[0]\n",
    "            stage_index = stage_index_df.index.tolist()[0]\n",
    "            turb_index = turb_index_df.index.tolist()[0]\n",
    "\n",
    "            # get indices of NAP (might break, try except to catch that)\n",
    "            stage_index_NAP = stage_index_df_NAP.index.tolist()[0]\n",
    "            turb_index_NAP = turb_index_df_NAP.index.tolist()[0]\n",
    "\n",
    "            # call get ends of peak to get beginning and ending of peaks\n",
    "            try:\n",
    "                left, right = get_ends_of_peak(cands_df, index_of_peak)\n",
    "\n",
    "                prev_dist = abs(index_of_peak - left)\n",
    "                next_dist = abs(index_of_peak - right)\n",
    "\n",
    "            except IndexError as e:\n",
    "                # something went wrong, just set prev_dist and next dist to time range init\n",
    "                prev_dist = TIME_RANGE_INIT\n",
    "                next_dist = TIME_RANGE_INIT\n",
    "                print(e)\n",
    "\n",
    "            # try except for nap\n",
    "            try:\n",
    "                left_NAP, right_NAP = get_ends_of_peak(fdom_NAP_index_lookup, NAP_index)\n",
    "\n",
    "                prev_dist_NAP = abs(NAP_index - left_NAP)\n",
    "                next_dist_NAP = abs(NAP_index - right_NAP)\n",
    "\n",
    "            except IndexError as e:\n",
    "                prev_dist_NAP = TIME_RANGE_INIT\n",
    "                next_dist_NAP = TIME_RANGE_INIT\n",
    "                print(\"INDEX ERROR: \" + str(e))\n",
    "\n",
    "            # use this timestamp to make a dataframe of raw stuff\n",
    "            # get data from fDOM_raw file\n",
    "            # call build temp dataframes to build df's:\n",
    "            new_fdom_raw, new_stage, new_turb_raw = build_temp_dataframes(\n",
    "                fDOM_raw,\n",
    "                stage_raw,\n",
    "                turb_raw,\n",
    "                prev_dist,\n",
    "                next_dist,\n",
    "                index_of_peak,\n",
    "                stage_index,\n",
    "                turb_index,\n",
    "            )\n",
    "\n",
    "            new_fdom_raw_NAP, new_stage_NAP, new_turb_raw_NAP = build_temp_dataframes(\n",
    "                fDOM_raw,\n",
    "                stage_raw,\n",
    "                turb_raw,\n",
    "                prev_dist_NAP,\n",
    "                next_dist_NAP,\n",
    "                NAP_index,\n",
    "                stage_index_NAP,\n",
    "                turb_index_NAP,\n",
    "            )\n",
    "\n",
    "            #####################################################################################\n",
    "            #                                                                                   #\n",
    "            #                               ACTUAL CHANGES TO DATA                              #\n",
    "            #                                                                                   #\n",
    "            #####################################################################################\n",
    "\n",
    "            # decide whether to augment Non anomaly peak first, or anomaly peak first\n",
    "            which = random.randint(0, 1)\n",
    "\n",
    "            # if which is 0, augment NAP first\n",
    "            if which == 0:\n",
    "                # TODO: Add data smoothing here from last augment section to this new section\n",
    "                # make this a function\n",
    "\n",
    "                # ~~~~~~~~~~~ AUGMENTING NAP ~~~~~~~~~\n",
    "                # augment data\n",
    "                new_fdom_raw_NAP = augment_data(new_fdom_raw_NAP, NAP_index)\n",
    "\n",
    "                (\n",
    "                    new_label_NAP,\n",
    "                    new_fdom_raw_NAP,\n",
    "                    new_stage_NAP,\n",
    "                    new_turb_raw_NAP,\n",
    "                    new_time_entry,\n",
    "                ) = update_dataframes(\n",
    "                    prev_added_entry,\n",
    "                    new_fdom_raw_NAP,\n",
    "                    NAP_index,\n",
    "                    prev_dist_NAP,\n",
    "                    augmented_fDOM_raw,\n",
    "                    new_stage_NAP,\n",
    "                    new_turb_raw_NAP,\n",
    "                    \"NAP\",  # NAP label is just NAP\n",
    "                )\n",
    "\n",
    "                # call concat dataframes to put the rest together\n",
    "                (\n",
    "                    augmented_fDOM_labeled,\n",
    "                    augmented_fDOM_raw,\n",
    "                    augmented_stage_raw_fdom,\n",
    "                    augmented_turb_raw_fdom,\n",
    "                ) = concat_dataframes(\n",
    "                    augmented_fDOM_labeled,\n",
    "                    augmented_fDOM_raw,\n",
    "                    augmented_stage_raw_fdom,\n",
    "                    augmented_turb_raw_fdom,\n",
    "                    new_fdom_raw_NAP,\n",
    "                    new_stage_NAP,\n",
    "                    new_turb_raw_NAP,\n",
    "                    new_label_NAP,\n",
    "                )\n",
    "\n",
    "                # update prev time entry\n",
    "                prev_added_entry = new_time_entry\n",
    "\n",
    "                # TODO: Add data smoothing here from last augment section to this new section\n",
    "\n",
    "                ####### AUGMENT ANOMALY PEAK #######\n",
    "\n",
    "                # call augment data function\n",
    "                new_fdom_raw = augment_data(new_fdom_raw, index_of_peak)\n",
    "\n",
    "                # call update dataframes to set new indices and timestamps\n",
    "                (\n",
    "                    new_label,\n",
    "                    new_fdom_raw,\n",
    "                    new_stage,\n",
    "                    new_turb_raw,\n",
    "                    new_time_entry,\n",
    "                ) = update_dataframes(\n",
    "                    prev_added_entry,\n",
    "                    new_fdom_raw,\n",
    "                    index_of_peak,\n",
    "                    prev_dist,\n",
    "                    augmented_fDOM_raw,\n",
    "                    new_stage,\n",
    "                    new_turb_raw,\n",
    "                    label_of_peak,\n",
    "                )\n",
    "\n",
    "                # concat the rest of the peaks\n",
    "                (\n",
    "                    augmented_fDOM_labeled,\n",
    "                    augmented_fDOM_raw,\n",
    "                    augmented_stage_raw_fdom,\n",
    "                    augmented_turb_raw_fdom,\n",
    "                ) = concat_dataframes(\n",
    "                    augmented_fDOM_labeled,\n",
    "                    augmented_fDOM_raw,\n",
    "                    augmented_stage_raw_fdom,\n",
    "                    augmented_turb_raw_fdom,\n",
    "                    new_fdom_raw,\n",
    "                    new_stage,\n",
    "                    new_turb_raw,\n",
    "                    new_label,\n",
    "                )\n",
    "\n",
    "                # update prev time entry\n",
    "                prev_added_entry = new_time_entry\n",
    "\n",
    "            # else, augment anomaly first\n",
    "            else:\n",
    "                # TODO: Add data smoothing here from last augment section to this new section\n",
    "\n",
    "                ####### AUGMENT ANOMALY PEAK #######\n",
    "                # call augment data function\n",
    "                new_fdom_raw = augment_data(new_fdom_raw, index_of_peak)\n",
    "\n",
    "                # call update dataframes to set new indices and timestamps\n",
    "                (\n",
    "                    new_label,\n",
    "                    new_fdom_raw,\n",
    "                    new_stage,\n",
    "                    new_turb_raw,\n",
    "                    new_time_entry,\n",
    "                ) = update_dataframes(\n",
    "                    prev_added_entry,\n",
    "                    new_fdom_raw,\n",
    "                    index_of_peak,\n",
    "                    prev_dist,\n",
    "                    augmented_fDOM_raw,\n",
    "                    new_stage,\n",
    "                    new_turb_raw,\n",
    "                    label_of_peak,\n",
    "                )\n",
    "\n",
    "                # concat the rest of the peaks\n",
    "                (\n",
    "                    augmented_fDOM_labeled,\n",
    "                    augmented_fDOM_raw,\n",
    "                    augmented_stage_raw_fdom,\n",
    "                    augmented_turb_raw_fdom,\n",
    "                ) = concat_dataframes(\n",
    "                    augmented_fDOM_labeled,\n",
    "                    augmented_fDOM_raw,\n",
    "                    augmented_stage_raw_fdom,\n",
    "                    augmented_turb_raw_fdom,\n",
    "                    new_fdom_raw,\n",
    "                    new_stage,\n",
    "                    new_turb_raw,\n",
    "                    new_label,\n",
    "                )\n",
    "\n",
    "                # update prev time entry\n",
    "                prev_added_entry = new_time_entry\n",
    "\n",
    "                # TODO: Add data smoothing here from last augment section to this new section\n",
    "\n",
    "                # ~~~~~~~~~~~ AUGMENTING NAP ~~~~~~~~~\n",
    "                # augment data\n",
    "                new_fdom_raw_NAP = augment_data(new_fdom_raw_NAP, NAP_index)\n",
    "\n",
    "                (\n",
    "                    new_label_NAP,\n",
    "                    new_fdom_raw_NAP,\n",
    "                    new_stage_NAP,\n",
    "                    new_turb_raw_NAP,\n",
    "                    new_time_entry,\n",
    "                ) = update_dataframes(\n",
    "                    prev_added_entry,\n",
    "                    new_fdom_raw_NAP,\n",
    "                    NAP_index,\n",
    "                    prev_dist_NAP,\n",
    "                    augmented_fDOM_raw,\n",
    "                    new_stage_NAP,\n",
    "                    new_turb_raw_NAP,\n",
    "                    \"NAP\",  # NAP label is just NAP\n",
    "                )\n",
    "\n",
    "                # call concat dataframes to put the rest together\n",
    "                (\n",
    "                    augmented_fDOM_labeled,\n",
    "                    augmented_fDOM_raw,\n",
    "                    augmented_stage_raw_fdom,\n",
    "                    augmented_turb_raw_fdom,\n",
    "                ) = concat_dataframes(\n",
    "                    augmented_fDOM_labeled,\n",
    "                    augmented_fDOM_raw,\n",
    "                    augmented_stage_raw_fdom,\n",
    "                    augmented_turb_raw_fdom,\n",
    "                    new_fdom_raw_NAP,\n",
    "                    new_stage_NAP,\n",
    "                    new_turb_raw_NAP,\n",
    "                    new_label_NAP,\n",
    "                )\n",
    "\n",
    "                # update prev time entry\n",
    "                prev_added_entry = new_time_entry\n",
    "\n",
    "        else:\n",
    "            # we missed some data points, append them to the missed data dataframe\n",
    "            missed_fDOM_peaks.append(timestamp_of_peak)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell allows you to print out the augmented dataframes in full\n",
    "\"\"\"\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 10)\n",
    "\n",
    "print(\"Labeled Peaks Augmented\")\n",
    "print(augmented_fDOM_labeled)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Raw fDOM Augmented\")\n",
    "print(augmented_fDOM_raw)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Raw Stage Augmented\")\n",
    "print(augmented_stage_raw_fdom)\n",
    "print(\"\\n\")\n",
    "\n",
    "# print(\"Raw Turbidity Augmented\")\n",
    "# print(augmented_turb_raw_fdom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualize data with matplotlib\n",
    "\"\"\"\n",
    "fig = plt.figure()\n",
    "x = augmented_turb_raw_fdom['timestamp']\n",
    "y = augmented_turb_raw_fdom['value']\n",
    "\n",
    "line_fdom = plt.Line2D(augmented_fDOM_raw['timestamp'], augmented_fDOM_raw['value'])\n",
    "line_turb = plt.Line2D(augmented_turb_raw_fdom['timestamp'], augmented_turb_raw_fdom['value'], color='red')\n",
    "line_stage = plt.Line2D(augmented_stage_raw_fdom['timestamp'], augmented_stage_raw_fdom['value'], color='orange')\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.add_line(line_fdom)\n",
    "ax.add_line(line_turb)\n",
    "ax.add_line(line_stage)\n",
    "ax.set_xlim(min(x), max(x))\n",
    "ax.set_ylim(min(y) - 10, max(y) + 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment turbidity data\n",
    "The following code blocks augment turbidity data.\n",
    "NOTE: This is currently nonfunctional, and needs to be rewritten using the new functions written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "#                                                                                   #\n",
    "#                               DATAFRAME SETUP SECTION                             #\n",
    "#                                                                                   #\n",
    "#####################################################################################\n",
    "\n",
    "# TODO: REDO THIS USING NEW FUNCTIONS\n",
    "# FIXME: This is currently broken, the peak value is wrong\n",
    "\n",
    "# labeled turb\n",
    "augmented_turb_labeled = pd.DataFrame(\n",
    "    columns=[\"timestamp_of_peak\", \"value_of_peak\", \"label_of_peak\", \"idx_of_peak\"]\n",
    ")\n",
    "\n",
    "# raw turb\n",
    "augmented_turb_raw = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "\n",
    "# raw fdom\n",
    "augmented_fDOM_raw_turb = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "\n",
    "# raw stage\n",
    "augmented_stage_raw_turb = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "\n",
    "# variable to keep the last entry in the dataframe for stage\n",
    "# defaults to the last entry that was in fdom/turb raw csv files, in julian format\n",
    "prev_added_entry = STARTING_TIMESTAMP\n",
    "\n",
    "# a list of peaks that don't align with the fDOM raw file that was aligned with stage\n",
    "# i believe its just peaks that don't align with stage in general for whatever reason\n",
    "missed_turb_peaks = []\n",
    "\n",
    "# ~~~~~ Collect starting and ending points of each turb peak type ~~~~~~\n",
    "turb_pp_index_lookup = get_cands_turb_PP()\n",
    "\n",
    "\n",
    "new_peak_val = 5\n",
    "\n",
    "#####################################################################################\n",
    "#                                                                                   #\n",
    "#                               AUGMENT DATA SECTION                                #\n",
    "#                                                                                   #\n",
    "#####################################################################################\n",
    "\n",
    "\"\"\" Augment turbidity data by calling previously written function \"\"\"\n",
    "for iteration in range(ITERATIONS):\n",
    "    \"\"\" Resample turb labeled peaks at each iteration for more variance \"\"\"\n",
    "    # labeled turb peaks\n",
    "    turb_anon_peaks = turb_labeled[turb_labeled[\"label_of_peak\"] != \"NPP\"]\n",
    "    turb_anon_peaks = turb_anon_peaks.sample(frac=1).reset_index()\n",
    "\n",
    "    # iterate over each peak\n",
    "    for i, row in turb_anon_peaks.iterrows():\n",
    "\n",
    "        \"\"\"Get raw turb data points\"\"\"\n",
    "        timestamp_of_peak = turb_anon_peaks.loc[i, \"timestamp_of_peak\"]\n",
    "        label_of_peak = turb_anon_peaks.loc[i, \"label_of_peak\"]\n",
    "\n",
    "        # get index dataframes of each type\n",
    "        fdom_index_df = fDOM_raw[fDOM_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "        stage_index_df = stage_raw[stage_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "        turb_index_df = turb_raw[turb_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "\n",
    "        if len(turb_index_df.index.to_list()) != 0:\n",
    "            # get indices of each data type from index df's\n",
    "            index_of_peak = turb_index_df.index.tolist()[0]\n",
    "            stage_index = stage_index_df.index.tolist()[0]\n",
    "            fdom_index = fdom_index_df.index.tolist()[0]\n",
    "\n",
    "            # check to see if any overlap occurs between peaks\n",
    "            prev_dist = TIME_RANGE_INIT\n",
    "            next_dist = TIME_RANGE_INIT\n",
    "\n",
    "            if label_of_peak == \"PP\":\n",
    "                cands_df = turb_pp_index_lookup\n",
    "\n",
    "            try:\n",
    "                left, right = get_ends_of_peak(cands_df, index_of_peak)\n",
    "\n",
    "                prev_dist = abs(index_of_peak - left)\n",
    "                next_dist = abs(index_of_peak - right)\n",
    "\n",
    "            except IndexError:\n",
    "                # index wrong, reset prev dist\n",
    "                print(\"INDEX ERROR!\")\n",
    "                prev_dist = TIME_RANGE_INIT\n",
    "                next_dist = TIME_RANGE_INIT \n",
    "\n",
    "            # use this timestamp to make a dataframe of raw stuff\n",
    "            # get data from fDOM_raw file\n",
    "            fDOM_raw_time_range = pd.DataFrame(\n",
    "                fDOM_raw.iloc[fdom_index - prev_dist : fdom_index + next_dist + 1]\n",
    "            )\n",
    "\n",
    "            # get stage data range\n",
    "            stage_time_range = pd.DataFrame(\n",
    "                stage_raw.iloc[stage_index - prev_dist : stage_index + next_dist + 1]\n",
    "            )\n",
    "\n",
    "            # get turbidity data range\n",
    "            turb_time_range = pd.DataFrame(\n",
    "                turb_raw.iloc[index_of_peak - prev_dist : index_of_peak + next_dist + 1]\n",
    "            )\n",
    "\n",
    "            # make a copy of the modified data\n",
    "            new_fdom_raw = copy.deepcopy(fDOM_raw_time_range)\n",
    "            new_stage = copy.deepcopy(stage_time_range)\n",
    "            new_turb_raw = copy.deepcopy(turb_time_range)\n",
    "\n",
    "            # peak index can change when we add in x data\n",
    "            new_fdom_peak_index = -1\n",
    "            new_peak_timestamp = -1\n",
    "            \n",
    "            #####################################################################################\n",
    "            #                                                                                   #\n",
    "            #                               ACTUAL CHANGES TO DATA                              #\n",
    "            #                                                                                   #\n",
    "            #####################################################################################\n",
    "\n",
    "            # gen random num to decide whether to widen or heighten\n",
    "            random.seed() \n",
    "            widen_or_heighten = random.randint(0, 1)\n",
    "\n",
    "            # if 0, widen the peak\n",
    "            if widen_or_heighten == 0:\n",
    "               # get current peak val\n",
    "                main_peak_val = new_turb_raw.loc[index_of_peak, \"value\"]\n",
    "\n",
    "                # gen a random number between 0.01 and 0.2 to increase height by for previous and next peak\n",
    "                multiplier = random.uniform(0.001, 0.2)\n",
    "                \n",
    "                # new peak vals\n",
    "                val_before_peak = new_turb_raw.loc[index_of_peak - 1, \"value\"] * (1 + multiplier)\n",
    "                val_after_peak = new_turb_raw.loc[index_of_peak + 1, \"value\"] * (1 + multiplier)\n",
    "\n",
    "                # ensure new value does not go over main peak val\n",
    "                fail_count = 0\n",
    "                can_widen = True\n",
    "                while val_before_peak > main_peak_val and val_after_peak > main_peak_val:\n",
    "                    fail_count += 1 \n",
    "                    if fail_count > 20:\n",
    "                        # not possible to fix peak, break and forget about it\n",
    "                        can_widen = False\n",
    "                        break \n",
    "                    \n",
    "                    # re gen num\n",
    "                    multiplier = random.uniform(0.001, 0.2)\n",
    "                    val_before_peak = new_turb_raw.loc[index_of_peak - 1, \"value\"] * (1 + multiplier)\n",
    "                    val_after_peak = new_turb_raw.loc[index_of_peak + 1, \"value\"] * (1 + multiplier) \n",
    "                \n",
    "                if can_widen:\n",
    "                    # set new vals\n",
    "                    new_turb_raw.loc[index_of_peak - 1, \"value\"] = val_before_peak\n",
    "                    new_turb_raw.loc[index_of_peak + 1, \"value\"] = val_after_peak \n",
    "            \n",
    "            # else, lets heighten the peak\n",
    "            else:\n",
    "                random_val = random.uniform(LOWER_BOUND_AMPLITUDE_MULTIPLIER, UPPER_BOUND_AMPLITUDE_MULTIPLIER)\n",
    "                new_peak_val = new_turb_raw.loc[index_of_peak, \"value\"] * (1 + random_val)\n",
    "                new_turb_raw.loc[index_of_peak, \"value\"] = new_peak_val\n",
    "\n",
    "            # get the next possible timestamp\n",
    "            new_time_entry = next_time_entry(prev_added_entry)\n",
    "\n",
    "            # update all timestamps for augmented data\n",
    "            for i, row in new_turb_raw.iterrows():\n",
    "                # if timestamps equal, we have the relative peak\n",
    "                if new_turb_raw.loc[i, \"timestamp\"] == timestamp_of_peak:\n",
    "                    # register index here\n",
    "                    new_turb_peak_index = get_last_augment_index(augmented_turb_raw)\n",
    "                    new_peak_timestamp = new_time_entry\n",
    "\n",
    "                # update timestamps\n",
    "                new_fdom_raw.loc[i, \"timestamp\"] = new_time_entry\n",
    "                new_stage.loc[i, \"timestamp\"] = new_time_entry\n",
    "                new_turb_raw.loc[i, \"timestamp\"] = new_time_entry\n",
    "\n",
    "                # get next time stamp\n",
    "                new_time_entry = next_time_entry(new_time_entry)\n",
    "\n",
    "            # add entries to turb\n",
    "            augmented_turb_raw = pd.concat(\n",
    "                [augmented_turb_raw, new_turb_raw], ignore_index=True\n",
    "            )\n",
    "\n",
    "            new_label = pd.DataFrame(\n",
    "                [\n",
    "                    [\n",
    "                        new_peak_timestamp,\n",
    "                        new_peak_val,\n",
    "                        label_of_peak,\n",
    "                        new_turb_peak_index,\n",
    "                    ]\n",
    "                ],\n",
    "                columns=[\n",
    "                    \"timestamp_of_peak\",\n",
    "                    \"value_of_peak\",\n",
    "                    \"label_of_peak\",\n",
    "                    \"idx_of_peak\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            # add entries to labeled turb\n",
    "            augmented_turb_labeled = pd.concat([augmented_turb_labeled, new_label])\n",
    "\n",
    "            # add entries to stage\n",
    "            augmented_stage_raw_turb = pd.concat(\n",
    "                [augmented_stage_raw_turb, new_stage], ignore_index=True\n",
    "            )\n",
    "\n",
    "            # add entries into raw fDOM\n",
    "            augmented_fDOM_raw_turb = pd.concat(\n",
    "                [augmented_fDOM_raw_turb, new_fdom_raw], ignore_index=True\n",
    "            )\n",
    "\n",
    "            # update prev time entry\n",
    "            prev_added_entry = new_time_entry\n",
    "\n",
    "        else:\n",
    "            # we missed some data points, append them to the missed data dataframe\n",
    "            missed_fDOM_peaks.append(timestamp_of_peak)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell allows you to print out the augmented dataframes in full\n",
    "\"\"\"\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.width', 1000)\n",
    "# pd.set_option('display.colheader_justify', 'center')\n",
    "# pd.set_option('display.precision', 3)\n",
    "\n",
    "# print(\"Labeled Peaks Augmented\")\n",
    "# print(augmented_turb_labeled)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(\"Raw fDOM Augmented\")\n",
    "# print(augmented_fDOM_raw_turb)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(\"Raw Stage Augmented\")\n",
    "# print(augmented_stage_raw_turb)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(\"Raw Turbidity Augmented\")\n",
    "# print(augmented_turb_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualize data with matplotlib\n",
    "\"\"\"\n",
    "fig = plt.figure()\n",
    "x = augmented_turb_raw['timestamp']\n",
    "y = augmented_turb_raw['value']\n",
    "\n",
    "line_fdom = plt.Line2D(augmented_fDOM_raw_turb['timestamp'], augmented_fDOM_raw_turb['value'])\n",
    "line_turb = plt.Line2D(augmented_turb_raw['timestamp'], augmented_turb_raw['value'], color='red')\n",
    "line_stage = plt.Line2D(augmented_stage_raw_turb['timestamp'], augmented_stage_raw_turb['value'], color='orange')\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.add_line(line_fdom)\n",
    "ax.add_line(line_turb)\n",
    "ax.add_line(line_stage)\n",
    "ax.set_xlim(min(x), max(x))\n",
    "ax.set_ylim(min(y) - 10, max(y) + 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move augmented data into csv files\n",
    "The following codeblock creates csv files for the augmented data.\n",
    "\n",
    "### NOTE ON DATA:\n",
    "Due to the random sampling used when augmenting fDOM and turbidity,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Augmented Data Paths \"\"\"\n",
    "# trainset\n",
    "trainset_fdom_path = \"../Data/augmented_data/trainset_plotting/fdom/\"\n",
    "trainset_turb_path = \"../Data/augmented_data/trainset_plotting/turb/\"\n",
    "\n",
    "# unlabeled data\n",
    "unlabeled_fdom_path = \"../Data/augmented_data/fdom/unlabeled/\"\n",
    "unlabeled_turb_path = \"../Data/augmented_data/turb/unlabeled/\"\n",
    "\n",
    "# labeled data\n",
    "labeled_fdom_path = \"../Data/augmented_data/fdom/labeled/\"\n",
    "labeled_turb_path = \"../Data/augmented_data/turb/labeled/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_augmented_data_to_csv():\n",
    "    # call to_csv for each dataframe\n",
    "    # for each dataframe, we also drop the index\n",
    "\n",
    "    # write fDOM augmented data\n",
    "    augmented_fDOM_labeled.to_csv(labeled_fdom_path + 'labeled_fdom_peaks.csv', index=False)\n",
    "    augmented_fDOM_raw.to_csv(unlabeled_fdom_path + 'unlabeled_fdom.csv', index=False)\n",
    "    augmented_turb_raw_fdom.to_csv(unlabeled_fdom_path + 'unlabeled_turb.csv', index=False)\n",
    "    augmented_stage_raw_fdom.to_csv(unlabeled_fdom_path + 'unlabeled_stage.csv', index=False)\n",
    "\n",
    "    # write turb augmented data\n",
    "    augmented_turb_labeled.to_csv(labeled_turb_path + 'labeled_turb_peaks.csv', index=False)\n",
    "    augmented_turb_raw.to_csv(unlabeled_turb_path + 'unlabeled_turb.csv', index=False)\n",
    "    augmented_fDOM_raw_turb.to_csv(unlabeled_turb_path + 'unlabeled_fdom.csv', index=False)\n",
    "    augmented_stage_raw_turb.to_csv(unlabeled_turb_path + 'unlabeled_stage.csv', index=False)\n",
    "\n",
    "\n",
    "def convert_df_julian_to_datetime(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts a dataframe julian timestamp to datetime ISO 8601 format\n",
    "\n",
    "    df: a dataframe\n",
    "\n",
    "    return: changed dataframe\n",
    "    \"\"\"\n",
    "    # iterate over dataframe, replacing timestamp vals\n",
    "    for i, row in df.iterrows():\n",
    "        df.loc[i, \"timestamp\"] = dp.julian_to_datetime(df.loc[i, \"timestamp\"]).isoformat()\n",
    "        \n",
    "        # add stupid 0.00Z to fit trainset format\n",
    "        df.loc[i, \"timestamp\"] = df.loc[i, \"timestamp\"] + \".000Z\"\n",
    "\n",
    "    return df\n",
    "\n",
    "def write_to_trainset_csv():\n",
    "    # TODO: add peak labels in\n",
    "\n",
    "    # start by creating a dataframe that has the correct columns\n",
    "    trainset_fdom_df = pd.DataFrame(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "    trainset_turb_df = pd.DataFrame(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "\n",
    "    # ~~~~~~~ fDOM section ~~~~~~~\n",
    "    # start by just adding the fDOM data into the series, need to replace all timestamps\n",
    "    fdom_trainset_raw = copy.deepcopy(augmented_fDOM_raw)\n",
    "    fdom_turb_trainset_raw = copy.deepcopy(augmented_turb_raw_fdom)\n",
    "    fdom_stage_trainset_raw = copy.deepcopy(augmented_stage_raw_fdom)\n",
    "    \n",
    "    # convert timestamps to julian\n",
    "    fdom_trainset_raw = convert_df_julian_to_datetime(fdom_trainset_raw)\n",
    "    fdom_turb_trainset_raw = convert_df_julian_to_datetime(fdom_turb_trainset_raw)\n",
    "    fdom_stage_trainset_raw = convert_df_julian_to_datetime(fdom_stage_trainset_raw)\n",
    "\n",
    "    # add in new values\n",
    "    fdom_trainset_raw[\"series\"] = \"fDOM\"\n",
    "    fdom_trainset_raw[\"label\"] = \"\"\n",
    "\n",
    "    fdom_turb_trainset_raw[\"series\"] = \"turb\"\n",
    "    fdom_turb_trainset_raw[\"label\"] = \"\"\n",
    "\n",
    "    fdom_stage_trainset_raw[\"series\"] = \"stage\"\n",
    "    fdom_stage_trainset_raw[\"label\"] = \"\"\n",
    "\n",
    "    # reorder columns\n",
    "    fdom_trainset_raw = fdom_trainset_raw.reindex(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "    fdom_turb_trainset_raw = fdom_turb_trainset_raw.reindex(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "    fdom_stage_trainset_raw = fdom_stage_trainset_raw.reindex(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "\n",
    "    # concat into single dataframe\n",
    "    trainset_fdom_df = pd.concat([fdom_trainset_raw, fdom_turb_trainset_raw, fdom_stage_trainset_raw])\n",
    "\n",
    "    # sort together\n",
    "    trainset_fdom_df = trainset_fdom_df.sort_values(by=['timestamp'], kind='stable')\n",
    "\n",
    "    # export to csv\n",
    "    trainset_fdom_df.to_csv(trainset_fdom_path + \"fdom_augmented.csv\", index=False)\n",
    "\n",
    "    # ~~~~~~~ turbidity section ~~~~~~~\n",
    "\n",
    "    # create new dataframes\n",
    "    turb_trainset_raw = copy.deepcopy(augmented_turb_raw)\n",
    "    turb_fdom_trainset_raw = copy.deepcopy(augmented_fDOM_raw_turb)\n",
    "    turb_stage_trainset_raw = copy.deepcopy(augmented_stage_raw_turb)\n",
    "\n",
    "    # convert timestamps\n",
    "    turb_trainset_raw = convert_df_julian_to_datetime(turb_trainset_raw)\n",
    "    turb_fdom_trainset_raw = convert_df_julian_to_datetime(turb_fdom_trainset_raw)\n",
    "    turb_stage_trainset_raw = convert_df_julian_to_datetime(turb_stage_trainset_raw)\n",
    "\n",
    "    # add in new values\n",
    "    turb_fdom_trainset_raw[\"series\"] = \"fDOM\"\n",
    "    turb_fdom_trainset_raw[\"label\"] = \"\"\n",
    "\n",
    "    turb_trainset_raw[\"series\"] = \"turb\"\n",
    "    turb_trainset_raw[\"label\"] = \"\"\n",
    "\n",
    "    turb_stage_trainset_raw[\"series\"] = \"stage\"\n",
    "    turb_stage_trainset_raw[\"label\"] = \"\"\n",
    "\n",
    "    # reorder columns\n",
    "    turb_fdom_trainset_raw = turb_fdom_trainset_raw.reindex(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "    turb_trainset_raw = turb_trainset_raw.reindex(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "    turb_stage_trainset_raw = turb_stage_trainset_raw.reindex(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "\n",
    "    # concat into single dataframe\n",
    "    trainset_turb_df = pd.concat([turb_fdom_trainset_raw, turb_trainset_raw, turb_stage_trainset_raw])\n",
    "\n",
    "    # sort together\n",
    "    trainset_turb_df = trainset_turb_df.sort_values(by=['timestamp'], kind='stable')\n",
    "\n",
    "    # export to csv\n",
    "    trainset_turb_df.to_csv(trainset_turb_path + \"turb_augmented.csv\", index=False)\n",
    "\n",
    "# write_augmented_data_to_csv()\n",
    "write_to_trainset_csv()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f14ca78382e4dc227f1e51b680612414c34712d66eddea25f1c0d0c728b236b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('anomaly-detection': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
