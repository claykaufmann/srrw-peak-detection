{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenting time-series data\n",
    "In this file, the data is augmented in order to create more of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.insert(1,'../')\n",
    "import Tools.data_processing as dp\n",
    "import Tools.data_movement as dm \n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "The following functions provide useful tools for the augmentation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def make_small_change():\n",
    "    \"\"\"\n",
    "    Makes changes to the current datapoints\n",
    "    \n",
    "    param: the range of datapoints by index\n",
    "\n",
    "    returns: the new data range to be appended to the data\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def next_time_entry(current_entry: float) -> float:\n",
    "    \"\"\"\n",
    "    This function returns the next time entry in julian time\n",
    "    \n",
    "    current_entry: a julina time float\n",
    "\n",
    "    return: julian time + 15 minutes from past julian time\n",
    "    \"\"\"\n",
    "\n",
    "    # convert julian to datetime\n",
    "    date_time_init = dp.julian_to_datetime(current_entry)\n",
    "\n",
    "    # find next date time (add 15 minutes)\n",
    "    next_entry = date_time_init + timedelta(minutes=15)\n",
    "\n",
    "    # convert date time to julian time\n",
    "    final_julian_time = dp.datetime_to_julian(next_entry)\n",
    "\n",
    "    # return julian time\n",
    "    return final_julian_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in data\n",
    "The knowledge-based approach uses the data in `Data/converted_data/julian_format/`, so that is where the data augmentation will go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in raw data\n",
    "fDOM_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv')\n",
    "stage_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/stage_10.1.11-1.1.19.csv')\n",
    "turb_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv')\n",
    "\n",
    "# align stage to fDOM\n",
    "stage_data = dp.align_stage_to_fDOM(fDOM_data, stage_data)\n",
    "\n",
    "# read in labeled fDOM\n",
    "fDOM_labeled = pd.read_csv(\"../Data/labeled_data/ground_truths/fDOM/fDOM_all_julian_0k-300k.csv\")\n",
    "\n",
    "turb_labeled = pd.read_csv(\"../Data/labeled_data/ground_truths/turb/turb_pp/julian_time/turb_pp_0k-300k_labeled.csv\")\n",
    "\n",
    "# New data folder:\n",
    "AUGMENT_DATA_PATH = '../Data/augmented_data/julian_format/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into pandas dataframes for better indexing:\n",
    "fDOM_raw = pd.DataFrame(fDOM_data)\n",
    "fDOM_raw.columns = [\"timestamp\", \"value\"]\n",
    "\n",
    "turb_raw = pd.DataFrame(turb_data)\n",
    "turb_raw.columns = [\"timestamp\", \"value\"]\n",
    "\n",
    "stage_raw = pd.DataFrame(stage_data)\n",
    "stage_raw.columns = [\"timestamp\", \"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data\n",
    "timestamp_of_peak = fDOM_labeled.loc[88, 'timestamp_of_peak']\n",
    "index_df = fDOM_raw[fDOM_raw['timestamp'] == timestamp_of_peak]\n",
    "\n",
    "if len(index_df.index.to_list()) != 0:\n",
    "    index_of_peak = index_df.index.tolist()[0]\n",
    "\n",
    "    print(index_of_peak)\n",
    "    print(timestamp_of_peak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting Data\n",
    "We will augment data for each type of peak, and for each measurement.\n",
    "\n",
    "Starting with fDOM:\n",
    "1. PLP (plummeting peak)\n",
    "2. PP (phantom peak)\n",
    "3. SKP (skyrocketing peak)\n",
    "\n",
    "TODO: augment more peak types when they are labeled\n",
    "\n",
    "With turbidity:\n",
    "1. PP\n",
    "\n",
    "With stage:\n",
    "Unsure on this section so far.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of anomaly peaks from labeled fDOM data\n",
    "anom_peaks = fDOM_labeled[fDOM_labeled['label_of_peak'] != 'NAP']\n",
    "\n",
    "# reset the index as we removed many values\n",
    "anom_peaks = anom_peaks.reset_index()\n",
    "\n",
    "# create a dataframe to read in augmented fDOM data into\n",
    "augmented_fDOM_raw = pd.DataFrame()\n",
    "\n",
    "missed_fDOM_peaks = []\n",
    "\n",
    "# iterate over each peak\n",
    "for i, row in anom_peaks.iterrows():\n",
    "    # check to see if any overlap occurs between peaks\n",
    "    prev_dist = 5\n",
    "    next_dist = 5\n",
    "\n",
    "    if i == 0:\n",
    "        # we are at the first peak, check to see if there are 5 data points behind\n",
    "        # TODO: implement this\n",
    "        pass\n",
    "\n",
    "    elif i + 1 < anom_peaks.shape[0]:\n",
    "        # anywhere else in the middle, check for overlap\n",
    "\n",
    "        # check next 5\n",
    "        if row['idx_of_peak'] + 5 >= anom_peaks.loc[i + 1,'idx_of_peak'] - 5: # -5 becase we go back 5 peaks too\n",
    "            # change next_dist to whatever it needs to be \n",
    "            curr_dist_to_peak = abs(row['idx_of_peak'] - anom_peaks.loc[i + 1, 'idx_of_peak'] - 5)\n",
    "            next_dist = curr_dist_to_peak - 1\n",
    "\n",
    "        # check past 5\n",
    "        if row['idx_of_peak'] - 5 <= anom_peaks.loc[i - 1,'idx_of_peak'] + 5:\n",
    "            curr_dist_to_peak = abs(row['idx_of_peak'] - anom_peaks.loc[i - 1, 'idx_of_peak'] - 5)\n",
    "            prev_dist = curr_dist_to_peak - 1\n",
    "\n",
    "    else:\n",
    "        # if no next peak, we are at the last peak, ensure there are still 5 data points to read\n",
    "        # TODO: implement this\n",
    "        pass\n",
    "\n",
    "\n",
    "    \"\"\"Get raw fDOM data points\"\"\"\n",
    "    timestamp_of_peak = fDOM_labeled.loc[i, 'timestamp_of_peak']\n",
    "\n",
    "    index_df = fDOM_raw[fDOM_raw['timestamp'] == timestamp_of_peak]\n",
    "\n",
    "    if len(index_df.index.to_list()) != 0:\n",
    "        index_of_peak = index_df.index.tolist()[0]\n",
    "\n",
    "        # use this timestamp to make a dataframe of raw stuff\n",
    "        # iterate over fDOM raw\n",
    "\n",
    "        # get previous \"prev_dist\" points\n",
    "        #prev_points = pd.DataFrame(fDOM_raw)\n",
    "\n",
    "        # get next \"next_dist\" points\n",
    "        #next_points = pd.DataFrame()\n",
    "\n",
    "        # for each peak, get previous and next 5 datapoints from raw and stage data\n",
    "            # this works because we aligned the data\n",
    "\n",
    "        # make changes to the data\n",
    "        #changed_data = make_small_change()\n",
    "\n",
    "        # append these to a new file, in format of the raw fDOM file\n",
    "        #augmented_fDOM_raw.append(changed_data)\n",
    "\n",
    "        # append these to a new file, in format of the stage/turb/fDOM file\n",
    "        #augmented_stage_raw.append(changed_data)\n",
    "\n",
    "        # append these to a new file, in format of labeled data (so purely the peak, mark the index of the peak when starting)\n",
    "        #augmented_fDOM_labeled.append(changed_data)\n",
    "    \n",
    "    # TODO add this missed data into the overall data somehow\n",
    "    else:\n",
    "        # we missed some data points, append them to the missed data dataframe\n",
    "        missed_fDOM_peaks.append(timestamp_of_peak)\n",
    "\n",
    "print(len(missed_fDOM_peaks))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f14ca78382e4dc227f1e51b680612414c34712d66eddea25f1c0d0c728b236b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('anomaly-detection': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
