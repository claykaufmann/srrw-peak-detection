{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenting time-series data\n",
    "In this file, the data is augmented in order to create more of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.insert(1,'../')\n",
    "import Tools.data_processing as dp\n",
    "import Tools.data_movement as dm \n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "The following functions provide useful tools for the augmentation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_time_entry(current_entry: float) -> float:\n",
    "    \"\"\"\n",
    "    This function returns the next time entry in julian time\n",
    "\n",
    "    current_entry: a julina time float\n",
    "\n",
    "    return: julian time + 15 minutes from past julian time\n",
    "    \"\"\"\n",
    "\n",
    "    # convert julian to datetime\n",
    "    date_time_init = dp.julian_to_datetime(current_entry)\n",
    "\n",
    "    # find next date time (add 15 minutes)\n",
    "    next_entry = date_time_init + timedelta(minutes=15)\n",
    "\n",
    "    # convert date time to julian time\n",
    "    final_julian_time = dp.datetime_to_julian(next_entry)\n",
    "\n",
    "    # return julian time\n",
    "    return final_julian_time\n",
    "\n",
    "\n",
    "def reindex_augmented_data(data: pd.DataFrame, datatype: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reindex the augmented data so there are no overlaps\n",
    "\n",
    "    data: the data to reindex\n",
    "    datatype: fdom, turb, or stage\n",
    "\n",
    "    returns: reindexed data\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in data\n",
    "The knowledge-based approach uses the data in `Data/converted_data/julian_format/`, so that is where the data augmentation will go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in raw data\n",
    "fDOM_data = dm.read_in_preprocessed_timeseries(\n",
    "    \"../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv\"\n",
    ")\n",
    "stage_data = dm.read_in_preprocessed_timeseries(\n",
    "    \"../Data/converted_data/julian_format/stage_10.1.11-1.1.19.csv\"\n",
    ")\n",
    "turb_data = dm.read_in_preprocessed_timeseries(\n",
    "    \"../Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv\"\n",
    ")\n",
    "\n",
    "# align stage to fDOM\n",
    "stage_data = dp.align_stage_to_fDOM(fDOM_data, stage_data)\n",
    "\n",
    "# read in labeled fDOM\n",
    "fDOM_labeled = pd.read_csv(\n",
    "    \"../Data/labeled_data/ground_truths/fDOM/fDOM_all_julian_0k-300k.csv\"\n",
    ")\n",
    "\n",
    "turb_labeled = pd.read_csv(\n",
    "    \"../Data/labeled_data/ground_truths/turb/turb_pp/julian_time/turb_pp_0k-300k_labeled.csv\"\n",
    ")\n",
    "\n",
    "# New data folder:\n",
    "AUGMENT_DATA_PATH = \"../Data/augmented_data/julian_format/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into pandas dataframes for better indexing:\n",
    "fDOM_raw = pd.DataFrame(fDOM_data)\n",
    "fDOM_raw.columns = [\"timestamp\", \"value\"]\n",
    "\n",
    "turb_raw = pd.DataFrame(turb_data)\n",
    "turb_raw.columns = [\"timestamp\", \"value\"]\n",
    "\n",
    "stage_raw = pd.DataFrame(stage_data)\n",
    "stage_raw.columns = [\"timestamp\", \"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data\n",
    "# timestamp_of_peak = fDOM_labeled.loc[88, 'timestamp_of_peak']\n",
    "# index_df = fDOM_raw[fDOM_raw['timestamp'] == timestamp_of_peak]\n",
    "\n",
    "# if len(index_df.index.to_list()) != 0:\n",
    "#     index_of_peak = index_df.index.tolist()[0]\n",
    "\n",
    "#     print(index_of_peak)\n",
    "#     print(timestamp_of_peak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting Data\n",
    "We will augment data for each type of peak, and for each measurement.\n",
    "\n",
    "Starting with fDOM:\n",
    "1. PLP (plummeting peak)\n",
    "2. PP (phantom peak)\n",
    "3. SKP (skyrocketing peak)\n",
    "\n",
    "TODO: augment more peak types when they are labeled\n",
    "\n",
    "With turbidity:\n",
    "1. PP\n",
    "\n",
    "We start by creating our data frames to augment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Helpful constants \"\"\"\n",
    "TIME_RANGE_INIT = 5  # the base distance of points to add between the peaks\n",
    "\n",
    "\"\"\" Dataframes to be used by augmenter \"\"\"\n",
    "\n",
    "# labeled fDOM peaks\n",
    "fdom_anon_peaks = fDOM_labeled[fDOM_labeled[\"label_of_peak\"] != \"NAP\"]\n",
    "fdom_anon_peaks = fdom_anon_peaks.reset_index()  # reset index as values were removed\n",
    "\n",
    "# labeled turb peaks\n",
    "turb_anon_peaks = turb_labeled[turb_labeled[\"label_of_peak\"] != \"NPP\"]\n",
    "turb_anon_peaks = turb_anon_peaks.reset_index()\n",
    "\n",
    "# new dataframes for augmented labeled/raw fDOM\n",
    "augmented_fDOM_raw = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "augmented_fDOM_labeled = pd.DataFrame(\n",
    "    columns=[\"timestamp_of_peak\", \"value_of_peak\", \"label_of_peak\", \"idx_of_peak\"]\n",
    ")\n",
    "\n",
    "# dataframe for augmented stage\n",
    "augmented_stage_raw = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "\n",
    "# dataframes for augmented raw/labeled turbidity\n",
    "augmented_turb_raw = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "augmented_turb_labeled = pd.DataFrame(\n",
    "    columns=[\"timestamp_of_peak\", \"value_of_peak\", \"label_of_peak\", \"idx_of_peak\"]\n",
    ")\n",
    "\n",
    "# variable to keep the last entry in the dataframe for stage\n",
    "# defaults to the last entry that was in fdom/turb raw csv files, in julian format\n",
    "prev_added_entry = 2459096.9583333335\n",
    "\n",
    "# a list of peaks that don't align with the fDOM raw file that was aligned with stage\n",
    "# i believe its just peaks that don't align with stage in general for whatever reason\n",
    "missed_fDOM_peaks = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_augment_index(dataframe) -> int:\n",
    "    \"\"\"\n",
    "    Collects the last index of the augmented time series\n",
    "    \"\"\"\n",
    "    return dataframe.shape[0]\n",
    "\n",
    "\n",
    "# create data function to create new data\n",
    "def create_data(\n",
    "    fdom_range: pd.DataFrame,\n",
    "    label_of_peak: str,\n",
    "    stage_range: pd.DataFrame,\n",
    "    turb_range: pd.DataFrame,\n",
    "    peak_timestamp: float,\n",
    "    datatype: str,\n",
    "    last_timestamp,\n",
    "):\n",
    "    \"\"\"\n",
    "    Makes changes to the current datapoints, by modifying peaks, and adding in values if needed\n",
    "\n",
    "    data: the range of datapoints by index in a dataframe\n",
    "\n",
    "    peak_index: the index of the actual peak we are modifying\n",
    "\n",
    "    datatype: \"fdom\" or \"turb\"\n",
    "\n",
    "    returns: the new data range to be appended to the data\n",
    "    \"\"\"\n",
    "    # TODO: make this function not take in df's, or return df's\n",
    "\n",
    "    # make a copy of the modified data\n",
    "    new_fdom_raw = copy.deepcopy(fdom_range)\n",
    "    new_stage = copy.deepcopy(stage_range)\n",
    "    new_turb_raw = copy.deepcopy(turb_range)\n",
    "\n",
    "    # peak index can change when we add in x data\n",
    "    new_fdom_peak_index = -1\n",
    "    new_peak_timestamp = -1\n",
    "\n",
    "    if datatype == \"fdom\":\n",
    "        # augment fDOM, using timestamp\n",
    "        new_peak_val = new_fdom_raw.loc[peak_timestamp, \"value\"] * 1.10\n",
    "        new_fdom_raw.loc[peak_index, \"value\"] = new_peak_val\n",
    "\n",
    "        # insert necessary values into turb and stage\n",
    "\n",
    "        # get the next possible timestamp\n",
    "        new_time_entry = next_time_entry(last_timestamp)\n",
    "\n",
    "        # update all timestamps for augmented data\n",
    "        for i, row in new_fdom_raw.iterrows():\n",
    "            # if timestamps equal, we have the relative peak\n",
    "            if new_fdom_raw.loc[i, \"timestamp\"] == peak_timestamp:\n",
    "                # register index here\n",
    "                new_fdom_peak_index = get_last_augment_index(augmented_fDOM_raw)\n",
    "                new_peak_timestamp = new_time_entry\n",
    "\n",
    "            # update timestamps\n",
    "            new_fdom_raw.loc[i, \"timestamp\"] = new_time_entry\n",
    "            new_stage.loc[i, \"timestamp\"] = new_time_entry\n",
    "            new_turb_raw.loc[i, \"timestamp\"] = new_time_entry\n",
    "\n",
    "            # get next time stamp\n",
    "            new_time_entry = next_time_entry(new_time_entry)\n",
    "\n",
    "        # add entries into raw fDOM\n",
    "        augmented_fDOM_raw = pd.concat(augmented_fDOM_raw, new_fdom_raw)\n",
    "\n",
    "        # add peak to labeled fDOM\n",
    "        new_labeled_fDOM = {\n",
    "            \"timestamp_of_peak\": new_peak_timestamp,\n",
    "            \"value_of_peak\": new_peak_val,\n",
    "            \"label_of_peak\": label_of_peak,\n",
    "            \"idx_of_peak\": new_fdom_peak_index,\n",
    "        }\n",
    "        augmented_fDOM_labeled = pd.concat([augmented_fDOM_labeled, new_labeled_fDOM])\n",
    "\n",
    "        # add entries to stage\n",
    "        augmented_stage_raw = pd.concat(augmented_stage_raw, new_stage)\n",
    "\n",
    "        # add entries to turb\n",
    "        augmented_turb_raw = pd.concat(augmented_turb_raw, new_turb_raw)\n",
    "\n",
    "    else:\n",
    "\n",
    "        # augment turb\n",
    "\n",
    "        # add entries to stage as needed\n",
    "\n",
    "        # add entries to fdom as needed\n",
    "        pass\n",
    "\n",
    "    return (\n",
    "        augmented_fDOM_raw,\n",
    "        augmented_fDOM_labeled,\n",
    "        augmented_stage_raw,\n",
    "        augmented_turb_raw,\n",
    "        new_fdom_peak_index,\n",
    "        new_time_entry,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting fDOM\n",
    "The next codeblocks augment fDOM data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HACK: there are overlaps in time ranges, might not be an issue but it could be an issue\n",
    "# TODO: turn this into a callable function when it is finished\n",
    "\n",
    "\n",
    "# iterate over each peak\n",
    "for i, row in fdom_anon_peaks.iterrows():\n",
    "    # check to see if any overlap occurs between peaks\n",
    "    prev_dist = TIME_RANGE_INIT\n",
    "    next_dist = TIME_RANGE_INIT\n",
    "\n",
    "    if i == 0:\n",
    "        # we are at the first peak, check to see if there are 5 data points behind\n",
    "        # TODO: implement this\n",
    "        # note that its actually not needed\n",
    "        pass\n",
    "\n",
    "    elif i + 1 < fdom_anon_peaks.shape[0]:\n",
    "        # anywhere else in the middle, check for overlap\n",
    "        # FIXME: not currently checking for overlaps, we might not need to tho\n",
    "\n",
    "        # check next 5\n",
    "        if (\n",
    "            row[\"idx_of_peak\"] + TIME_RANGE_INIT\n",
    "            >= fdom_anon_peaks.loc[i + 1, \"idx_of_peak\"] - TIME_RANGE_INIT\n",
    "        ):  # -5 becase we go back 5 peaks too\n",
    "            # change next_dist to whatever it needs to be\n",
    "            curr_dist_to_peak = abs(\n",
    "                row[\"idx_of_peak\"]\n",
    "                - fdom_anon_peaks.loc[i + 1, \"idx_of_peak\"]\n",
    "                - TIME_RANGE_INIT\n",
    "            )\n",
    "            next_dist = curr_dist_to_peak - 1\n",
    "\n",
    "        # check past 5\n",
    "        if (\n",
    "            row[\"idx_of_peak\"] - TIME_RANGE_INIT\n",
    "            <= fdom_anon_peaks.loc[i - 1, \"idx_of_peak\"] + TIME_RANGE_INIT\n",
    "        ):\n",
    "            curr_dist_to_peak = abs(\n",
    "                row[\"idx_of_peak\"]\n",
    "                - fdom_anon_peaks.loc[i - 1, \"idx_of_peak\"]\n",
    "                - TIME_RANGE_INIT\n",
    "            )\n",
    "            prev_dist = curr_dist_to_peak - 1\n",
    "\n",
    "    else:\n",
    "        # if no next peak, we are at the last peak, ensure there are still 5 data points to read\n",
    "        # TODO: implement this\n",
    "        pass\n",
    "\n",
    "    \"\"\"Get raw fDOM data points\"\"\"\n",
    "    timestamp_of_peak = fDOM_labeled.loc[i, \"timestamp_of_peak\"]\n",
    "    label_of_peak = fDOM_labeled.loc[i, \"label_of_peak\"]\n",
    "\n",
    "    # get index dataframes of each type\n",
    "    # HACK: there has got to be a better way to do this\n",
    "    fdom_index_df = fDOM_raw[fDOM_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "    stage_index_df = stage_raw[stage_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "    turb_index_df = turb_raw[turb_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "\n",
    "    if len(fdom_index_df.index.to_list()) != 0:\n",
    "        # get indices of each data type from index df's\n",
    "        index_of_peak = fdom_index_df.index.tolist()[0]\n",
    "        stage_index = stage_index_df.index.tolist()[0]\n",
    "        turb_index = turb_index_df.index.tolist()[0]\n",
    "\n",
    "        # use this timestamp to make a dataframe of raw stuff\n",
    "        # get data from fDOM_raw file\n",
    "        fDOM_raw_time_range = pd.DataFrame(\n",
    "            fDOM_raw.iloc[index_of_peak - prev_dist : index_of_peak + next_dist]\n",
    "        )\n",
    "\n",
    "        # get stage data range\n",
    "        stage_time_range = pd.DataFrame(\n",
    "            stage_raw.iloc[stage_index - prev_dist : stage_index + next_dist]\n",
    "        )\n",
    "\n",
    "        # get turbidity data range\n",
    "        turb_time_range = pd.DataFrame(\n",
    "            turb_raw.iloc[turb_index - prev_dist : turb_index + next_dist]\n",
    "        )\n",
    "\n",
    "        # get augmented data\n",
    "        (\n",
    "            augmented_fDOM_raw,\n",
    "            augmented_fDOM_labeled,\n",
    "            augmented_stage_raw,\n",
    "            augmented_turb_raw,\n",
    "            new_peak_index,\n",
    "            last_time_entry,\n",
    "        ) = create_data(\n",
    "            fDOM_raw_time_range,\n",
    "            label_of_peak,\n",
    "            stage_time_range,\n",
    "            turb_time_range,\n",
    "            timestamp_of_peak,\n",
    "            \"fdom\",\n",
    "            prev_added_entry,\n",
    "        )\n",
    "\n",
    "        # update prev time entry\n",
    "        prev_added_entry = last_time_entry\n",
    "\n",
    "    # TODO add this missed data into the overall data somehow\n",
    "    else:\n",
    "        # we missed some data points, append them to the missed data dataframe\n",
    "        missed_fDOM_peaks.append(timestamp_of_peak)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment turbidity data\n",
    "The following code blocks augment turbidity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Augment turbidity data by calling previously written function \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move augmented data into csv files\n",
    "The following codeblock creates csv files for the augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Augmented Data Paths \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this function\n",
    "def write_augmented_data_to_csv():\n",
    "    # call to_csv for each dataframe\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f14ca78382e4dc227f1e51b680612414c34712d66eddea25f1c0d0c728b236b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('anomaly-detection': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
