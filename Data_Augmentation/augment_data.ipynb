{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenting time-series data\n",
    "In this file, the data is augmented in order to create more of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.insert(1,'../')\n",
    "import Tools.data_processing as dp\n",
    "import Tools.data_movement as dm \n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import copy\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "The following functions provide useful tools for the augmentation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_time_entry(current_entry: float) -> float:\n",
    "    \"\"\"\n",
    "    This function returns the next time entry in julian time\n",
    "\n",
    "    current_entry: a julina time float\n",
    "\n",
    "    return: julian time + 15 minutes from past julian time\n",
    "    \"\"\"\n",
    "\n",
    "    # convert julian to datetime\n",
    "    date_time_init = dp.julian_to_datetime(current_entry)\n",
    "\n",
    "    # find next date time (add 15 minutes)\n",
    "    next_entry = date_time_init + timedelta(minutes=15)\n",
    "\n",
    "    # convert date time to julian time\n",
    "    final_julian_time = dp.datetime_to_julian(next_entry)\n",
    "\n",
    "    # return julian time\n",
    "    return final_julian_time\n",
    "\n",
    "\n",
    "def reindex_augmented_data(data: pd.DataFrame, datatype: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reindex the augmented data so there are no overlaps\n",
    "\n",
    "    data: the data to reindex\n",
    "    datatype: fdom, turb, or stage\n",
    "\n",
    "    returns: reindexed data\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_last_augment_index(dataframe) -> int:\n",
    "    \"\"\"\n",
    "    Collects the last index of the augmented time series\n",
    "    \"\"\"\n",
    "    return dataframe.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in data\n",
    "The knowledge-based approach uses the data in `Data/converted_data/julian_format/`, so that is where the data augmentation will go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in raw data\n",
    "fDOM_data = dm.read_in_preprocessed_timeseries(\n",
    "    \"../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv\"\n",
    ")\n",
    "stage_data = dm.read_in_preprocessed_timeseries(\n",
    "    \"../Data/converted_data/julian_format/stage_10.1.11-1.1.19.csv\"\n",
    ")\n",
    "turb_data = dm.read_in_preprocessed_timeseries(\n",
    "    \"../Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv\"\n",
    ")\n",
    "\n",
    "# align stage to fDOM\n",
    "stage_data = dp.align_stage_to_fDOM(fDOM_data, stage_data)\n",
    "\n",
    "# read in labeled fDOM\n",
    "fDOM_labeled = pd.read_csv(\n",
    "    \"../Data/labeled_data/ground_truths/fDOM/fDOM_all_julian_0k-300k.csv\"\n",
    ")\n",
    "\n",
    "# read in labeled turb\n",
    "turb_labeled = pd.read_csv(\n",
    "    \"../Data/labeled_data/ground_truths/turb/turb_pp/julian_time/turb_pp_0k-300k_labeled.csv\"\n",
    ")\n",
    "\n",
    "# New data folder:\n",
    "AUGMENT_DATA_PATH = \"../Data/augmented_data/julian_format/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into pandas dataframes for better indexing:\n",
    "fDOM_raw = pd.DataFrame(fDOM_data)\n",
    "fDOM_raw.columns = [\"timestamp\", \"value\"]\n",
    "\n",
    "print(fDOM_raw.iloc[9399])\n",
    "\n",
    "turb_raw = pd.DataFrame(turb_data)\n",
    "turb_raw.columns = [\"timestamp\", \"value\"]\n",
    "\n",
    "stage_raw = pd.DataFrame(stage_data)\n",
    "stage_raw.columns = [\"timestamp\", \"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualize data\n",
    "\"\"\"\n",
    "fig = plt.figure()\n",
    "x = fDOM_raw['timestamp']\n",
    "y = turb_raw['value']\n",
    "\n",
    "\n",
    "line_fdom = plt.Line2D(fDOM_raw['timestamp'], fDOM_raw['value'])\n",
    "line_turb = plt.Line2D(turb_raw['timestamp'], turb_raw['value'], color='red')\n",
    "line_stage = plt.Line2D(stage_raw['timestamp'], stage_raw['value'], color='orange')\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.add_line(line_fdom)\n",
    "ax.add_line(line_turb)\n",
    "ax.add_line(line_stage)\n",
    "ax.set_xlim(min(x), max(x))\n",
    "ax.set_ylim(min(y) - 10, max(y) + 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting Data\n",
    "We will augment data for each type of peak, and for each measurement.\n",
    "\n",
    "Starting with fDOM:\n",
    "1. PLP (plummeting peak)\n",
    "2. PP (phantom peak)\n",
    "3. SKP (skyrocketing peak)\n",
    "\n",
    "TODO: augment more peak types when they are labeled\n",
    "\n",
    "With turbidity:\n",
    "1. PP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants\n",
    "We define two constants for use with augmenting the data:\n",
    "1. `TIME_RANGE_INIT`: the number of points before and after the relative peak that we take data from\n",
    "2. `ITERATIONS`: the number of times we loop over the list of anomaly peaks, and augment them\n",
    "3. `STARTING_TIMESTAMP`: the timestamp to start all augmented data at. The default value is 15 minutes after the last data measurement from the original set up data given to the project devs. As of 2/16/22, this timestamp is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Helpful constants \"\"\"\n",
    "TIME_RANGE_INIT = 5  # the base distance of points to add between the peaks\n",
    "ITERATIONS = 3 # number of times to loop over dataset and augment\n",
    "STARTING_TIMESTAMP = 2459096.9583333335"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting fDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "#                                                                                   #\n",
    "#                               DATAFRAME SETUP SECTION                             #\n",
    "#                                                                                   #\n",
    "#####################################################################################\n",
    "\n",
    "# new dataframes for augmented labeled/raw fDOM\n",
    "augmented_fDOM_raw = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "augmented_fDOM_labeled = pd.DataFrame(\n",
    "    columns=[\"timestamp_of_peak\", \"value_of_peak\", \"label_of_peak\", \"idx_of_peak\"]\n",
    ")\n",
    "\n",
    "# dataframe for augmented stage\n",
    "augmented_stage_raw_fdom = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "\n",
    "# dataframes for augmented raw/labeled turbidity\n",
    "augmented_turb_raw_fdom = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "augmented_turb_labeled_fdom = pd.DataFrame(\n",
    "    columns=[\"timestamp_of_peak\", \"value_of_peak\", \"label_of_peak\", \"idx_of_peak\"]\n",
    ")\n",
    "\n",
    "# variable to keep the last entry in the dataframe for stage\n",
    "# defaults to the last entry that was in fdom/turb raw csv files, in julian format\n",
    "prev_added_entry = STARTING_TIMESTAMP\n",
    "\n",
    "# a list of peaks that don't align with the fDOM raw file that was aligned with stage\n",
    "# i believe its just peaks that don't align with stage in general for whatever reason\n",
    "missed_fDOM_peaks = []\n",
    "\n",
    "#####################################################################################\n",
    "#                                                                                   #\n",
    "#                               AUGMENT DATA SECTION                                #\n",
    "#                                                                                   #\n",
    "#####################################################################################\n",
    "\n",
    "for iteration in range(ITERATIONS):\n",
    "    \"\"\"\n",
    "    Re-sample the fDOM labeled peaks to add variance to data\n",
    "    \"\"\"\n",
    "    # labeled fDOM peaks\n",
    "    fdom_anon_peaks = fDOM_labeled[fDOM_labeled[\"label_of_peak\"] != \"NAP\"]\n",
    "    # randomize the order, to add more\n",
    "    fdom_anon_peaks = fdom_anon_peaks.sample(frac=1).reset_index(\n",
    "        drop=True\n",
    "    )  # reset index as values were removed\n",
    "\n",
    "    # iterate over each peak\n",
    "    for i, row in fdom_anon_peaks.iterrows():\n",
    "        # check to see if any overlap occurs between peaks\n",
    "        prev_dist = TIME_RANGE_INIT\n",
    "        next_dist = TIME_RANGE_INIT\n",
    "\n",
    "        # Get raw fDOM data points\n",
    "        timestamp_of_peak = fdom_anon_peaks.loc[i, \"timestamp_of_peak\"]\n",
    "        label_of_peak = fdom_anon_peaks.loc[i, \"label_of_peak\"]\n",
    "\n",
    "        # get index dataframes of each type\n",
    "        fdom_index_df = fDOM_raw[fDOM_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "        stage_index_df = stage_raw[stage_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "        turb_index_df = turb_raw[turb_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "\n",
    "        if len(fdom_index_df.index.to_list()) != 0:\n",
    "            # get indices of each data type from index df's\n",
    "            index_of_peak = fdom_index_df.index.tolist()[0]\n",
    "            stage_index = stage_index_df.index.tolist()[0]\n",
    "            turb_index = turb_index_df.index.tolist()[0]\n",
    "\n",
    "            # use this timestamp to make a dataframe of raw stuff\n",
    "            # get data from fDOM_raw file\n",
    "            fDOM_raw_time_range = pd.DataFrame(\n",
    "                fDOM_raw.iloc[index_of_peak - prev_dist : index_of_peak + next_dist]\n",
    "            )\n",
    "\n",
    "            # get stage data range\n",
    "            stage_time_range = pd.DataFrame(\n",
    "                stage_raw.iloc[stage_index - prev_dist : stage_index + next_dist]\n",
    "            )\n",
    "\n",
    "            # get turbidity data range\n",
    "            turb_time_range = pd.DataFrame(\n",
    "                turb_raw.iloc[turb_index - prev_dist : turb_index + next_dist]\n",
    "            )\n",
    "\n",
    "            # make a copy of the modified data\n",
    "            new_fdom_raw = copy.deepcopy(fDOM_raw_time_range)\n",
    "            new_stage = copy.deepcopy(stage_time_range)\n",
    "            new_turb_raw = copy.deepcopy(turb_time_range)\n",
    "\n",
    "            # peak index can change when we add in x data\n",
    "            new_fdom_peak_index = -1\n",
    "            new_peak_timestamp = -1\n",
    "\n",
    "            # generate a random number to multiply the peak by, +- 0.1\n",
    "            # set seed\n",
    "            random.seed()\n",
    "            random_val = random.uniform(-0.1, 0.1)\n",
    "\n",
    "            new_peak_val = new_fdom_raw.loc[index_of_peak, \"value\"] * (1 + random_val)\n",
    "\n",
    "            new_fdom_raw.loc[index_of_peak, \"value\"] = new_peak_val\n",
    "\n",
    "            # TODO insert necessary values into turb and stage\n",
    "\n",
    "            # get the next possible timestamp\n",
    "            new_time_entry = next_time_entry(prev_added_entry)\n",
    "\n",
    "            # update all timestamps for augmented data\n",
    "            for i, row in new_fdom_raw.iterrows():\n",
    "                # if timestamps equal, we have the relative peak\n",
    "                if new_fdom_raw.loc[i, \"timestamp\"] == timestamp_of_peak:\n",
    "                    # register index here\n",
    "                    new_fdom_peak_index = get_last_augment_index(augmented_fDOM_raw)\n",
    "                    new_peak_timestamp = new_time_entry\n",
    "\n",
    "                # update timestamps\n",
    "                new_fdom_raw.loc[i, \"timestamp\"] = new_time_entry\n",
    "                new_stage.loc[i, \"timestamp\"] = new_time_entry\n",
    "                new_turb_raw.loc[i, \"timestamp\"] = new_time_entry\n",
    "\n",
    "                # get next time stamp\n",
    "                new_time_entry = next_time_entry(new_time_entry)\n",
    "\n",
    "            # add entries into raw fDOM\n",
    "            augmented_fDOM_raw = pd.concat(\n",
    "                [augmented_fDOM_raw, new_fdom_raw], ignore_index=True\n",
    "            )\n",
    "\n",
    "            new_label = pd.DataFrame(\n",
    "                [\n",
    "                    [\n",
    "                        new_peak_timestamp,\n",
    "                        new_peak_val,\n",
    "                        label_of_peak,\n",
    "                        new_fdom_peak_index,\n",
    "                    ]\n",
    "                ],\n",
    "                columns=[\n",
    "                    \"timestamp_of_peak\",\n",
    "                    \"value_of_peak\",\n",
    "                    \"label_of_peak\",\n",
    "                    \"idx_of_peak\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            # add entries to labeled fDOM\n",
    "            augmented_fDOM_labeled = pd.concat([augmented_fDOM_labeled, new_label])\n",
    "\n",
    "            # add entries to stage\n",
    "            augmented_stage_raw_fdom = pd.concat(\n",
    "                [augmented_stage_raw_fdom, new_stage], ignore_index=True\n",
    "            )\n",
    "\n",
    "            # add entries to turb\n",
    "            augmented_turb_raw_fdom = pd.concat(\n",
    "                [augmented_turb_raw_fdom, new_turb_raw], ignore_index=True\n",
    "            )\n",
    "\n",
    "            # update prev time entry\n",
    "            prev_added_entry = new_time_entry\n",
    "\n",
    "        else:\n",
    "            # we missed some data points, append them to the missed data dataframe\n",
    "            missed_fDOM_peaks.append(timestamp_of_peak)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell allows you to print out the augmented dataframes in full\n",
    "\"\"\"\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.width', 1000)\n",
    "# pd.set_option('display.colheader_justify', 'center')\n",
    "# pd.set_option('display.precision', 3)\n",
    "\n",
    "# print(\"Labeled Peaks Augmented\")\n",
    "# print(augmented_fDOM_labeled)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(\"Raw fDOM Augmented\")\n",
    "# print(augmented_fDOM_raw)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(\"Raw Stage Augmented\")\n",
    "# print(augmented_stage_raw_fdom)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(\"Raw Turbidity Augmented\")\n",
    "# print(augmented_turb_raw_fdom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualize data with matplotlib\n",
    "\"\"\"\n",
    "fig = plt.figure()\n",
    "x = augmented_turb_raw_fdom['timestamp']\n",
    "y = augmented_turb_raw_fdom['value']\n",
    "\n",
    "line_fdom = plt.Line2D(augmented_fDOM_raw['timestamp'], augmented_fDOM_raw['value'])\n",
    "line_turb = plt.Line2D(augmented_turb_raw_fdom['timestamp'], augmented_turb_raw_fdom['value'], color='red')\n",
    "line_stage = plt.Line2D(augmented_stage_raw_fdom['timestamp'], augmented_stage_raw_fdom['value'], color='orange')\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.add_line(line_fdom)\n",
    "ax.add_line(line_turb)\n",
    "ax.add_line(line_stage)\n",
    "ax.set_xlim(min(x), max(x))\n",
    "ax.set_ylim(min(y) - 10, max(y) + 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment turbidity data\n",
    "The following code blocks augment turbidity data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "#                                                                                   #\n",
    "#                               DATAFRAME SETUP SECTION                             #\n",
    "#                                                                                   #\n",
    "#####################################################################################\n",
    "\n",
    "# labeled turb\n",
    "augmented_turb_labeled = pd.DataFrame(\n",
    "    columns=[\"timestamp_of_peak\", \"value_of_peak\", \"label_of_peak\", \"idx_of_peak\"]\n",
    ")\n",
    "\n",
    "# raw turb\n",
    "augmented_turb_raw = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "\n",
    "# raw fdom\n",
    "augmented_fDOM_raw_turb = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "\n",
    "# raw stage\n",
    "augmented_stage_raw_turb = pd.DataFrame(columns=[\"timestamp\", \"value\"])\n",
    "\n",
    "# variable to keep the last entry in the dataframe for stage\n",
    "# defaults to the last entry that was in fdom/turb raw csv files, in julian format\n",
    "prev_added_entry = STARTING_TIMESTAMP\n",
    "\n",
    "# a list of peaks that don't align with the fDOM raw file that was aligned with stage\n",
    "# i believe its just peaks that don't align with stage in general for whatever reason\n",
    "missed_turb_peaks = []\n",
    "\n",
    "#####################################################################################\n",
    "#                                                                                   #\n",
    "#                               AUGMENT DATA SECTION                                #\n",
    "#                                                                                   #\n",
    "#####################################################################################\n",
    "\n",
    "\"\"\" Augment turbidity data by calling previously written function \"\"\"\n",
    "for iteration in range(ITERATIONS):\n",
    "    \"\"\" Resample turb labeled peaks at each iteration for more variance \"\"\"\n",
    "    # labeled turb peaks\n",
    "    turb_anon_peaks = turb_labeled[turb_labeled[\"label_of_peak\"] != \"NPP\"]\n",
    "    turb_anon_peaks = turb_anon_peaks.sample(frac=1).reset_index()\n",
    "\n",
    "    # iterate over each peak\n",
    "    for i, row in turb_anon_peaks.iterrows():\n",
    "        # check to see if any overlap occurs between peaks\n",
    "        prev_dist = TIME_RANGE_INIT\n",
    "        next_dist = TIME_RANGE_INIT\n",
    "\n",
    "        \"\"\"Get raw turb data points\"\"\"\n",
    "        timestamp_of_peak = turb_anon_peaks.loc[i, \"timestamp_of_peak\"]\n",
    "        label_of_peak = turb_anon_peaks.loc[i, \"label_of_peak\"]\n",
    "\n",
    "        # get index dataframes of each type\n",
    "        fdom_index_df = fDOM_raw[fDOM_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "        stage_index_df = stage_raw[stage_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "        turb_index_df = turb_raw[turb_raw[\"timestamp\"] == timestamp_of_peak]\n",
    "\n",
    "        if len(turb_index_df.index.to_list()) != 0:\n",
    "            # get indices of each data type from index df's\n",
    "            index_of_peak = turb_index_df.index.tolist()[0]\n",
    "            stage_index = stage_index_df.index.tolist()[0]\n",
    "            fdom_index = fdom_index_df.index.tolist()[0]\n",
    "\n",
    "            # use this timestamp to make a dataframe of raw stuff\n",
    "            # get data from fDOM_raw file\n",
    "            fDOM_raw_time_range = pd.DataFrame(\n",
    "                fDOM_raw.iloc[fdom_index - prev_dist : fdom_index + next_dist]\n",
    "            )\n",
    "\n",
    "            # get stage data range\n",
    "            stage_time_range = pd.DataFrame(\n",
    "                stage_raw.iloc[stage_index - prev_dist : stage_index + next_dist]\n",
    "            )\n",
    "\n",
    "            # get turbidity data range\n",
    "            turb_time_range = pd.DataFrame(\n",
    "                turb_raw.iloc[index_of_peak - prev_dist : index_of_peak + next_dist]\n",
    "            )\n",
    "\n",
    "            # make a copy of the modified data\n",
    "            new_fdom_raw = copy.deepcopy(fDOM_raw_time_range)\n",
    "            new_stage = copy.deepcopy(stage_time_range)\n",
    "            new_turb_raw = copy.deepcopy(turb_time_range)\n",
    "\n",
    "            # peak index can change when we add in x data\n",
    "            new_fdom_peak_index = -1\n",
    "            new_peak_timestamp = -1\n",
    "\n",
    "            # generate a random number to multiply the peak by, +- 0.1\n",
    "            # set seed\n",
    "            random.seed()\n",
    "            random_val = random.uniform(-0.1, 0.1)\n",
    "\n",
    "            new_peak_val = new_turb_raw.loc[index_of_peak, \"value\"] * (1 + random_val)\n",
    "\n",
    "            new_turb_raw.loc[index_of_peak, \"value\"] = new_peak_val\n",
    "\n",
    "            # TODO insert necessary values into fdom and stage\n",
    "\n",
    "            # get the next possible timestamp\n",
    "            new_time_entry = next_time_entry(prev_added_entry)\n",
    "\n",
    "            # update all timestamps for augmented data\n",
    "            for i, row in new_turb_raw.iterrows():\n",
    "                # if timestamps equal, we have the relative peak\n",
    "                if new_turb_raw.loc[i, \"timestamp\"] == timestamp_of_peak:\n",
    "                    # register index here\n",
    "                    new_turb_peak_index = get_last_augment_index(augmented_turb_raw)\n",
    "                    new_peak_timestamp = new_time_entry\n",
    "\n",
    "                # update timestamps\n",
    "                new_fdom_raw.loc[i, \"timestamp\"] = new_time_entry\n",
    "                new_stage.loc[i, \"timestamp\"] = new_time_entry\n",
    "                new_turb_raw.loc[i, \"timestamp\"] = new_time_entry\n",
    "\n",
    "                # get next time stamp\n",
    "                new_time_entry = next_time_entry(new_time_entry)\n",
    "\n",
    "            # add entries to turb\n",
    "            augmented_turb_raw = pd.concat(\n",
    "                [augmented_turb_raw, new_turb_raw], ignore_index=True\n",
    "            )\n",
    "\n",
    "            new_label = pd.DataFrame(\n",
    "                [\n",
    "                    [\n",
    "                        new_peak_timestamp,\n",
    "                        new_peak_val,\n",
    "                        label_of_peak,\n",
    "                        new_turb_peak_index,\n",
    "                    ]\n",
    "                ],\n",
    "                columns=[\n",
    "                    \"timestamp_of_peak\",\n",
    "                    \"value_of_peak\",\n",
    "                    \"label_of_peak\",\n",
    "                    \"idx_of_peak\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            # add entries to labeled turb\n",
    "            augmented_turb_labeled = pd.concat([augmented_turb_labeled, new_label])\n",
    "\n",
    "            # add entries to stage\n",
    "            augmented_stage_raw_turb = pd.concat(\n",
    "                [augmented_stage_raw_turb, new_stage], ignore_index=True\n",
    "            )\n",
    "\n",
    "            # add entries into raw fDOM\n",
    "            augmented_fDOM_raw_turb = pd.concat(\n",
    "                [augmented_fDOM_raw_turb, new_fdom_raw], ignore_index=True\n",
    "            )\n",
    "\n",
    "            # update prev time entry\n",
    "            prev_added_entry = new_time_entry\n",
    "\n",
    "        else:\n",
    "            # we missed some data points, append them to the missed data dataframe\n",
    "            missed_fDOM_peaks.append(timestamp_of_peak)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell allows you to print out the augmented dataframes in full\n",
    "\"\"\"\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.width', 1000)\n",
    "# pd.set_option('display.colheader_justify', 'center')\n",
    "# pd.set_option('display.precision', 3)\n",
    "\n",
    "# print(\"Labeled Peaks Augmented\")\n",
    "# print(augmented_turb_labeled)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(\"Raw fDOM Augmented\")\n",
    "# print(augmented_fDOM_raw_turb)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(\"Raw Stage Augmented\")\n",
    "# print(augmented_stage_raw_turb)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(\"Raw Turbidity Augmented\")\n",
    "# print(augmented_turb_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualize data with matplotlib\n",
    "\"\"\"\n",
    "fig = plt.figure()\n",
    "x = augmented_turb_raw['timestamp']\n",
    "y = augmented_turb_raw['value']\n",
    "\n",
    "line_fdom = plt.Line2D(augmented_fDOM_raw_turb['timestamp'], augmented_fDOM_raw_turb['value'])\n",
    "line_turb = plt.Line2D(augmented_turb_raw['timestamp'], augmented_turb_raw['value'], color='red')\n",
    "line_stage = plt.Line2D(augmented_stage_raw_turb['timestamp'], augmented_stage_raw_turb['value'], color='orange')\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.add_line(line_fdom)\n",
    "ax.add_line(line_turb)\n",
    "ax.add_line(line_stage)\n",
    "ax.set_xlim(min(x), max(x))\n",
    "ax.set_ylim(min(y) - 10, max(y) + 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move augmented data into csv files\n",
    "The following codeblock creates csv files for the augmented data.\n",
    "\n",
    "### NOTE ON DATA:\n",
    "Due to the random sampling used when augmenting fDOM and turbidity,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Augmented Data Paths \"\"\"\n",
    "# trainset\n",
    "trainset_fdom_path = \"../Data/augmented_data/trainset_plotting/fdom/\"\n",
    "trainset_turb_path = \"../Data/augmented_data/trainset_plotting/turb/\"\n",
    "\n",
    "# unlabeled data\n",
    "unlabeled_fdom_path = \"../Data/augmented_data/fdom/unlabeled/\"\n",
    "unlabeled_turb_path = \"../Data/augmented_data/turb/unlabeled/\"\n",
    "\n",
    "# labeled data\n",
    "labeled_fdom_path = \"../Data/augmented_data/fdom/labeled/\"\n",
    "labeled_turb_path = \"../Data/augmented_data/turb/labeled/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_augmented_data_to_csv():\n",
    "    # call to_csv for each dataframe\n",
    "    # for each dataframe, we also drop the index\n",
    "\n",
    "    # write fDOM augmented data\n",
    "    augmented_fDOM_labeled.to_csv(labeled_fdom_path + 'labeled_fdom_peaks.csv', index=False)\n",
    "    augmented_fDOM_raw.to_csv(unlabeled_fdom_path + 'unlabeled_fdom.csv', index=False)\n",
    "    augmented_turb_raw_fdom.to_csv(unlabeled_fdom_path + 'unlabeled_turb.csv', index=False)\n",
    "    augmented_stage_raw_fdom.to_csv(unlabeled_fdom_path + 'unlabeled_stage.csv', index=False)\n",
    "\n",
    "    # write turb augmented data\n",
    "    augmented_turb_labeled.to_csv(labeled_turb_path + 'labeled_turb_peaks.csv', index=False)\n",
    "    augmented_turb_raw.to_csv(unlabeled_turb_path + 'unlabeled_turb.csv', index=False)\n",
    "    augmented_fDOM_raw_turb.to_csv(unlabeled_turb_path + 'unlabeled_fdom.csv', index=False)\n",
    "    augmented_stage_raw_turb.to_csv(unlabeled_turb_path + 'unlabeled_stage.csv', index=False)\n",
    "\n",
    "\n",
    "def convert_df_julian_to_datetime(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts a dataframe julian timestamp to datetime ISO 8601 format\n",
    "\n",
    "    df: a dataframe\n",
    "\n",
    "    return: changed dataframe\n",
    "    \"\"\"\n",
    "    # iterate over dataframe, replacing timestamp vals\n",
    "    for i, row in df.iterrows():\n",
    "        df.loc[i, \"timestamp\"] = dp.julian_to_datetime(df.loc[i, \"timestamp\"]).isoformat()\n",
    "        \n",
    "        # add stupid 0.00Z to fit trainset format\n",
    "        df.loc[i, \"timestamp\"] = df.loc[i, \"timestamp\"] + \".000Z\"\n",
    "\n",
    "    return df\n",
    "\n",
    "def write_to_trainset_csv():\n",
    "    # TODO: add peak labels in\n",
    "\n",
    "    # start by creating a dataframe that has the correct columns\n",
    "    trainset_fdom_df = pd.DataFrame(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "    trainset_turb_df = pd.DataFrame(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "\n",
    "    # ~~~~~~~ fDOM section ~~~~~~~\n",
    "    # start by just adding the fDOM data into the series, need to replace all timestamps\n",
    "    fdom_trainset_raw = copy.deepcopy(augmented_fDOM_raw)\n",
    "    fdom_turb_trainset_raw = copy.deepcopy(augmented_turb_raw_fdom)\n",
    "    fdom_stage_trainset_raw = copy.deepcopy(augmented_stage_raw_fdom)\n",
    "    \n",
    "    # convert timestamps to julian\n",
    "    fdom_trainset_raw = convert_df_julian_to_datetime(fdom_trainset_raw)\n",
    "    fdom_turb_trainset_raw = convert_df_julian_to_datetime(fdom_turb_trainset_raw)\n",
    "    fdom_stage_trainset_raw = convert_df_julian_to_datetime(fdom_stage_trainset_raw)\n",
    "\n",
    "    # add in new values\n",
    "    fdom_trainset_raw[\"series\"] = \"fDOM\"\n",
    "    fdom_trainset_raw[\"label\"] = \"\"\n",
    "\n",
    "    fdom_turb_trainset_raw[\"series\"] = \"turb\"\n",
    "    fdom_turb_trainset_raw[\"label\"] = \"\"\n",
    "\n",
    "    fdom_stage_trainset_raw[\"series\"] = \"stage\"\n",
    "    fdom_stage_trainset_raw[\"label\"] = \"\"\n",
    "\n",
    "    # reorder columns\n",
    "    fdom_trainset_raw = fdom_trainset_raw.reindex(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "    fdom_turb_trainset_raw = fdom_turb_trainset_raw.reindex(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "    fdom_stage_trainset_raw = fdom_stage_trainset_raw.reindex(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "\n",
    "    # concat into single dataframe\n",
    "    trainset_fdom_df = pd.concat([fdom_trainset_raw, fdom_turb_trainset_raw, fdom_stage_trainset_raw])\n",
    "\n",
    "    # sort together\n",
    "    trainset_fdom_df = trainset_fdom_df.sort_values(by=['timestamp'], kind='stable')\n",
    "\n",
    "    # export to csv\n",
    "    trainset_fdom_df.to_csv(trainset_fdom_path + \"fdom_augmented.csv\", index=False)\n",
    "\n",
    "    # ~~~~~~~ turbidity section ~~~~~~~\n",
    "\n",
    "    # create new dataframes\n",
    "    turb_trainset_raw = copy.deepcopy(augmented_turb_raw)\n",
    "    turb_fdom_trainset_raw = copy.deepcopy(augmented_fDOM_raw_turb)\n",
    "    turb_stage_trainset_raw = copy.deepcopy(augmented_stage_raw_turb)\n",
    "\n",
    "    # convert timestamps\n",
    "    turb_trainset_raw = convert_df_julian_to_datetime(turb_trainset_raw)\n",
    "    turb_fdom_trainset_raw = convert_df_julian_to_datetime(turb_fdom_trainset_raw)\n",
    "    turb_stage_trainset_raw = convert_df_julian_to_datetime(turb_stage_trainset_raw)\n",
    "\n",
    "    # add in new values\n",
    "    turb_fdom_trainset_raw[\"series\"] = \"fDOM\"\n",
    "    turb_fdom_trainset_raw[\"label\"] = \"\"\n",
    "\n",
    "    turb_trainset_raw[\"series\"] = \"turb\"\n",
    "    turb_trainset_raw[\"label\"] = \"\"\n",
    "\n",
    "    turb_stage_trainset_raw[\"series\"] = \"stage\"\n",
    "    turb_stage_trainset_raw[\"label\"] = \"\"\n",
    "\n",
    "    # reorder columns\n",
    "    turb_fdom_trainset_raw = turb_fdom_trainset_raw.reindex(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "    turb_trainset_raw = turb_trainset_raw.reindex(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "    turb_stage_trainset_raw = turb_stage_trainset_raw.reindex(columns=[\"series\", \"timestamp\", \"value\", \"label\"])\n",
    "\n",
    "    # concat into single dataframe\n",
    "    trainset_turb_df = pd.concat([turb_fdom_trainset_raw, turb_trainset_raw, turb_stage_trainset_raw])\n",
    "\n",
    "    # sort together\n",
    "    trainset_turb_df = trainset_turb_df.sort_values(by=['timestamp'], kind='stable')\n",
    "\n",
    "    # export to csv\n",
    "    trainset_turb_df.to_csv(trainset_turb_path + \"turb_augmented.csv\", index=False)\n",
    "\n",
    "#write_augmented_data_to_csv()\n",
    "write_to_trainset_csv()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f14ca78382e4dc227f1e51b680612414c34712d66eddea25f1c0d0c728b236b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('anomaly-detection': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
