{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Classification of Anomaly Peaks with 1D resnet\n",
    "Using normal train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "from sklearn import preprocessing\n",
    "from resnet import ResNet1D\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, balanced_accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "sys.path.insert(1, \"../\")\n",
    "\n",
    "from datasets import fdomDataset, fdomAugOnlyDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "WINDOW_SIZE = 15 # the size of each data segment\n",
    "TEST_SIZE = 0.10\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data files\n",
    "fdom_raw_data = (\n",
    "    \"../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv\"\n",
    ")\n",
    "stage_raw_data = \"../Data/converted_data/julian_format/stage_10.1.11-1.1.19.csv\"\n",
    "turb_raw_data = (\n",
    "    \"../Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv\"\n",
    ")\n",
    "\n",
    "fdom_labeled = \"../Data/labeled_data/ground_truths/fDOM/fDOM_all_julian_0k-300k.csv\"\n",
    "\n",
    "fdom_raw_augmented = \"../Data/augmented_data/fdom/unlabeled/unlabeled_fdom.csv\"\n",
    "fdom_labeled_augmented = \"../Data/augmented_data/fdom/labeled/labeled_fdom_peaks.csv\"\n",
    "\n",
    "turb_augmented_raw_data = \"../Data/augmented_data/fdom/unlabeled/unlabeled_turb.csv\"\n",
    "\n",
    "stage_augmented_data_fn = \"../Data/augmented_data/fdom/unlabeled/unlabeled_stage.csv\"\n",
    "\n",
    "fdom_fpt_lookup_path = \"../Data/augmented_data/fdom/fpt_lookup.csv\"\n",
    "fdom_fsk_lookup_path = \"../Data/augmented_data/fdom/fsk_lookup.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# get device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/claykaufmann/Projects/srrw-anomaly-detection/Deep_Learning/../Tools/get_candidates.py:221: PeakPropertyWarning: some peaks have a prominence of 0\n",
      "  peaks, props = find_peaks(\n",
      "/Users/claykaufmann/Projects/srrw-anomaly-detection/Deep_Learning/../Tools/get_candidates.py:389: PeakPropertyWarning: some peaks have a prominence of 0\n",
      "  peaks, props = find_peaks(\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "classes = [\"NAP\", \"FSK\", \"FPT\", \"PLP\", \"PP\", \"SKP\"]\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "targets = le.fit_transform(classes)\n",
    "\n",
    "# # train on class balanced data\n",
    "train_dataset = fdomAugOnlyDataset(\n",
    "    le,\n",
    "    fdom_raw_augmented,\n",
    "    stage_augmented_data_fn,\n",
    "    turb_augmented_raw_data,\n",
    "    fdom_labeled_augmented,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    fpt_lookup_filename=fdom_fpt_lookup_path,\n",
    "    fsk_lookup_filename=fdom_fsk_lookup_path,\n",
    ")\n",
    "\n",
    "# test on unbalanced data\n",
    "test_dataset = fdomDataset(\n",
    "    le,\n",
    "    fdom_raw_data,\n",
    "    stage_raw_data,\n",
    "    turb_raw_data,\n",
    "    fdom_labeled,\n",
    "    # fdom_raw_augmented, # uncomment these lines to add augmented data into test dataset\n",
    "    # stage_augmented_data_fn,\n",
    "    # turb_augmented_raw_data,\n",
    "    # fdom_labeled_augmented,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    fpt_lookup_filename=fdom_fpt_lookup_path,\n",
    "    fsk_lookup_filename=fdom_fsk_lookup_path,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training/testing\n",
    "This should not be the final iteration, this is just to get initial results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is a custom collate function which pads different length objects into one shape, allowing variable sized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_padd(batch):\n",
    "    '''\n",
    "    Pads batch of variable length\n",
    "    '''\n",
    "\n",
    "    label_list, sample_list, lengths = [], [], []\n",
    "\n",
    "    for (sample, label) in batch:\n",
    "        label_list.append(label)\n",
    "        # convert sample to tensor\n",
    "        sample = torch.tensor(sample, dtype=torch.float64).T # tranpose to send in data, pad_sequences won't accept original\n",
    "\n",
    "        # append to lengths\n",
    "        lengths.append(sample.shape[0])\n",
    "        \n",
    "        sample_list.append(sample)\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "\n",
    "    sample_list = torch.nn.utils.rnn.pad_sequence(sample_list, batch_first=True, padding_value=0)\n",
    "\n",
    "    # re-tranpose list, so we go back to a 4 channel dataset\n",
    "    sample_list = sample_list.transpose(1, 2)\n",
    "    \n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "    \n",
    "    return [sample_list.to(device), label_list.to(device), lengths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training / testing\n",
    "# train_size = int(0.85 * len(dataset))\n",
    "# test_size = len(dataset) - train_size\n",
    "# train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# create dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn_padd\n",
    ")\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn_padd\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1              [-1, 64, 716]           4,160\n",
      "   MyConv1dPadSame-2              [-1, 64, 716]               0\n",
      "       BatchNorm1d-3              [-1, 64, 716]             128\n",
      "              ReLU-4              [-1, 64, 716]               0\n",
      "            Conv1d-5              [-1, 64, 716]          65,600\n",
      "   MyConv1dPadSame-6              [-1, 64, 716]               0\n",
      "       BatchNorm1d-7              [-1, 64, 716]             128\n",
      "              ReLU-8              [-1, 64, 716]               0\n",
      "           Dropout-9              [-1, 64, 716]               0\n",
      "           Conv1d-10              [-1, 64, 716]          65,600\n",
      "  MyConv1dPadSame-11              [-1, 64, 716]               0\n",
      "       BasicBlock-12              [-1, 64, 716]               0\n",
      "      BatchNorm1d-13              [-1, 64, 716]             128\n",
      "             ReLU-14              [-1, 64, 716]               0\n",
      "          Dropout-15              [-1, 64, 716]               0\n",
      "           Conv1d-16              [-1, 64, 358]          65,600\n",
      "  MyConv1dPadSame-17              [-1, 64, 358]               0\n",
      "      BatchNorm1d-18              [-1, 64, 358]             128\n",
      "             ReLU-19              [-1, 64, 358]               0\n",
      "          Dropout-20              [-1, 64, 358]               0\n",
      "           Conv1d-21              [-1, 64, 358]          65,600\n",
      "  MyConv1dPadSame-22              [-1, 64, 358]               0\n",
      "        MaxPool1d-23              [-1, 64, 358]               0\n",
      "MyMaxPool1dPadSame-24              [-1, 64, 358]               0\n",
      "       BasicBlock-25              [-1, 64, 358]               0\n",
      "      BatchNorm1d-26              [-1, 64, 358]             128\n",
      "             ReLU-27              [-1, 64, 358]               0\n",
      "          Dropout-28              [-1, 64, 358]               0\n",
      "           Conv1d-29              [-1, 64, 358]          65,600\n",
      "  MyConv1dPadSame-30              [-1, 64, 358]               0\n",
      "      BatchNorm1d-31              [-1, 64, 358]             128\n",
      "             ReLU-32              [-1, 64, 358]               0\n",
      "          Dropout-33              [-1, 64, 358]               0\n",
      "           Conv1d-34              [-1, 64, 358]          65,600\n",
      "  MyConv1dPadSame-35              [-1, 64, 358]               0\n",
      "       BasicBlock-36              [-1, 64, 358]               0\n",
      "      BatchNorm1d-37              [-1, 64, 358]             128\n",
      "             ReLU-38              [-1, 64, 358]               0\n",
      "          Dropout-39              [-1, 64, 358]               0\n",
      "           Conv1d-40              [-1, 64, 358]          65,600\n",
      "  MyConv1dPadSame-41              [-1, 64, 358]               0\n",
      "      BatchNorm1d-42              [-1, 64, 358]             128\n",
      "             ReLU-43              [-1, 64, 358]               0\n",
      "          Dropout-44              [-1, 64, 358]               0\n",
      "           Conv1d-45              [-1, 64, 358]          65,600\n",
      "  MyConv1dPadSame-46              [-1, 64, 358]               0\n",
      "       BasicBlock-47              [-1, 64, 358]               0\n",
      "      BatchNorm1d-48              [-1, 64, 358]             128\n",
      "             ReLU-49              [-1, 64, 358]               0\n",
      "          Dropout-50              [-1, 64, 358]               0\n",
      "           Conv1d-51              [-1, 64, 358]          65,600\n",
      "  MyConv1dPadSame-52              [-1, 64, 358]               0\n",
      "      BatchNorm1d-53              [-1, 64, 358]             128\n",
      "             ReLU-54              [-1, 64, 358]               0\n",
      "          Dropout-55              [-1, 64, 358]               0\n",
      "           Conv1d-56              [-1, 64, 358]          65,600\n",
      "  MyConv1dPadSame-57              [-1, 64, 358]               0\n",
      "       BasicBlock-58              [-1, 64, 358]               0\n",
      "      BatchNorm1d-59              [-1, 64, 358]             128\n",
      "             ReLU-60              [-1, 64, 358]               0\n",
      "          Dropout-61              [-1, 64, 358]               0\n",
      "           Conv1d-62              [-1, 64, 358]          65,600\n",
      "  MyConv1dPadSame-63              [-1, 64, 358]               0\n",
      "      BatchNorm1d-64              [-1, 64, 358]             128\n",
      "             ReLU-65              [-1, 64, 358]               0\n",
      "          Dropout-66              [-1, 64, 358]               0\n",
      "           Conv1d-67              [-1, 64, 358]          65,600\n",
      "  MyConv1dPadSame-68              [-1, 64, 358]               0\n",
      "       BasicBlock-69              [-1, 64, 358]               0\n",
      "      BatchNorm1d-70              [-1, 64, 358]             128\n",
      "             ReLU-71              [-1, 64, 358]               0\n",
      "          Dropout-72              [-1, 64, 358]               0\n",
      "           Conv1d-73              [-1, 64, 358]          65,600\n",
      "  MyConv1dPadSame-74              [-1, 64, 358]               0\n",
      "      BatchNorm1d-75              [-1, 64, 358]             128\n",
      "             ReLU-76              [-1, 64, 358]               0\n",
      "          Dropout-77              [-1, 64, 358]               0\n",
      "           Conv1d-78              [-1, 64, 358]          65,600\n",
      "  MyConv1dPadSame-79              [-1, 64, 358]               0\n",
      "       BasicBlock-80              [-1, 64, 358]               0\n",
      "      BatchNorm1d-81              [-1, 64, 358]             128\n",
      "             ReLU-82              [-1, 64, 358]               0\n",
      "          Dropout-83              [-1, 64, 358]               0\n",
      "           Conv1d-84              [-1, 64, 179]          65,600\n",
      "  MyConv1dPadSame-85              [-1, 64, 179]               0\n",
      "      BatchNorm1d-86              [-1, 64, 179]             128\n",
      "             ReLU-87              [-1, 64, 179]               0\n",
      "          Dropout-88              [-1, 64, 179]               0\n",
      "           Conv1d-89              [-1, 64, 179]          65,600\n",
      "  MyConv1dPadSame-90              [-1, 64, 179]               0\n",
      "        MaxPool1d-91              [-1, 64, 179]               0\n",
      "MyMaxPool1dPadSame-92              [-1, 64, 179]               0\n",
      "       BasicBlock-93              [-1, 64, 179]               0\n",
      "      BatchNorm1d-94              [-1, 64, 179]             128\n",
      "             ReLU-95              [-1, 64, 179]               0\n",
      "          Dropout-96              [-1, 64, 179]               0\n",
      "           Conv1d-97              [-1, 64, 179]          65,600\n",
      "  MyConv1dPadSame-98              [-1, 64, 179]               0\n",
      "      BatchNorm1d-99              [-1, 64, 179]             128\n",
      "            ReLU-100              [-1, 64, 179]               0\n",
      "         Dropout-101              [-1, 64, 179]               0\n",
      "          Conv1d-102              [-1, 64, 179]          65,600\n",
      " MyConv1dPadSame-103              [-1, 64, 179]               0\n",
      "      BasicBlock-104              [-1, 64, 179]               0\n",
      "     BatchNorm1d-105              [-1, 64, 179]             128\n",
      "            ReLU-106              [-1, 64, 179]               0\n",
      "         Dropout-107              [-1, 64, 179]               0\n",
      "          Conv1d-108              [-1, 64, 179]          65,600\n",
      " MyConv1dPadSame-109              [-1, 64, 179]               0\n",
      "     BatchNorm1d-110              [-1, 64, 179]             128\n",
      "            ReLU-111              [-1, 64, 179]               0\n",
      "         Dropout-112              [-1, 64, 179]               0\n",
      "          Conv1d-113              [-1, 64, 179]          65,600\n",
      " MyConv1dPadSame-114              [-1, 64, 179]               0\n",
      "      BasicBlock-115              [-1, 64, 179]               0\n",
      "     BatchNorm1d-116              [-1, 64, 179]             128\n",
      "            ReLU-117              [-1, 64, 179]               0\n",
      "         Dropout-118              [-1, 64, 179]               0\n",
      "          Conv1d-119              [-1, 64, 179]          65,600\n",
      " MyConv1dPadSame-120              [-1, 64, 179]               0\n",
      "     BatchNorm1d-121              [-1, 64, 179]             128\n",
      "            ReLU-122              [-1, 64, 179]               0\n",
      "         Dropout-123              [-1, 64, 179]               0\n",
      "          Conv1d-124              [-1, 64, 179]          65,600\n",
      " MyConv1dPadSame-125              [-1, 64, 179]               0\n",
      "      BasicBlock-126              [-1, 64, 179]               0\n",
      "     BatchNorm1d-127              [-1, 64, 179]             128\n",
      "            ReLU-128              [-1, 64, 179]               0\n",
      "         Dropout-129              [-1, 64, 179]               0\n",
      "          Conv1d-130              [-1, 64, 179]          65,600\n",
      " MyConv1dPadSame-131              [-1, 64, 179]               0\n",
      "     BatchNorm1d-132              [-1, 64, 179]             128\n",
      "            ReLU-133              [-1, 64, 179]               0\n",
      "         Dropout-134              [-1, 64, 179]               0\n",
      "          Conv1d-135              [-1, 64, 179]          65,600\n",
      " MyConv1dPadSame-136              [-1, 64, 179]               0\n",
      "      BasicBlock-137              [-1, 64, 179]               0\n",
      "     BatchNorm1d-138              [-1, 64, 179]             128\n",
      "            ReLU-139              [-1, 64, 179]               0\n",
      "         Dropout-140              [-1, 64, 179]               0\n",
      "          Conv1d-141             [-1, 128, 179]         131,200\n",
      " MyConv1dPadSame-142             [-1, 128, 179]               0\n",
      "     BatchNorm1d-143             [-1, 128, 179]             256\n",
      "            ReLU-144             [-1, 128, 179]               0\n",
      "         Dropout-145             [-1, 128, 179]               0\n",
      "          Conv1d-146             [-1, 128, 179]         262,272\n",
      " MyConv1dPadSame-147             [-1, 128, 179]               0\n",
      "      BasicBlock-148             [-1, 128, 179]               0\n",
      "     BatchNorm1d-149             [-1, 128, 179]             256\n",
      "            ReLU-150             [-1, 128, 179]               0\n",
      "         Dropout-151             [-1, 128, 179]               0\n",
      "          Conv1d-152              [-1, 128, 90]         262,272\n",
      " MyConv1dPadSame-153              [-1, 128, 90]               0\n",
      "     BatchNorm1d-154              [-1, 128, 90]             256\n",
      "            ReLU-155              [-1, 128, 90]               0\n",
      "         Dropout-156              [-1, 128, 90]               0\n",
      "          Conv1d-157              [-1, 128, 90]         262,272\n",
      " MyConv1dPadSame-158              [-1, 128, 90]               0\n",
      "       MaxPool1d-159              [-1, 128, 90]               0\n",
      "MyMaxPool1dPadSame-160              [-1, 128, 90]               0\n",
      "      BasicBlock-161              [-1, 128, 90]               0\n",
      "     BatchNorm1d-162              [-1, 128, 90]             256\n",
      "            ReLU-163              [-1, 128, 90]               0\n",
      "         Dropout-164              [-1, 128, 90]               0\n",
      "          Conv1d-165              [-1, 128, 90]         262,272\n",
      " MyConv1dPadSame-166              [-1, 128, 90]               0\n",
      "     BatchNorm1d-167              [-1, 128, 90]             256\n",
      "            ReLU-168              [-1, 128, 90]               0\n",
      "         Dropout-169              [-1, 128, 90]               0\n",
      "          Conv1d-170              [-1, 128, 90]         262,272\n",
      " MyConv1dPadSame-171              [-1, 128, 90]               0\n",
      "      BasicBlock-172              [-1, 128, 90]               0\n",
      "     BatchNorm1d-173              [-1, 128, 90]             256\n",
      "            ReLU-174              [-1, 128, 90]               0\n",
      "         Dropout-175              [-1, 128, 90]               0\n",
      "          Conv1d-176              [-1, 128, 90]         262,272\n",
      " MyConv1dPadSame-177              [-1, 128, 90]               0\n",
      "     BatchNorm1d-178              [-1, 128, 90]             256\n",
      "            ReLU-179              [-1, 128, 90]               0\n",
      "         Dropout-180              [-1, 128, 90]               0\n",
      "          Conv1d-181              [-1, 128, 90]         262,272\n",
      " MyConv1dPadSame-182              [-1, 128, 90]               0\n",
      "      BasicBlock-183              [-1, 128, 90]               0\n",
      "     BatchNorm1d-184              [-1, 128, 90]             256\n",
      "            ReLU-185              [-1, 128, 90]               0\n",
      "         Dropout-186              [-1, 128, 90]               0\n",
      "          Conv1d-187              [-1, 128, 90]         262,272\n",
      " MyConv1dPadSame-188              [-1, 128, 90]               0\n",
      "     BatchNorm1d-189              [-1, 128, 90]             256\n",
      "            ReLU-190              [-1, 128, 90]               0\n",
      "         Dropout-191              [-1, 128, 90]               0\n",
      "          Conv1d-192              [-1, 128, 90]         262,272\n",
      " MyConv1dPadSame-193              [-1, 128, 90]               0\n",
      "      BasicBlock-194              [-1, 128, 90]               0\n",
      "     BatchNorm1d-195              [-1, 128, 90]             256\n",
      "            ReLU-196              [-1, 128, 90]               0\n",
      "         Dropout-197              [-1, 128, 90]               0\n",
      "          Conv1d-198              [-1, 128, 90]         262,272\n",
      " MyConv1dPadSame-199              [-1, 128, 90]               0\n",
      "     BatchNorm1d-200              [-1, 128, 90]             256\n",
      "            ReLU-201              [-1, 128, 90]               0\n",
      "         Dropout-202              [-1, 128, 90]               0\n",
      "          Conv1d-203              [-1, 128, 90]         262,272\n",
      " MyConv1dPadSame-204              [-1, 128, 90]               0\n",
      "      BasicBlock-205              [-1, 128, 90]               0\n",
      "     BatchNorm1d-206              [-1, 128, 90]             256\n",
      "            ReLU-207              [-1, 128, 90]               0\n",
      "         Dropout-208              [-1, 128, 90]               0\n",
      "          Conv1d-209              [-1, 128, 90]         262,272\n",
      " MyConv1dPadSame-210              [-1, 128, 90]               0\n",
      "     BatchNorm1d-211              [-1, 128, 90]             256\n",
      "            ReLU-212              [-1, 128, 90]               0\n",
      "         Dropout-213              [-1, 128, 90]               0\n",
      "          Conv1d-214              [-1, 128, 90]         262,272\n",
      " MyConv1dPadSame-215              [-1, 128, 90]               0\n",
      "      BasicBlock-216              [-1, 128, 90]               0\n",
      "     BatchNorm1d-217              [-1, 128, 90]             256\n",
      "            ReLU-218              [-1, 128, 90]               0\n",
      "         Dropout-219              [-1, 128, 90]               0\n",
      "          Conv1d-220              [-1, 128, 45]         262,272\n",
      " MyConv1dPadSame-221              [-1, 128, 45]               0\n",
      "     BatchNorm1d-222              [-1, 128, 45]             256\n",
      "            ReLU-223              [-1, 128, 45]               0\n",
      "         Dropout-224              [-1, 128, 45]               0\n",
      "          Conv1d-225              [-1, 128, 45]         262,272\n",
      " MyConv1dPadSame-226              [-1, 128, 45]               0\n",
      "       MaxPool1d-227              [-1, 128, 45]               0\n",
      "MyMaxPool1dPadSame-228              [-1, 128, 45]               0\n",
      "      BasicBlock-229              [-1, 128, 45]               0\n",
      "     BatchNorm1d-230              [-1, 128, 45]             256\n",
      "            ReLU-231              [-1, 128, 45]               0\n",
      "         Dropout-232              [-1, 128, 45]               0\n",
      "          Conv1d-233              [-1, 128, 45]         262,272\n",
      " MyConv1dPadSame-234              [-1, 128, 45]               0\n",
      "     BatchNorm1d-235              [-1, 128, 45]             256\n",
      "            ReLU-236              [-1, 128, 45]               0\n",
      "         Dropout-237              [-1, 128, 45]               0\n",
      "          Conv1d-238              [-1, 128, 45]         262,272\n",
      " MyConv1dPadSame-239              [-1, 128, 45]               0\n",
      "      BasicBlock-240              [-1, 128, 45]               0\n",
      "     BatchNorm1d-241              [-1, 128, 45]             256\n",
      "            ReLU-242              [-1, 128, 45]               0\n",
      "         Dropout-243              [-1, 128, 45]               0\n",
      "          Conv1d-244              [-1, 128, 45]         262,272\n",
      " MyConv1dPadSame-245              [-1, 128, 45]               0\n",
      "     BatchNorm1d-246              [-1, 128, 45]             256\n",
      "            ReLU-247              [-1, 128, 45]               0\n",
      "         Dropout-248              [-1, 128, 45]               0\n",
      "          Conv1d-249              [-1, 128, 45]         262,272\n",
      " MyConv1dPadSame-250              [-1, 128, 45]               0\n",
      "      BasicBlock-251              [-1, 128, 45]               0\n",
      "     BatchNorm1d-252              [-1, 128, 45]             256\n",
      "            ReLU-253              [-1, 128, 45]               0\n",
      "         Dropout-254              [-1, 128, 45]               0\n",
      "          Conv1d-255              [-1, 128, 45]         262,272\n",
      " MyConv1dPadSame-256              [-1, 128, 45]               0\n",
      "     BatchNorm1d-257              [-1, 128, 45]             256\n",
      "            ReLU-258              [-1, 128, 45]               0\n",
      "         Dropout-259              [-1, 128, 45]               0\n",
      "          Conv1d-260              [-1, 128, 45]         262,272\n",
      " MyConv1dPadSame-261              [-1, 128, 45]               0\n",
      "      BasicBlock-262              [-1, 128, 45]               0\n",
      "     BatchNorm1d-263              [-1, 128, 45]             256\n",
      "            ReLU-264              [-1, 128, 45]               0\n",
      "         Dropout-265              [-1, 128, 45]               0\n",
      "          Conv1d-266              [-1, 128, 45]         262,272\n",
      " MyConv1dPadSame-267              [-1, 128, 45]               0\n",
      "     BatchNorm1d-268              [-1, 128, 45]             256\n",
      "            ReLU-269              [-1, 128, 45]               0\n",
      "         Dropout-270              [-1, 128, 45]               0\n",
      "          Conv1d-271              [-1, 128, 45]         262,272\n",
      " MyConv1dPadSame-272              [-1, 128, 45]               0\n",
      "      BasicBlock-273              [-1, 128, 45]               0\n",
      "     BatchNorm1d-274              [-1, 128, 45]             256\n",
      "            ReLU-275              [-1, 128, 45]               0\n",
      "         Dropout-276              [-1, 128, 45]               0\n",
      "          Conv1d-277              [-1, 256, 45]         524,544\n",
      " MyConv1dPadSame-278              [-1, 256, 45]               0\n",
      "     BatchNorm1d-279              [-1, 256, 45]             512\n",
      "            ReLU-280              [-1, 256, 45]               0\n",
      "         Dropout-281              [-1, 256, 45]               0\n",
      "          Conv1d-282              [-1, 256, 45]       1,048,832\n",
      " MyConv1dPadSame-283              [-1, 256, 45]               0\n",
      "      BasicBlock-284              [-1, 256, 45]               0\n",
      "     BatchNorm1d-285              [-1, 256, 45]             512\n",
      "            ReLU-286              [-1, 256, 45]               0\n",
      "         Dropout-287              [-1, 256, 45]               0\n",
      "          Conv1d-288              [-1, 256, 23]       1,048,832\n",
      " MyConv1dPadSame-289              [-1, 256, 23]               0\n",
      "     BatchNorm1d-290              [-1, 256, 23]             512\n",
      "            ReLU-291              [-1, 256, 23]               0\n",
      "         Dropout-292              [-1, 256, 23]               0\n",
      "          Conv1d-293              [-1, 256, 23]       1,048,832\n",
      " MyConv1dPadSame-294              [-1, 256, 23]               0\n",
      "       MaxPool1d-295              [-1, 256, 23]               0\n",
      "MyMaxPool1dPadSame-296              [-1, 256, 23]               0\n",
      "      BasicBlock-297              [-1, 256, 23]               0\n",
      "     BatchNorm1d-298              [-1, 256, 23]             512\n",
      "            ReLU-299              [-1, 256, 23]               0\n",
      "         Dropout-300              [-1, 256, 23]               0\n",
      "          Conv1d-301              [-1, 256, 23]       1,048,832\n",
      " MyConv1dPadSame-302              [-1, 256, 23]               0\n",
      "     BatchNorm1d-303              [-1, 256, 23]             512\n",
      "            ReLU-304              [-1, 256, 23]               0\n",
      "         Dropout-305              [-1, 256, 23]               0\n",
      "          Conv1d-306              [-1, 256, 23]       1,048,832\n",
      " MyConv1dPadSame-307              [-1, 256, 23]               0\n",
      "      BasicBlock-308              [-1, 256, 23]               0\n",
      "     BatchNorm1d-309              [-1, 256, 23]             512\n",
      "            ReLU-310              [-1, 256, 23]               0\n",
      "         Dropout-311              [-1, 256, 23]               0\n",
      "          Conv1d-312              [-1, 256, 23]       1,048,832\n",
      " MyConv1dPadSame-313              [-1, 256, 23]               0\n",
      "     BatchNorm1d-314              [-1, 256, 23]             512\n",
      "            ReLU-315              [-1, 256, 23]               0\n",
      "         Dropout-316              [-1, 256, 23]               0\n",
      "          Conv1d-317              [-1, 256, 23]       1,048,832\n",
      " MyConv1dPadSame-318              [-1, 256, 23]               0\n",
      "      BasicBlock-319              [-1, 256, 23]               0\n",
      "     BatchNorm1d-320              [-1, 256, 23]             512\n",
      "            ReLU-321              [-1, 256, 23]               0\n",
      "         Dropout-322              [-1, 256, 23]               0\n",
      "          Conv1d-323              [-1, 256, 23]       1,048,832\n",
      " MyConv1dPadSame-324              [-1, 256, 23]               0\n",
      "     BatchNorm1d-325              [-1, 256, 23]             512\n",
      "            ReLU-326              [-1, 256, 23]               0\n",
      "         Dropout-327              [-1, 256, 23]               0\n",
      "          Conv1d-328              [-1, 256, 23]       1,048,832\n",
      " MyConv1dPadSame-329              [-1, 256, 23]               0\n",
      "      BasicBlock-330              [-1, 256, 23]               0\n",
      "     BatchNorm1d-331              [-1, 256, 23]             512\n",
      "            ReLU-332              [-1, 256, 23]               0\n",
      "         Dropout-333              [-1, 256, 23]               0\n",
      "          Conv1d-334              [-1, 256, 23]       1,048,832\n",
      " MyConv1dPadSame-335              [-1, 256, 23]               0\n",
      "     BatchNorm1d-336              [-1, 256, 23]             512\n",
      "            ReLU-337              [-1, 256, 23]               0\n",
      "         Dropout-338              [-1, 256, 23]               0\n",
      "          Conv1d-339              [-1, 256, 23]       1,048,832\n",
      " MyConv1dPadSame-340              [-1, 256, 23]               0\n",
      "      BasicBlock-341              [-1, 256, 23]               0\n",
      "     BatchNorm1d-342              [-1, 256, 23]             512\n",
      "            ReLU-343              [-1, 256, 23]               0\n",
      "         Dropout-344              [-1, 256, 23]               0\n",
      "          Conv1d-345              [-1, 256, 23]       1,048,832\n",
      " MyConv1dPadSame-346              [-1, 256, 23]               0\n",
      "     BatchNorm1d-347              [-1, 256, 23]             512\n",
      "            ReLU-348              [-1, 256, 23]               0\n",
      "         Dropout-349              [-1, 256, 23]               0\n",
      "          Conv1d-350              [-1, 256, 23]       1,048,832\n",
      " MyConv1dPadSame-351              [-1, 256, 23]               0\n",
      "      BasicBlock-352              [-1, 256, 23]               0\n",
      "     BatchNorm1d-353              [-1, 256, 23]             512\n",
      "            ReLU-354              [-1, 256, 23]               0\n",
      "         Dropout-355              [-1, 256, 23]               0\n",
      "          Conv1d-356              [-1, 256, 12]       1,048,832\n",
      " MyConv1dPadSame-357              [-1, 256, 12]               0\n",
      "     BatchNorm1d-358              [-1, 256, 12]             512\n",
      "            ReLU-359              [-1, 256, 12]               0\n",
      "         Dropout-360              [-1, 256, 12]               0\n",
      "          Conv1d-361              [-1, 256, 12]       1,048,832\n",
      " MyConv1dPadSame-362              [-1, 256, 12]               0\n",
      "       MaxPool1d-363              [-1, 256, 12]               0\n",
      "MyMaxPool1dPadSame-364              [-1, 256, 12]               0\n",
      "      BasicBlock-365              [-1, 256, 12]               0\n",
      "     BatchNorm1d-366              [-1, 256, 12]             512\n",
      "            ReLU-367              [-1, 256, 12]               0\n",
      "         Dropout-368              [-1, 256, 12]               0\n",
      "          Conv1d-369              [-1, 256, 12]       1,048,832\n",
      " MyConv1dPadSame-370              [-1, 256, 12]               0\n",
      "     BatchNorm1d-371              [-1, 256, 12]             512\n",
      "            ReLU-372              [-1, 256, 12]               0\n",
      "         Dropout-373              [-1, 256, 12]               0\n",
      "          Conv1d-374              [-1, 256, 12]       1,048,832\n",
      " MyConv1dPadSame-375              [-1, 256, 12]               0\n",
      "      BasicBlock-376              [-1, 256, 12]               0\n",
      "     BatchNorm1d-377              [-1, 256, 12]             512\n",
      "            ReLU-378              [-1, 256, 12]               0\n",
      "         Dropout-379              [-1, 256, 12]               0\n",
      "          Conv1d-380              [-1, 256, 12]       1,048,832\n",
      " MyConv1dPadSame-381              [-1, 256, 12]               0\n",
      "     BatchNorm1d-382              [-1, 256, 12]             512\n",
      "            ReLU-383              [-1, 256, 12]               0\n",
      "         Dropout-384              [-1, 256, 12]               0\n",
      "          Conv1d-385              [-1, 256, 12]       1,048,832\n",
      " MyConv1dPadSame-386              [-1, 256, 12]               0\n",
      "      BasicBlock-387              [-1, 256, 12]               0\n",
      "     BatchNorm1d-388              [-1, 256, 12]             512\n",
      "            ReLU-389              [-1, 256, 12]               0\n",
      "         Dropout-390              [-1, 256, 12]               0\n",
      "          Conv1d-391              [-1, 256, 12]       1,048,832\n",
      " MyConv1dPadSame-392              [-1, 256, 12]               0\n",
      "     BatchNorm1d-393              [-1, 256, 12]             512\n",
      "            ReLU-394              [-1, 256, 12]               0\n",
      "         Dropout-395              [-1, 256, 12]               0\n",
      "          Conv1d-396              [-1, 256, 12]       1,048,832\n",
      " MyConv1dPadSame-397              [-1, 256, 12]               0\n",
      "      BasicBlock-398              [-1, 256, 12]               0\n",
      "     BatchNorm1d-399              [-1, 256, 12]             512\n",
      "            ReLU-400              [-1, 256, 12]               0\n",
      "         Dropout-401              [-1, 256, 12]               0\n",
      "          Conv1d-402              [-1, 256, 12]       1,048,832\n",
      " MyConv1dPadSame-403              [-1, 256, 12]               0\n",
      "     BatchNorm1d-404              [-1, 256, 12]             512\n",
      "            ReLU-405              [-1, 256, 12]               0\n",
      "         Dropout-406              [-1, 256, 12]               0\n",
      "          Conv1d-407              [-1, 256, 12]       1,048,832\n",
      " MyConv1dPadSame-408              [-1, 256, 12]               0\n",
      "      BasicBlock-409              [-1, 256, 12]               0\n",
      "     BatchNorm1d-410              [-1, 256, 12]             512\n",
      "            ReLU-411              [-1, 256, 12]               0\n",
      "         Dropout-412              [-1, 256, 12]               0\n",
      "          Conv1d-413              [-1, 512, 12]       2,097,664\n",
      " MyConv1dPadSame-414              [-1, 512, 12]               0\n",
      "     BatchNorm1d-415              [-1, 512, 12]           1,024\n",
      "            ReLU-416              [-1, 512, 12]               0\n",
      "         Dropout-417              [-1, 512, 12]               0\n",
      "          Conv1d-418              [-1, 512, 12]       4,194,816\n",
      " MyConv1dPadSame-419              [-1, 512, 12]               0\n",
      "      BasicBlock-420              [-1, 512, 12]               0\n",
      "     BatchNorm1d-421              [-1, 512, 12]           1,024\n",
      "            ReLU-422              [-1, 512, 12]               0\n",
      "         Dropout-423              [-1, 512, 12]               0\n",
      "          Conv1d-424               [-1, 512, 6]       4,194,816\n",
      " MyConv1dPadSame-425               [-1, 512, 6]               0\n",
      "     BatchNorm1d-426               [-1, 512, 6]           1,024\n",
      "            ReLU-427               [-1, 512, 6]               0\n",
      "         Dropout-428               [-1, 512, 6]               0\n",
      "          Conv1d-429               [-1, 512, 6]       4,194,816\n",
      " MyConv1dPadSame-430               [-1, 512, 6]               0\n",
      "       MaxPool1d-431               [-1, 512, 6]               0\n",
      "MyMaxPool1dPadSame-432               [-1, 512, 6]               0\n",
      "      BasicBlock-433               [-1, 512, 6]               0\n",
      "     BatchNorm1d-434               [-1, 512, 6]           1,024\n",
      "            ReLU-435               [-1, 512, 6]               0\n",
      "         Dropout-436               [-1, 512, 6]               0\n",
      "          Conv1d-437               [-1, 512, 6]       4,194,816\n",
      " MyConv1dPadSame-438               [-1, 512, 6]               0\n",
      "     BatchNorm1d-439               [-1, 512, 6]           1,024\n",
      "            ReLU-440               [-1, 512, 6]               0\n",
      "         Dropout-441               [-1, 512, 6]               0\n",
      "          Conv1d-442               [-1, 512, 6]       4,194,816\n",
      " MyConv1dPadSame-443               [-1, 512, 6]               0\n",
      "      BasicBlock-444               [-1, 512, 6]               0\n",
      "     BatchNorm1d-445               [-1, 512, 6]           1,024\n",
      "            ReLU-446               [-1, 512, 6]               0\n",
      "         Dropout-447               [-1, 512, 6]               0\n",
      "          Conv1d-448               [-1, 512, 6]       4,194,816\n",
      " MyConv1dPadSame-449               [-1, 512, 6]               0\n",
      "     BatchNorm1d-450               [-1, 512, 6]           1,024\n",
      "            ReLU-451               [-1, 512, 6]               0\n",
      "         Dropout-452               [-1, 512, 6]               0\n",
      "          Conv1d-453               [-1, 512, 6]       4,194,816\n",
      " MyConv1dPadSame-454               [-1, 512, 6]               0\n",
      "      BasicBlock-455               [-1, 512, 6]               0\n",
      "     BatchNorm1d-456               [-1, 512, 6]           1,024\n",
      "            ReLU-457               [-1, 512, 6]               0\n",
      "         Dropout-458               [-1, 512, 6]               0\n",
      "          Conv1d-459               [-1, 512, 6]       4,194,816\n",
      " MyConv1dPadSame-460               [-1, 512, 6]               0\n",
      "     BatchNorm1d-461               [-1, 512, 6]           1,024\n",
      "            ReLU-462               [-1, 512, 6]               0\n",
      "         Dropout-463               [-1, 512, 6]               0\n",
      "          Conv1d-464               [-1, 512, 6]       4,194,816\n",
      " MyConv1dPadSame-465               [-1, 512, 6]               0\n",
      "      BasicBlock-466               [-1, 512, 6]               0\n",
      "     BatchNorm1d-467               [-1, 512, 6]           1,024\n",
      "            ReLU-468               [-1, 512, 6]               0\n",
      "         Dropout-469               [-1, 512, 6]               0\n",
      "          Conv1d-470               [-1, 512, 6]       4,194,816\n",
      " MyConv1dPadSame-471               [-1, 512, 6]               0\n",
      "     BatchNorm1d-472               [-1, 512, 6]           1,024\n",
      "            ReLU-473               [-1, 512, 6]               0\n",
      "         Dropout-474               [-1, 512, 6]               0\n",
      "          Conv1d-475               [-1, 512, 6]       4,194,816\n",
      " MyConv1dPadSame-476               [-1, 512, 6]               0\n",
      "      BasicBlock-477               [-1, 512, 6]               0\n",
      "     BatchNorm1d-478               [-1, 512, 6]           1,024\n",
      "            ReLU-479               [-1, 512, 6]               0\n",
      "         Dropout-480               [-1, 512, 6]               0\n",
      "          Conv1d-481               [-1, 512, 6]       4,194,816\n",
      " MyConv1dPadSame-482               [-1, 512, 6]               0\n",
      "     BatchNorm1d-483               [-1, 512, 6]           1,024\n",
      "            ReLU-484               [-1, 512, 6]               0\n",
      "         Dropout-485               [-1, 512, 6]               0\n",
      "          Conv1d-486               [-1, 512, 6]       4,194,816\n",
      " MyConv1dPadSame-487               [-1, 512, 6]               0\n",
      "      BasicBlock-488               [-1, 512, 6]               0\n",
      "     BatchNorm1d-489               [-1, 512, 6]           1,024\n",
      "            ReLU-490               [-1, 512, 6]               0\n",
      "         Dropout-491               [-1, 512, 6]               0\n",
      "          Conv1d-492               [-1, 512, 3]       4,194,816\n",
      " MyConv1dPadSame-493               [-1, 512, 3]               0\n",
      "     BatchNorm1d-494               [-1, 512, 3]           1,024\n",
      "            ReLU-495               [-1, 512, 3]               0\n",
      "         Dropout-496               [-1, 512, 3]               0\n",
      "          Conv1d-497               [-1, 512, 3]       4,194,816\n",
      " MyConv1dPadSame-498               [-1, 512, 3]               0\n",
      "       MaxPool1d-499               [-1, 512, 3]               0\n",
      "MyMaxPool1dPadSame-500               [-1, 512, 3]               0\n",
      "      BasicBlock-501               [-1, 512, 3]               0\n",
      "     BatchNorm1d-502               [-1, 512, 3]           1,024\n",
      "            ReLU-503               [-1, 512, 3]               0\n",
      "         Dropout-504               [-1, 512, 3]               0\n",
      "          Conv1d-505               [-1, 512, 3]       4,194,816\n",
      " MyConv1dPadSame-506               [-1, 512, 3]               0\n",
      "     BatchNorm1d-507               [-1, 512, 3]           1,024\n",
      "            ReLU-508               [-1, 512, 3]               0\n",
      "         Dropout-509               [-1, 512, 3]               0\n",
      "          Conv1d-510               [-1, 512, 3]       4,194,816\n",
      " MyConv1dPadSame-511               [-1, 512, 3]               0\n",
      "      BasicBlock-512               [-1, 512, 3]               0\n",
      "     BatchNorm1d-513               [-1, 512, 3]           1,024\n",
      "            ReLU-514               [-1, 512, 3]               0\n",
      "         Dropout-515               [-1, 512, 3]               0\n",
      "          Conv1d-516               [-1, 512, 3]       4,194,816\n",
      " MyConv1dPadSame-517               [-1, 512, 3]               0\n",
      "     BatchNorm1d-518               [-1, 512, 3]           1,024\n",
      "            ReLU-519               [-1, 512, 3]               0\n",
      "         Dropout-520               [-1, 512, 3]               0\n",
      "          Conv1d-521               [-1, 512, 3]       4,194,816\n",
      " MyConv1dPadSame-522               [-1, 512, 3]               0\n",
      "      BasicBlock-523               [-1, 512, 3]               0\n",
      "     BatchNorm1d-524               [-1, 512, 3]           1,024\n",
      "            ReLU-525               [-1, 512, 3]               0\n",
      "         Dropout-526               [-1, 512, 3]               0\n",
      "          Conv1d-527               [-1, 512, 3]       4,194,816\n",
      " MyConv1dPadSame-528               [-1, 512, 3]               0\n",
      "     BatchNorm1d-529               [-1, 512, 3]           1,024\n",
      "            ReLU-530               [-1, 512, 3]               0\n",
      "         Dropout-531               [-1, 512, 3]               0\n",
      "          Conv1d-532               [-1, 512, 3]       4,194,816\n",
      " MyConv1dPadSame-533               [-1, 512, 3]               0\n",
      "      BasicBlock-534               [-1, 512, 3]               0\n",
      "     BatchNorm1d-535               [-1, 512, 3]           1,024\n",
      "            ReLU-536               [-1, 512, 3]               0\n",
      "         Dropout-537               [-1, 512, 3]               0\n",
      "          Conv1d-538               [-1, 512, 3]       4,194,816\n",
      " MyConv1dPadSame-539               [-1, 512, 3]               0\n",
      "     BatchNorm1d-540               [-1, 512, 3]           1,024\n",
      "            ReLU-541               [-1, 512, 3]               0\n",
      "         Dropout-542               [-1, 512, 3]               0\n",
      "          Conv1d-543               [-1, 512, 3]       4,194,816\n",
      " MyConv1dPadSame-544               [-1, 512, 3]               0\n",
      "      BasicBlock-545               [-1, 512, 3]               0\n",
      "     BatchNorm1d-546               [-1, 512, 3]           1,024\n",
      "            ReLU-547               [-1, 512, 3]               0\n",
      "          Linear-548                    [-1, 6]           3,078\n",
      "================================================================\n",
      "Total params: 131,017,414\n",
      "Trainable params: 131,017,414\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 40.64\n",
      "Params size (MB): 499.79\n",
      "Estimated Total Size (MB): 540.44\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# init model\n",
    "model = ResNet1D(\n",
    "    in_channels=4,\n",
    "    base_filters=64,\n",
    "    kernel_size=16,\n",
    "    stride=2,\n",
    "    n_block=48,\n",
    "    groups=1,  # check this\n",
    "    n_classes=len(classes),\n",
    "    downsample_gap=6,\n",
    "    increasefilter_gap=12,\n",
    "    verbose=False,\n",
    ").to(device)\n",
    "\n",
    "model = model.float()\n",
    "\n",
    "# print a model summary\n",
    "print(summary(model, (4, 716))) # 4 channels, of length (max batch length, set at 716 here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer/criterion\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "all_loss = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/118 [00:00<?, ?it/s]/var/folders/3w/lhkpgfc505n81_2vs8svxfpr0000gn/T/ipykernel_62890/2188637797.py:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/miniforge3/conda-bld/pytorch-recipe_1647804309932/work/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  label_list = torch.tensor(label_list, dtype=torch.int64)\n",
      "                                                               \r"
     ]
    }
   ],
   "source": [
    "prog_bar = tqdm(trainloader, desc='Training', leave=False)\n",
    "for i, batch in enumerate(prog_bar):\n",
    "    x = batch[0].to(device)\n",
    "\n",
    "    # squeeze y to flatten predictions into 1d tensor\n",
    "    y = batch[1].squeeze().to(device)\n",
    "\n",
    "    pred = model(x.float())\n",
    "\n",
    "    loss = criterion(pred, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    all_loss.append(loss.item())\n",
    "\n",
    "# save model\n",
    "torch.save(model, \"./results/models/fdom/raw/may-two.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/srrw/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/srrw/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/srrw/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class: NAP   is 29.4 %\n",
      "Accuracy for class: FSK   is 100.0 %\n",
      "Accuracy for class: FPT   is 0.0 %\n",
      "Accuracy for class: PLP   is 100.0 %\n",
      "Accuracy for class: PP    is 6.6 %\n",
      "Accuracy for class: SKP   is 0.0 %\n",
      "[[447   1   0 609  45 416]\n",
      " [  0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   1]\n",
      " [  0   0   0  26   0   0]\n",
      " [ 30   0   0   0   4  27]\n",
      " [  0   0   0  29   0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         FPT       0.00      0.00      0.00         1\n",
      "         FSK       0.50      1.00      0.67         1\n",
      "         NAP       0.94      0.29      0.45      1518\n",
      "         PLP       0.04      1.00      0.08        26\n",
      "          PP       0.08      0.07      0.07        61\n",
      "         SKP       0.00      0.00      0.00        29\n",
      "\n",
      "    accuracy                           0.29      1636\n",
      "   macro avg       0.26      0.39      0.21      1636\n",
      "weighted avg       0.87      0.29      0.42      1636\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'fDOM Peak Detection Ratio Confusion Matrix'}>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test model\n",
    "model = torch.load(\"./results/models/fdom/raw/may-two.pt\")\n",
    "model.eval()\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "prog_bar = tqdm(testloader, desc=\"Testing\", leave=False)\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(prog_bar):\n",
    "        x = batch[0].to(device)\n",
    "\n",
    "        y = batch[1].squeeze().to(device)\n",
    "\n",
    "        outs = model(x.float())\n",
    "\n",
    "        _, preds = torch.max(outs, 1)\n",
    "\n",
    "        for label, prediction in zip(y, preds):\n",
    "            # convert label and prediction to current vals\n",
    "            label = le.inverse_transform([label])[0]\n",
    "            prediction = le.inverse_transform([prediction])[0]\n",
    "\n",
    "            y_pred.append(prediction)\n",
    "            y_true.append(label)\n",
    "\n",
    "\n",
    "# build conf matrix\n",
    "conf = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "\n",
    "# review the classnames here\n",
    "df_cm = pd.DataFrame(\n",
    "    conf / conf.sum(axis=1)[:, np.newaxis], index=[i for i in classes], columns=[i for i in classes]\n",
    ")\n",
    "\n",
    "# classification report\n",
    "acc_report = classification_report(y_true, y_pred)\n",
    "print(acc_report)\n",
    "\n",
    "bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "print(f\"Balanced accuracy: {bal_acc}\")\n",
    "\n",
    "# display conf matrix\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "plt.xlabel(\"Ground Truths\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(label=\"fDOM Peak Detection Ratio Confusion Matrix\")\n",
    "\n",
    "plot = sn.heatmap(df_cm, annot=True)\n",
    "plot.get_figure().savefig(\"./results/graphics/fdom/raw/may-2.png\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ed753961bdc37ee89b4275051722ceb8ec0b57b8793db9d189305c313070a7d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('srrw')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
