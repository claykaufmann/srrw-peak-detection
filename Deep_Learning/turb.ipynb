{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "from sklearn import preprocessing\n",
    "from resnet import ResNet1D\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    balanced_accuracy_score,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "sys.path.insert(1, \"../\")\n",
    "\n",
    "from datasets import turbidityDataset, turbAugOnlyDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "WINDOW_SIZE = 15  # the size of each data segment\n",
    "TEST_SIZE = 0.10\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data files\n",
    "fdom_raw_data = \"../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv\"\n",
    "stage_raw_data = \"../Data/converted_data/julian_format/stage_10.1.11-1.1.19.csv\"\n",
    "turb_raw_data = (\n",
    "    \"../Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv\"\n",
    ")\n",
    "\n",
    "turb_labeled = \"../Data/labeled_data/ground_truths/turb/turb_all_julian_0k-300k.csv\"\n",
    "\n",
    "fdom_raw_augmented = \"../Data/augmented_data/turb/unlabeled/unlabeled_fdom.csv\"\n",
    "turb_labeled_augmented = \"../Data/augmented_data/turb/labeled/labeled_turb_peaks.csv\"\n",
    "\n",
    "turb_augmented_raw_data = \"../Data/augmented_data/turb/unlabeled/unlabeled_turb.csv\"\n",
    "\n",
    "stage_augmented_data_fn = \"../Data/augmented_data/turb/unlabeled/unlabeled_stage.csv\"\n",
    "\n",
    "turb_fpt_lookup_path = \"../Data/augmented_data/turb/fpt_lookup.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"NAP\", \"FPT\", \"PP\", \"SKP\"]\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "targets = le.fit_transform(classes)\n",
    "\n",
    "train_dataset = turbAugOnlyDataset(\n",
    "    le,\n",
    "    fdom_raw_augmented,\n",
    "    stage_augmented_data_fn,\n",
    "    turb_augmented_raw_data,\n",
    "    turb_labeled_augmented,\n",
    "    turb_fpt_lookup_path,\n",
    "    WINDOW_SIZE,\n",
    ")\n",
    "\n",
    "test_dataset = turbidityDataset(\n",
    "    le,\n",
    "    fdom_raw_data,\n",
    "    stage_raw_data,\n",
    "    turb_raw_data,\n",
    "    turb_labeled,\n",
    "    window_size=WINDOW_SIZE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_pad(batch):\n",
    "    \"\"\"\n",
    "    Pads batch of variable length\n",
    "    \"\"\"\n",
    "\n",
    "    label_list, sample_list, lengths = [], [], []\n",
    "\n",
    "    for (sample, label) in batch:\n",
    "        label_list.append(label)\n",
    "        # convert sample to tensor\n",
    "        sample = torch.tensor(\n",
    "            sample, dtype=torch.float64\n",
    "        ).T  # tranpose to send in data, pad_sequences won't accept original\n",
    "\n",
    "        # append to lengths\n",
    "        lengths.append(sample.shape[0])\n",
    "\n",
    "        sample_list.append(sample)\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "\n",
    "    sample_list = torch.nn.utils.rnn.pad_sequence(\n",
    "        sample_list, batch_first=True, padding_value=0\n",
    "    )\n",
    "\n",
    "    # re-tranpose list, so we go back to a 4 channel dataset\n",
    "    sample_list = sample_list.transpose(1, 2)\n",
    "\n",
    "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
    "\n",
    "    return [sample_list.to(device), label_list.to(device), lengths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training / testing\n",
    "# train_size = int(0.85 * len(dataset))\n",
    "# test_size = len(dataset) - train_size\n",
    "# train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# create dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn_pad\n",
    ")\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn_pad\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "model = ResNet1D(\n",
    "    in_channels=4,\n",
    "    base_filters=64,\n",
    "    kernel_size=16,\n",
    "    stride=2,\n",
    "    n_block=48,\n",
    "    groups=1,  # check this\n",
    "    n_classes=len(classes),\n",
    "    downsample_gap=6,\n",
    "    increasefilter_gap=12,\n",
    "    verbose=False,\n",
    ").to(device)\n",
    "\n",
    "model = model.float()\n",
    "\n",
    "# print a model summary\n",
    "print(\n",
    "    summary(model, (4, 716))\n",
    ")  # 4 channels, of length (max batch length, set at 716 here, this may be incorrect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer/criterion\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "all_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog_bar = tqdm(trainloader, desc=\"Training\", leave=False)\n",
    "for i, batch in enumerate(prog_bar):\n",
    "    x = batch[0].to(device)\n",
    "\n",
    "    # squeeze y to flatten predictions into 1d tensor\n",
    "    y = batch[1].squeeze().to(device)\n",
    "\n",
    "    pred = model(x.float())\n",
    "\n",
    "    loss = criterion(pred, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    all_loss.append(loss.item())\n",
    "\n",
    "# save model\n",
    "torch.save(model, \"./results/models/turb/raw/may-fifth.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "model = torch.load(\"./results/models/turb/raw/may-two.pt\")\n",
    "model.eval()\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "prog_bar = tqdm(testloader, desc=\"Testing\", leave=False)\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(prog_bar):\n",
    "        x = batch[0].to(device)\n",
    "\n",
    "        y = batch[1].squeeze().to(device)\n",
    "\n",
    "        outs = model(x.float())\n",
    "\n",
    "        _, preds = torch.max(outs, 1)\n",
    "\n",
    "        for label, prediction in zip(y, preds):\n",
    "            # convert label and prediction to current vals\n",
    "            label = le.inverse_transform([label])[0]\n",
    "            prediction = le.inverse_transform([prediction])[0]\n",
    "\n",
    "            y_pred.append(prediction)\n",
    "            y_true.append(label)\n",
    "\n",
    "\n",
    "# build conf matrix\n",
    "conf = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "\n",
    "# review the classnames here\n",
    "df_cm = pd.DataFrame(\n",
    "    conf / conf.sum(axis=1)[:, np.newaxis],\n",
    "    index=[i for i in classes],\n",
    "    columns=[i for i in classes],\n",
    ")\n",
    "\n",
    "# classification report\n",
    "acc_report = classification_report(y_true, y_pred)\n",
    "print(acc_report)\n",
    "\n",
    "bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "print(f\"Balanced accuracy: {bal_acc}\")\n",
    "\n",
    "# display conf matrix\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "plt.xlabel(\"Ground Truths\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(label=\"fDOM Peak Detection Ratio Confusion Matrix\")\n",
    "\n",
    "plot = sn.heatmap(df_cm, annot=True)\n",
    "plot.get_figure().savefig(\"./results/graphics/turb/raw/may-2.png\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ed753961bdc37ee89b4275051722ceb8ec0b57b8793db9d189305c313070a7d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('srrw')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
