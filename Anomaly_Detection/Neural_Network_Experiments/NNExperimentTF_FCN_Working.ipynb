{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NNExperimentTF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkclpho3HRol"
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time \n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split \n",
        "import scipy.io as sio\n",
        "import copy\n",
        "import pickle\n",
        "import numpy as np\n",
        "from scipy.signal import find_peaks\n",
        "from os.path import dirname, join as pjoin\n",
        "import datetime\n",
        "import csv\n",
        "import math\n",
        "import sys\n",
        "\n",
        "output_directory = \"./turb_PP_fcn\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyaL6qQhI9qd"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oh0smbZHkUj"
      },
      "source": [
        "# Import Data \n",
        "with open('/content/drive/MyDrive/NSF_Research/NN/Data/fDOM_data', 'rb') as f: fDOM_data = pickle.load(f)\n",
        "with open('/content/drive/MyDrive/NSF_Research/NN/Data/turb_data', 'rb') as f: turb_data = pickle.load(f)\n",
        "with open('/content/drive/MyDrive/NSF_Research/NN/Data/stage_data', 'rb') as f: stage_data = pickle.load(f)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQ9efGyGfRPN",
        "outputId": "417a1766-985c-4546-c002-95d211b7c9bf"
      },
      "source": [
        "print(X.shape)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1047, 3, 61)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUdyWiC1LYFZ"
      },
      "source": [
        "X,y = create_turbPP_dataset(fDOM_data, stage_data, turb_data, window_size = 30)\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "num_classes = len(np.unique(y, axis = 0))\n",
        "\n",
        "# transform labels from integers to one hot vectors\n",
        "encoder = OneHotEncoder(categories='auto')\n",
        "encoder.fit(y.reshape(-1,1))\n",
        "y = encoder.transform(y.reshape(-1,1)).toarray()\n",
        "y_true = np.argmax(y, axis =1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 1)\n",
        "\n",
        "fcn = FCN(output_directory, input_shape, num_classes)\n",
        "fcn.fit(X_train, y_train, X_test, y_test, y_true)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EK9umAmRmjK",
        "outputId": "69becbc8-2b73-4109-ea77-b4c40ba4db1f"
      },
      "source": [
        ""
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(837, 3, 61)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StkkDtO6MztR"
      },
      "source": [
        "class FCN:\n",
        "\n",
        "  def __init__(self, output_directory, input_shape, nb_classes):\n",
        "    self.output_directory = output_directory\n",
        "    self.model = self.build_model(input_shape, nb_classes)\n",
        "    print(self.model.summary())\n",
        "    self.model.save_weights(self.output_directory+'model_init.hdf5')\n",
        "    return\n",
        "\n",
        "  def build_model(self, input_shape, nb_classes):\n",
        "    input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "    conv1 = keras.layers.Conv1D(filters=128, kernel_size=8, padding='same')(input_layer)\n",
        "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
        "    conv1 = keras.layers.Activation(activation='relu')(conv1)\n",
        "\n",
        "    conv2 = keras.layers.Conv1D(filters=256, kernel_size=5, padding='same')(conv1)\n",
        "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
        "    conv2 = keras.layers.Activation('relu')(conv2)\n",
        "\n",
        "    conv3 = keras.layers.Conv1D(128, kernel_size=3,padding='same')(conv2)\n",
        "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
        "    conv3 = keras.layers.Activation('relu')(conv3)\n",
        "\n",
        "    gap_layer = keras.layers.GlobalAveragePooling1D()(conv3)\n",
        "\n",
        "    output_layer = keras.layers.Dense(nb_classes, activation='softmax')(gap_layer)\n",
        "\n",
        "    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    overfit_callback = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience= 5,verbose = True)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer = keras.optimizers.Adam(), metrics=['accuracy'])\n",
        "\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50, \n",
        "      min_lr=0.0001)\n",
        "\n",
        "    file_path = self.output_directory+'best_model.hdf5'\n",
        "\n",
        "    model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss', \n",
        "      save_best_only=True)\n",
        "\n",
        "    self.callbacks = [reduce_lr,model_checkpoint, overfit_callback]\n",
        "\n",
        "    return model \n",
        "\n",
        "  def fit(self, x_train, y_train, x_val, y_val,y_true):\n",
        "    \n",
        "    # x_val and y_val are only used to monitor the test loss and NOT for training  \n",
        "    batch_size = 16\n",
        "    num_epochs = 50\n",
        "\n",
        "    mini_batch_size = int(min(x_train.shape[0]/10, batch_size))\n",
        "\n",
        "    start_time = time.time() \n",
        "\n",
        "    hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=num_epochs,\n",
        "      verbose=True, validation_data=(x_val,y_val), callbacks=self.callbacks)\n",
        "    \n",
        "    duration = time.time() - start_time\n",
        "\n",
        "    self.model.save(self.output_directory+'last_model.hdf5')\n",
        "\n",
        "    model = keras.models.load_model(self.output_directory+'best_model.hdf5')\n",
        "\n",
        "    y_pred = model.predict(x_val)\n",
        "\n",
        "    # convert the predicted from binary to integer \n",
        "    y_pred = np.argmax(y_pred , axis=1)\n",
        "\n",
        "    # save_logs(self.output_directory, hist, y_pred, y_true, duration)\n",
        "\n",
        "    # keras.backend.clear_session()\n",
        "\n",
        "  def predict(self, x_test, y_true,x_train,y_train,y_test):\n",
        "    model_path = self.output_directory + 'best_model.hdf5'\n",
        "    model = keras.models.load_model(model_path)\n",
        "    y_pred = model.predict(x_test)\n",
        "    return np.argmax(y_pred, axis=1)\n",
        "\t\t"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCuyDp48Ixdq"
      },
      "source": [
        "def create_turbPP_dataset(fDOM_data, stage_data, turb_data, window_size = 15):\n",
        "    \n",
        "    # Get turb candidate peaks - these were the values used to get the set of turb candidate peaks\n",
        "    params = {'prom' : [6,None],\n",
        "                    'width': [None, None],\n",
        "                    'wlen' : 200,\n",
        "                    'dist' : 1,\n",
        "                    'rel_h': .6}\n",
        "    \n",
        "    turb_peaks, turb_props = find_peaks(turb_data[:,1], \n",
        "                                        prominence = params['prom'],\n",
        "                                        width = params['width'],\n",
        "                                        wlen = params['wlen'],\n",
        "                                        distance = params['dist'],\n",
        "                                        rel_height = params['rel_h'])\n",
        "    \n",
        "    turb_peaks, turb_props = delete_missing_data_peaks(turb_data, turb_peaks, turb_props, '/content/drive/MyDrive/NSF_Research/NN/Data/flat_plat_ranges.txt')\n",
        "\n",
        "    # Import ground truths for these peaks\n",
        "    gt_fname_t = '/content/drive/MyDrive/NSF_Research/NN/Data/turb_pp_0k-300k-2_labeled'\n",
        "    with open(gt_fname_t, 'r', newline = '') as f:\n",
        "        reader = csv.reader(f, delimiter = ',')\n",
        "        # gt entries in form: ['timestamp_of_peak', 'value_of_peak','label_of_peak','idx_of_peak']\n",
        "        next(reader)\n",
        "        ground_truth = [0 if row[2] == 'NPP' else 1 for row in reader] \n",
        "        f.close()  \n",
        "\n",
        "    # Reshape data \n",
        "    fDOM_data = fDOM_data[:,1].reshape(-1,1)\n",
        "    stage_data = stage_data[:,1].reshape(-1,1)\n",
        "    turb_data = turb_data[:,1].reshape(-1,1)\n",
        "                                         \n",
        "        \n",
        "    # Use Robust scaler to scale data\n",
        "    fDOM_scaler = RobustScaler().fit(fDOM_data)\n",
        "    fDOM_data_scaled = fDOM_scaler.transform(fDOM_data)\n",
        "    \n",
        "    turb_scaler = RobustScaler().fit(turb_data)\n",
        "    turb_data_scaled = turb_scaler.transform(turb_data)\n",
        "    \n",
        "    stage_scaler = RobustScaler().fit(stage_data)\n",
        "    stage_data_scaled = stage_scaler.transform(stage_data)\n",
        "    \n",
        "    # Created \"sequenced\" data, where middle point is the peak --> sequence is (window_size * 2) + 1\n",
        "    X = []\n",
        "    y = []\n",
        "    \n",
        "    for i,peak in enumerate(turb_peaks):\n",
        "        if peak - window_size > 0 and peak + window_size < len(fDOM_data): \n",
        "            sample = np.hstack((fDOM_data_scaled[peak - window_size:peak + window_size + 1],\n",
        "                                stage_data_scaled[peak - window_size:peak + window_size + 1],\n",
        "                                turb_data_scaled[peak - window_size:peak + window_size + 1]))\n",
        "            y.append(ground_truth[i])\n",
        "            X.append(sample)\n",
        "    \n",
        "    return X, y\n",
        "X,y = create_turbPP_dataset(fDOM_data, stage_data, turb_data, window_size = 30)\n",
        "# X = np.array(X)\n",
        "# y = np.array(y)\n",
        "# print(X.shape)\n",
        "# print(X[0])"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaCpC9qXwKDV",
        "outputId": "14c2290e-2783-48fa-c0c1-4dbc48508ed6"
      },
      "source": [
        "f = fDOM_data[0:30,1].reshape(-1,1)\n",
        "t = turb_data[0:30,1].reshape(-1,1)\n",
        "s = stage_data[0:30,1].reshape(-1,1)\n",
        "\n",
        "x = np.hstack((f,t,s))\n",
        "print(x.shape)\n",
        "print(x[0:5])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30, 3)\n",
            "[[23.41625856  0.37038     0.414     ]\n",
            " [23.35176495  0.34945476  0.414     ]\n",
            " [23.17670763  1.00651259  0.414     ]\n",
            " [23.21663052  1.01488318  0.414     ]\n",
            " [23.17056495  0.59637476  0.414     ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AUbfD7BLMzq"
      },
      "source": [
        "def datetime_to_julian(date : datetime.datetime) -> float:\n",
        "    \"\"\"\n",
        "    Convert datetime object to julian time using algorithm outlined by Fliegel and van Flandern (1968):\n",
        "    \n",
        "    date:   date to convert\n",
        "\n",
        "    return: julian time equivalent\n",
        "    \"\"\"\n",
        "    interm1 = math.floor((14-date.month)/12)\n",
        "    interm2 = date.year + 4800 - interm1\n",
        "    interm3 = date.month + 12*interm1 - 3\n",
        "\n",
        "    jdn = (date.day + math.floor((153*interm3 + 2)/5) + 365*interm2 + \n",
        "           math.floor(interm2/4) - math.floor(interm2/100) + math.floor(interm2/400) - 32045)\n",
        "\n",
        "    jd = jdn + (date.hour - 12) / 24 + date.minute / 1440 + date.second / 86400 + date.microsecond / 86400000000\n",
        "    return jd\n",
        "\n",
        "def delete_missing_data_peaks(data, peaks, props, missing_file_path):\n",
        "    \"\"\" \n",
        "    Delete peaks that occur during time periods designated as \"missing data\"\n",
        "    \n",
        "    data:              timeseries that peaks occured in\n",
        "    peaks:             indices of peaks detected\n",
        "    props:             properties associated with each peak\n",
        "    missing_file_path: file path of missing date ranges\n",
        "    return:            filtered peaks and props\n",
        "    \"\"\"\n",
        "    with open(missing_file_path,newline='') as file:\n",
        "        reader = csv.reader(file, delimiter = ',')\n",
        "        time_list = []\n",
        "        for row in reader:\n",
        "            time_list.append([datetime_to_julian(datetime.datetime.strptime(row[0],'%Y-%m-%d %H:%M:%S')),\n",
        "                              datetime_to_julian(datetime.datetime.strptime(row[1],'%Y-%m-%d %H:%M:%S'))])\n",
        "            \n",
        "        # Identify and remove violating peaks \n",
        "        keep_indices = list(np.linspace(0,peaks.shape[0]-1,peaks.shape[0]))\n",
        "        for i,idx in enumerate(peaks): \n",
        "            time = data[idx,0]\n",
        "            for row in time_list: \n",
        "                if time >= row[0] and time <= row[1]:\n",
        "                    keep_indices.remove(i)\n",
        "                    break\n",
        "        \n",
        "        peaks = np.take(peaks,keep_indices,0)  \n",
        "        \n",
        "        # Remove properties for violating peaks\n",
        "        for key in props:\n",
        "            props[key] = np.take(props[key], keep_indices,0)\n",
        "        \n",
        "        return peaks, props"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y524KhkodzqS"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 81,
      "outputs": []
    }
  ]
}