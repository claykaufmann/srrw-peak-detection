{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "turpPP_DeepLearning.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyaL6qQhI9qd"
      },
      "source": [
        "### Imports and connect to my gDrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkclpho3HRol"
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "import seaborn as sn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import time \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split \n",
        "import scipy.io as sio\n",
        "import copy\n",
        "import pickle\n",
        "import numpy as np\n",
        "from scipy.signal import find_peaks\n",
        "from os.path import dirname, join as pjoin\n",
        "import datetime\n",
        "import csv\n",
        "import math\n",
        "import sys\n",
        "\n",
        "output_directory = \"./drive/MyDrive/NSF_Research/NN/turb_PP_resNet/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oh0smbZHkUj"
      },
      "source": [
        "# Import Data \n",
        "with open('/content/drive/MyDrive/NSF_Research/NN/Data/fDOM_data', 'rb') as f: fDOM_data = pickle.load(f)\n",
        "with open('/content/drive/MyDrive/NSF_Research/NN/Data/turb_data', 'rb') as f: turb_data = pickle.load(f)\n",
        "with open('/content/drive/MyDrive/NSF_Research/NN/Data/stage_data', 'rb') as f: stage_data = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixGBCz116r2G"
      },
      "source": [
        "# K-Fold Cross Validation w/ FCN + 1 Dense Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMigEu-862ZR"
      },
      "source": [
        "# Define Parameters\n",
        "num_splits = 5\n",
        "batch_size = 32\n",
        "epochs = 200\n",
        "overfit_patience = 15\n",
        "\n",
        "accumulated_test_metrics = []\n",
        "accumulated_cfmxs = {}\n",
        "\n",
        "# Get unformatted data\n",
        "X,y = create_turbPP_dataset(fDOM_data, stage_data, turb_data, window_size = 10)\n",
        "\n",
        "# Reshape and format to make train and test set for deep learning models\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "print(X[0].shape)\n",
        "input_shape = X[0].shape\n",
        "num_classes = len(np.unique(y, axis = 0))\n",
        "encoder = OneHotEncoder(categories='auto')\n",
        "encoder.fit(y.reshape(-1,1))\n",
        "y = encoder.transform(y.reshape(-1,1)).toarray()\n",
        "y_true = np.argmax(y, axis =1)\n",
        "\n",
        "train_test_split_indices = TimeSeriesSplit(num_splits).split(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMVoSn_77Hsk"
      },
      "source": [
        "split = 1\n",
        "\n",
        "for train_val_indices, test_indices in train_test_split_indices:\n",
        "  print(\"Split: \", split)\n",
        "  X_train, y_train = np.take(X,train_val_indices, axis = 0), np.take(y,train_val_indices, axis = 0)\n",
        "\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = .10)\n",
        "  X_test, y_test = np.take(X, test_indices, axis = 0), np.take(y,test_indices, axis = 0)\n",
        "\n",
        "  model = ResNet(output_directory, input_shape, num_classes,overfit_patience)\n",
        "  model.fit(X_train,y_train,X_val,y_val,batch_size,epochs)\n",
        "  test_pred= model.predict(X_test)\n",
        "\n",
        "  cfmx = confusion_matrix(np.argmax(y_test, axis = 1),np.argmax(test_pred, axis = 1))\n",
        "  accumulated_cfmxs[split] = copy.deepcopy(cfmx)\n",
        "\n",
        "  test_ba = metrics.balanced_accuracy_score(np.argmax(y_test, axis = 1), np.argmax(test_pred, axis = 1))\n",
        "  test_f1 = metrics.f1_score(np.argmax(y_test, axis = 1), np.argmax(test_pred, axis = 1))\n",
        "  TN, FP, FN, TP = cfmx.ravel()\n",
        "  test_tpr = TP/(TP+FN)\n",
        "  test_tnr = TN/(TN+FP)\n",
        "  test_precision = TP/(TP+FP) \n",
        "\n",
        "  print(\"Test F1: \", test_f1, \" Test BA: \", test_ba)\n",
        "  accumulated_test_metrics.append({'f1' : test_f1, 'ba': test_ba, 'precision': test_precision, 'tpr': test_tpr, 'tnr': test_tnr})\n",
        "\n",
        "  split +=1 \n",
        "\n",
        "\n",
        "# Display Resuls \n",
        "mean_f1 = 0\n",
        "mean_ba = 0 \n",
        "mean_sensitivity = 0\n",
        "mean_specificity = 0 \n",
        "mean_precision = 0\n",
        "\n",
        "for split in accumulated_test_metrics:\n",
        "  \n",
        "    mean_f1+=split['f1']\n",
        "    mean_ba+=split['ba']\n",
        "    mean_sensitivity += split['tpr']\n",
        "    mean_specificity += split['tnr']\n",
        "    mean_precision += split['precision']\n",
        "\n",
        "print(\"Mean Test F1: \", mean_f1/len(accumulated_test_metrics))\n",
        "print(\"Mean Test BA: \", mean_ba/len(accumulated_test_metrics))\n",
        "print(\"Mean Test TPR: \", mean_sensitivity/len(accumulated_test_metrics))\n",
        "print(\"Mean Test TNR: \", mean_specificity/len(accumulated_test_metrics))\n",
        "print(\"Mean Test Precision: \", mean_precision/len(accumulated_test_metrics))\n",
        "\n",
        "mean_cfmx = np.zeros((2,2))\n",
        "for key in accumulated_cfmxs.keys():\n",
        "    mean_cfmx += accumulated_cfmxs[key]\n",
        "mean_cfmx = mean_cfmx / num_splits\n",
        "\n",
        "plt.figure(figsize = (10,7))\n",
        "plt.title(label = 'Turbidity Phantom Peak (ResNet)')\n",
        "\n",
        "sn.set(font_scale = 1.5)\n",
        "sn.heatmap( pd.DataFrame(mean_cfmx.astype('float') / mean_cfmx.sum(axis=1)[:, np.newaxis],index = ['Negative', 'Positive'], columns = ['Negative','Positive']), annot = True, annot_kws ={\"size\": 16})\n",
        "plt.xlabel('Ground Truths')\n",
        "plt.ylabel('Predictions')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize = (10,7))\n",
        "plt.title(label = 'Turbidity Phantom Peak (ResNet)')\n",
        "\n",
        "sn.set(font_scale = 1.5)\n",
        "sn.heatmap( pd.DataFrame(mean_cfmx,index = ['Negative', 'Positive'], columns = ['Negative','Positive']), annot = True, annot_kws ={\"size\": 16})\n",
        "plt.xlabel('Ground Truths')\n",
        "plt.ylabel('Predictions')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StkkDtO6MztR"
      },
      "source": [
        "# Fully Convolutional Network ... that I have corrupted by adding fully connected layers at the end\n",
        "class FCN:\n",
        "\n",
        "  def __init__(self, output_directory, input_shape, nb_classes, overfit_patience):\n",
        "    self.output_directory = output_directory\n",
        "    self.model = self.build_model(input_shape, nb_classes, overfit_patience)\n",
        "    print(self.model.summary())\n",
        "    self.model.save_weights(self.output_directory+'model_init.hdf5')\n",
        "    return\n",
        "\n",
        "\n",
        "  def build_model(self, input_shape, nb_classes, overfit_patience):\n",
        "    padding = 'same'\n",
        "    input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "    conv1 = keras.layers.Conv1D(filters=128,kernel_size=3,padding=padding)(input_layer)\n",
        "    conv1 = keras.layers.Activation('relu')(conv1)\n",
        "    conv1 = keras.layers.AveragePooling1D(pool_size=2)(conv1)\n",
        "\n",
        "    conv2 = keras.layers.Conv1D(filters=256,kernel_size=3,padding=padding)(conv1)\n",
        "    conv2 = keras.layers.Activation('relu')(conv2)\n",
        "    conv2 = keras.layers.AveragePooling1D(pool_size=2)(conv2)\n",
        "\n",
        "    conv3 = keras.layers.Conv1D(filters=128,kernel_size=3,padding=padding)(conv2)\n",
        "    conv3 = keras.layers.Activation('relu')(conv3)\n",
        "    conv3 = keras.layers.AveragePooling1D(pool_size=2)(conv3)\n",
        "    \n",
        "\n",
        "    flatten_layer = keras.layers.Flatten()(conv3)\n",
        "    \n",
        "    fc1 = keras.layers.Dense(units = 64, activation = 'relu')(flatten_layer)\n",
        "    fc2 = keras.layers.Dense(units = 16, activation = 'relu')(fc1)\n",
        "\n",
        "    output_layer = keras.layers.Dense(units=nb_classes,activation='softmax')(fc2)\n",
        "\n",
        "    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    overfit_callback = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience= overfit_patience,verbose = True)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer = keras.optimizers.Adam(), metrics=['accuracy'])\n",
        "\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, \n",
        "      min_lr=0.0001)\n",
        "\n",
        "    file_path = self.output_directory+'best_model.hdf5'\n",
        "\n",
        "    model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss', \n",
        "      save_best_only=True)\n",
        "\n",
        "    self.callbacks = [reduce_lr,model_checkpoint, overfit_callback]\n",
        "\n",
        "    return model \n",
        "\n",
        "  def fit(self, x_train, y_train, x_val, y_val, batch_size, epochs):\n",
        "    \n",
        "    mini_batch_size = int(min(x_train.shape[0]/10, batch_size))\n",
        "\n",
        "    hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=epochs,\n",
        "      verbose=False, validation_data=(x_val,y_val), callbacks=self.callbacks)\n",
        "    \n",
        "    self.model.save(self.output_directory+'last_model.hdf5')\n",
        "\n",
        "    keras.backend.clear_session()\n",
        "\n",
        "  def predict(self, X):\n",
        "    model_path = self.output_directory + 'best_model.hdf5'\n",
        "    model = keras.models.load_model(model_path)\n",
        "    return model.predict(X)\n",
        "    \n",
        "\t\t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzyyfAotaYru"
      },
      "source": [
        "class ResNet:\n",
        "\n",
        "  def __init__(self, output_directory, input_shape, nb_classes, overfit_patience):\n",
        "    self.output_directory = output_directory\n",
        "    self.model = self.build_model(input_shape, nb_classes,64,overfit_patience)\n",
        "    # print(self.model.summary())\n",
        "    self.model.save_weights(self.output_directory+'model_init.hdf5')\n",
        "    return\n",
        "  \n",
        "  \n",
        "\n",
        "  def build_model(self, input_shape, num_classes, num_feature_maps, overfit_patience):\n",
        "  \n",
        "    input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "    # BLOCK 1\n",
        "\n",
        "    conv_x = keras.layers.Conv1D(filters=num_feature_maps, kernel_size=8, padding='same')(input_layer)\n",
        "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "    conv_y = keras.layers.Conv1D(filters=num_feature_maps, kernel_size=5, padding='same')(conv_x)\n",
        "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "    conv_z = keras.layers.Conv1D(filters=num_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
        "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "    # expand channels for the sum\n",
        "    shortcut_y = keras.layers.Conv1D(filters=num_feature_maps, kernel_size=1, padding='same')(input_layer)\n",
        "    shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "    output_block_1 = keras.layers.add([shortcut_y, conv_z])\n",
        "    output_block_1 = keras.layers.Activation('relu')(output_block_1)\n",
        "\n",
        "    # BLOCK 2\n",
        "\n",
        "    conv_x = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=8, padding='same')(output_block_1)\n",
        "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "    conv_y = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
        "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "    conv_z = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
        "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "    # expand channels for the sum\n",
        "    shortcut_y = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n",
        "    shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "    output_block_2 = keras.layers.add([shortcut_y, conv_z])\n",
        "    output_block_2 = keras.layers.Activation('relu')(output_block_2)\n",
        "\n",
        "    # # BLOCK 3\n",
        "\n",
        "    # conv_x = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n",
        "    # conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "    # conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "    # conv_y = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
        "    # conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "    # conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "    # conv_z = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
        "    # conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "    # # no need to expand channels because they are equal\n",
        "    # shortcut_y = keras.layers.BatchNormalization()(output_block_2)\n",
        "\n",
        "    # output_block_3 = keras.layers.add([shortcut_y, conv_z])\n",
        "    # output_block_3 = keras.layers.Activation('relu')(output_block_3)\n",
        "\n",
        "    # FINAL\n",
        "\n",
        "    flatten_layer = keras.layers.Flatten()(output_block_2)\n",
        "    \n",
        "    fc1 = keras.layers.Dense(units = 64, activation = 'relu')(flatten_layer)\n",
        "    fc2 = keras.layers.Dense(units = 16, activation = 'relu')(fc1)\n",
        "\n",
        "    output_layer = keras.layers.Dense(units=num_classes,activation='softmax')(fc2)\n",
        "\n",
        "    # output_layer = keras.layers.Dense(num_classes, activation='softmax')(gap_layer)\n",
        "\n",
        "    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=overfit_patience, min_lr=0.0001)\n",
        "\n",
        "    overfit_callback = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience= overfit_patience,verbose = True)\n",
        "\n",
        "    file_path = self.output_directory + 'best_model.hdf5'\n",
        "\n",
        "    model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss',\n",
        "                                                        save_best_only=True)\n",
        "\n",
        "    self.callbacks = [reduce_lr, model_checkpoint,overfit_callback]\n",
        "\n",
        "    return model\n",
        "\n",
        "  def fit(self, x_train, y_train, x_val, y_val, batch_size, epochs):\n",
        "    \n",
        "    mini_batch_size = int(min(x_train.shape[0]/10, batch_size))\n",
        "\n",
        "    hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=epochs,\n",
        "      verbose=False, validation_data=(x_val,y_val), callbacks=self.callbacks)\n",
        "    \n",
        "    self.model.save(self.output_directory+'last_model.hdf5')\n",
        "\n",
        "\n",
        "\n",
        "    # convert the predicted from binary to integer \n",
        "    \n",
        "\n",
        "    # save_logs(self.output_directory, hist, y_pred, y_true, duration)\n",
        "\n",
        "    keras.backend.clear_session()\n",
        "\n",
        "  def predict(self, X):\n",
        "    model_path = self.output_directory + 'best_model.hdf5'\n",
        "    model = keras.models.load_model(model_path)\n",
        "    return model.predict(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCuyDp48Ixdq"
      },
      "source": [
        "def create_turbPP_dataset(fDOM_data, stage_data, turb_data, window_size = 15):\n",
        "    \n",
        "    # Get turb candidate peaks - these were the values used to get the set of turb candidate peaks\n",
        "    params = {'prom' : [6,None],\n",
        "                    'width': [None, None],\n",
        "                    'wlen' : 200,\n",
        "                    'dist' : 1,\n",
        "                    'rel_h': .6}\n",
        "    \n",
        "    turb_peaks, turb_props = find_peaks(turb_data[:,1], \n",
        "                                        prominence = params['prom'],\n",
        "                                        width = params['width'],\n",
        "                                        wlen = params['wlen'],\n",
        "                                        distance = params['dist'],\n",
        "                                        rel_height = params['rel_h'])\n",
        "    \n",
        "    turb_peaks, turb_props = delete_missing_data_peaks(turb_data, turb_peaks, turb_props, '/content/drive/MyDrive/NSF_Research/NN/Data/flat_plat_ranges.txt')\n",
        "\n",
        "    # Import ground truths for these peaks\n",
        "    gt_fname_t = '/content/drive/MyDrive/NSF_Research/NN/Data/turb_pp_0k-300k-2_labeled'\n",
        "    with open(gt_fname_t, 'r', newline = '') as f:\n",
        "        reader = csv.reader(f, delimiter = ',')\n",
        "        # gt entries in form: ['timestamp_of_peak', 'value_of_peak','label_of_peak','idx_of_peak']\n",
        "        next(reader)\n",
        "        ground_truth = [0 if row[2] == 'NPP' else 1 for row in reader] \n",
        "        f.close()  \n",
        "\n",
        "    # Reshape data \n",
        "    fDOM_data = fDOM_data[:,1].reshape(-1,1)\n",
        "    stage_data = stage_data[:,1].reshape(-1,1)\n",
        "    turb_data = turb_data[:,1].reshape(-1,1)\n",
        "                                         \n",
        "        \n",
        "    # Use Robust scaler to scale data\n",
        "    fDOM_scaler = RobustScaler().fit(fDOM_data)\n",
        "    fDOM_data_scaled = fDOM_scaler.transform(fDOM_data)\n",
        "    \n",
        "    turb_scaler = RobustScaler().fit(turb_data)\n",
        "    turb_data_scaled = turb_scaler.transform(turb_data)\n",
        "    \n",
        "    stage_scaler = RobustScaler().fit(stage_data)\n",
        "    stage_data_scaled = stage_scaler.transform(stage_data)\n",
        "    \n",
        "    # Created \"sequenced\" data, where middle point is the peak --> sequence is (window_size * 2) + 1\n",
        "    X = []\n",
        "    y = []\n",
        "    \n",
        "    for i,peak in enumerate(turb_peaks):\n",
        "        if peak - window_size > 0 and peak + window_size < len(fDOM_data): \n",
        "            sample = np.hstack((fDOM_data_scaled[peak - window_size:peak + window_size + 1],\n",
        "                                stage_data_scaled[peak - window_size:peak + window_size + 1],\n",
        "                                turb_data_scaled[peak - window_size:peak + window_size + 1])).T\n",
        "            y.append(ground_truth[i])\n",
        "            X.append(sample.T)\n",
        "    \n",
        "    return X, y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AUbfD7BLMzq"
      },
      "source": [
        "def datetime_to_julian(date : datetime.datetime) -> float:\n",
        "    \"\"\"\n",
        "    Convert datetime object to julian time using algorithm outlined by Fliegel and van Flandern (1968):\n",
        "    \n",
        "    date:   date to convert\n",
        "\n",
        "    return: julian time equivalent\n",
        "    \"\"\"\n",
        "    interm1 = math.floor((14-date.month)/12)\n",
        "    interm2 = date.year + 4800 - interm1\n",
        "    interm3 = date.month + 12*interm1 - 3\n",
        "\n",
        "    jdn = (date.day + math.floor((153*interm3 + 2)/5) + 365*interm2 + \n",
        "           math.floor(interm2/4) - math.floor(interm2/100) + math.floor(interm2/400) - 32045)\n",
        "\n",
        "    jd = jdn + (date.hour - 12) / 24 + date.minute / 1440 + date.second / 86400 + date.microsecond / 86400000000\n",
        "    return jd\n",
        "\n",
        "def delete_missing_data_peaks(data, peaks, props, missing_file_path):\n",
        "    \"\"\" \n",
        "    Delete peaks that occur during time periods designated as \"missing data\"\n",
        "    \n",
        "    data:              timeseries that peaks occured in\n",
        "    peaks:             indices of peaks detected\n",
        "    props:             properties associated with each peak\n",
        "    missing_file_path: file path of missing date ranges\n",
        "    return:            filtered peaks and props\n",
        "    \"\"\"\n",
        "    with open(missing_file_path,newline='') as file:\n",
        "        reader = csv.reader(file, delimiter = ',')\n",
        "        time_list = []\n",
        "        for row in reader:\n",
        "            time_list.append([datetime_to_julian(datetime.datetime.strptime(row[0],'%Y-%m-%d %H:%M:%S')),\n",
        "                              datetime_to_julian(datetime.datetime.strptime(row[1],'%Y-%m-%d %H:%M:%S'))])\n",
        "            \n",
        "        # Identify and remove violating peaks \n",
        "        keep_indices = list(np.linspace(0,peaks.shape[0]-1,peaks.shape[0]))\n",
        "        for i,idx in enumerate(peaks): \n",
        "            time = data[idx,0]\n",
        "            for row in time_list: \n",
        "                if time >= row[0] and time <= row[1]:\n",
        "                    keep_indices.remove(i)\n",
        "                    break\n",
        "        \n",
        "        peaks = np.take(peaks,keep_indices,0)  \n",
        "        \n",
        "        # Remove properties for violating peaks\n",
        "        for key in props:\n",
        "            props[key] = np.take(props[key], keep_indices,0)\n",
        "        \n",
        "        return peaks, props"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}