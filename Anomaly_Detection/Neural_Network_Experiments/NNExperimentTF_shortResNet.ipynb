{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkclpho3HRol"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split \n",
    "import scipy.io as sio\n",
    "import copy\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from os.path import dirname, join as pjoin\n",
    "import datetime\n",
    "import csv\n",
    "import math\n",
    "import sys\n",
    "\n",
    "output_directory = \"./drive/MyDrive/NSF_Research/NN/turb_PP_resNet/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyaL6qQhI9qd"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7oh0smbZHkUj"
   },
   "outputs": [],
   "source": [
    "# Import Data \n",
    "with open('/content/drive/MyDrive/NSF_Research/NN/Data/fDOM_data', 'rb') as f: fDOM_data = pickle.load(f)\n",
    "with open('/content/drive/MyDrive/NSF_Research/NN/Data/turb_data', 'rb') as f: turb_data = pickle.load(f)\n",
    "with open('/content/drive/MyDrive/NSF_Research/NN/Data/stage_data', 'rb') as f: stage_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUdyWiC1LYFZ"
   },
   "outputs": [],
   "source": [
    "X,y = create_turbPP_dataset(fDOM_data, stage_data, turb_data, window_size = 30)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "num_classes = len(np.unique(y, axis = 0))\n",
    "\n",
    "# transform labels from integers to one hot vectors\n",
    "encoder = OneHotEncoder(categories='auto')\n",
    "encoder.fit(y.reshape(-1,1))\n",
    "y = encoder.transform(y.reshape(-1,1)).toarray()\n",
    "y_true = np.argmax(y, axis =1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 1)\n",
    "input_shape = X_train[0].shape\n",
    "fcn = FCN(output_directory, input_shape, num_classes)\n",
    "fcn.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixGBCz116r2G"
   },
   "source": [
    "# K-Fold Cross Validation w/ FCN + 1 Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mMigEu-862ZR"
   },
   "outputs": [],
   "source": [
    "# Define Parameters\n",
    "num_splits = 5\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "overfit_patience = 100\n",
    "\n",
    "test_results = []\n",
    "\n",
    "X,y = create_turbPP_dataset(fDOM_data, stage_data, turb_data, window_size = 10)\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(X[0].shape)\n",
    "input_shape = X[0].shape\n",
    "num_classes = len(np.unique(y, axis = 0))\n",
    "encoder = OneHotEncoder(categories='auto')\n",
    "encoder.fit(y.reshape(-1,1))\n",
    "y = encoder.transform(y.reshape(-1,1)).toarray()\n",
    "y_true = np.argmax(y, axis =1)\n",
    "\n",
    "train_test_split_indices = TimeSeriesSplit(num_splits).split(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMVoSn_77Hsk"
   },
   "outputs": [],
   "source": [
    "split = 1\n",
    "\n",
    "for train_val_indices, test_indices in train_test_split_indices:\n",
    "  print(\"Split: \", split)\n",
    "  X_train, y_train = np.take(X,train_val_indices, axis = 0), np.take(y,train_val_indices, axis = 0)\n",
    "\n",
    "  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = .10)\n",
    "  X_test, y_test = np.take(X, test_indices, axis = 0), np.take(y,test_indices, axis = 0)\n",
    "\n",
    "  model = ResNet(output_directory, input_shape, num_classes,overfit_patience)\n",
    "  model.fit(X_train,y_train,X_val,y_val,batch_size,epochs)\n",
    "  test_pred= model.predict(X_test)\n",
    "\n",
    "  test_ba = metrics.balanced_accuracy_score(np.argmax(y_test, axis = 1), np.argmax(test_pred, axis = 1))\n",
    "  test_f1 = metrics.f1_score(np.argmax(y_test, axis = 1), np.argmax(test_pred, axis = 1))\n",
    "  print(\"Test F1: \", test_f1, \" Test BA: \", test_ba)\n",
    "  test_results.append(np.array([test_f1, test_ba]))\n",
    "\n",
    "  split +=1 \n",
    "\n",
    "print(\"Mean Test F1: \", np.mean(np.array(test_results)[:,0]), \" Mean Test BA: \", np.mean(np.array(test_results)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StkkDtO6MztR"
   },
   "outputs": [],
   "source": [
    "class FCN:\n",
    "\n",
    "  def __init__(self, output_directory, input_shape, nb_classes, overfit_patience):\n",
    "    self.output_directory = output_directory\n",
    "    self.model = self.build_model(input_shape, nb_classes, overfit_patience)\n",
    "    # print(self.model.summary())\n",
    "    self.model.save_weights(self.output_directory+'model_init.hdf5')\n",
    "    return\n",
    "\n",
    "  # def build_model(self, input_shape, nb_classes, overfit_patience):\n",
    "  #   input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "  #   conv1 = keras.layers.Conv1D(filters=128, kernel_size=8, padding='same')(input_layer)\n",
    "  #   conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "  #   conv1 = keras.layers.Activation(activation='relu')(conv1)\n",
    "\n",
    "  #   conv2 = keras.layers.Conv1D(filters=256, kernel_size=5, padding='same')(conv1)\n",
    "  #   conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "  #   conv2 = keras.layers.Activation('relu')(conv2)\n",
    "\n",
    "  #   conv3 = keras.layers.Conv1D(128, kernel_size=3,padding='same')(conv2)\n",
    "  #   conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "  #   conv3 = keras.layers.Activation('relu')(conv3)\n",
    "\n",
    "  #   gap_layer = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "\n",
    "  #   fc1 = keras.layers.Dense(128)(gap_layer)\n",
    "  #   fc2 = keras.layers.Dense(64)(fc1)\n",
    "\n",
    "  #   output_layer = keras.layers.Dense(nb_classes, activation='softmax')(fc2)\n",
    "\n",
    "  #   model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "  #   overfit_callback = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience= overfit_patience,verbose = True)\n",
    "\n",
    "  #   model.compile(loss='categorical_crossentropy', optimizer = keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "\n",
    "  #   reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, \n",
    "  #     min_lr=0.0001)\n",
    "\n",
    "  #   file_path = self.output_directory+'best_model.hdf5'\n",
    "\n",
    "  #   model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss', \n",
    "  #     save_best_only=True)\n",
    "\n",
    "  #   self.callbacks = [reduce_lr,model_checkpoint, overfit_callback]\n",
    "\n",
    "  #   return model \n",
    "\n",
    "  def build_model(self, input_shape, nb_classes, overfit_patience):\n",
    "    padding = 'same'\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=128,kernel_size=5,padding=padding)(input_layer)\n",
    "    conv1 = keras.layers.Activation('relu')(conv1)\n",
    "    conv1 = keras.layers.AveragePooling1D(pool_size=2)(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=256,kernel_size=3,padding=padding)(conv1)\n",
    "    conv2 = keras.layers.Activation('relu')(conv2)\n",
    "    conv2 = keras.layers.AveragePooling1D(pool_size=2)(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(filters=128,kernel_size=3,padding=padding)(conv2)\n",
    "    conv3 = keras.layers.Activation('relu')(conv3)\n",
    "    conv3 = keras.layers.AveragePooling1D(pool_size=2)(conv3)\n",
    "    \n",
    "\n",
    "    flatten_layer = keras.layers.Flatten()(conv3)\n",
    "    \n",
    "    fc1 = keras.layers.Dense(units = 64, activation = 'relu')(flatten_layer)\n",
    "    fc2 = keras.layers.Dense(units = 16, activation = 'relu')(fc1)\n",
    "\n",
    "    output_layer = keras.layers.Dense(units=nb_classes,activation='softmax')(fc2)\n",
    "\n",
    "    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    overfit_callback = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience= overfit_patience,verbose = True)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, \n",
    "      min_lr=0.0001)\n",
    "\n",
    "    file_path = self.output_directory+'best_model.hdf5'\n",
    "\n",
    "    model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss', \n",
    "      save_best_only=True)\n",
    "\n",
    "    self.callbacks = [reduce_lr,model_checkpoint, overfit_callback]\n",
    "\n",
    "    return model \n",
    "\n",
    "  def fit(self, x_train, y_train, x_val, y_val, batch_size, epochs):\n",
    "    \n",
    "    mini_batch_size = int(min(x_train.shape[0]/10, batch_size))\n",
    "\n",
    "    # start_time = time.time() \n",
    "\n",
    "    hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=epochs,\n",
    "      verbose=False, validation_data=(x_val,y_val), callbacks=self.callbacks)\n",
    "    \n",
    "    # duration = time.time() - start_time\n",
    "\n",
    "    self.model.save(self.output_directory+'last_model.hdf5')\n",
    "\n",
    "    # model = keras.models.load_model(self.output_directory+'best_model.hdf5')\n",
    "\n",
    "    # y_pred = model.predict(x_val)\n",
    "\n",
    "    # convert the predicted from binary to integer \n",
    "    # y_pred = np.argmax(y_pred , axis=1)\n",
    "\n",
    "    # save_logs(self.output_directory, hist, y_pred, y_true, duration)\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "  def predict(self, X):\n",
    "    model_path = self.output_directory + 'best_model.hdf5'\n",
    "    model = keras.models.load_model(model_path)\n",
    "    return model.predict(X)\n",
    "    \n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IcgbBt1-WTk1"
   },
   "outputs": [],
   "source": [
    "class ResNet_Attention:\n",
    "\n",
    "  def __init__(self, output_directory, input_shape, nb_classes, overfit_patience):\n",
    "    self.output_directory = output_directory\n",
    "    self.model = self.build_model(input_shape, nb_classes, overfit_patience)\n",
    "    print(self.model.summary())\n",
    "    self.model.save_weights(self.output_directory+'model_init.hdf5')\n",
    "    return\n",
    "\n",
    "  def build_model(self, input_shape, nb_classes, num_feature_maps, overfit_patience):\n",
    "\n",
    "    num_feature_maps = 64\n",
    "\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    # BLOCK 1\n",
    "\n",
    "    conv_x = keras.layers.Conv1D(filters=num_feature_maps, kernel_size=8, padding='same')(input_layer)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "\n",
    "    conv_y = keras.layers.Conv1D(filters=num_feature_maps, kernel_size=5, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "\n",
    "    conv_z = keras.layers.Conv1D(filters=num_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "    # expand channels for the sum\n",
    "    shortcut_y = keras.layers.Conv1D(filters=num_feature_maps, kernel_size=1, padding='same')(input_layer)\n",
    "    shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "\n",
    "    output_block_1 = keras.layers.add([shortcut_y, conv_z])\n",
    "    output_block_1 = keras.layers.Activation('relu')(output_block_1)\n",
    "\n",
    "    # BLOCK 2\n",
    "\n",
    "    conv_x = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=8, padding='same')(output_block_1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "\n",
    "    conv_y = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "\n",
    "    conv_z = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "    # expand channels for the sum\n",
    "    shortcut_y = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n",
    "    shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "\n",
    "    output_block_2 = keras.layers.add([shortcut_y, conv_z])\n",
    "    output_block_2 = keras.layers.Activation('relu')(output_block_2)\n",
    "\n",
    "    # BLOCK 3\n",
    "\n",
    "    conv_x = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "\n",
    "    conv_y = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "\n",
    "    conv_z = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "    # no need to expand channels because they are equal\n",
    "    shortcut_y = keras.layers.BatchNormalization()(output_block_2)\n",
    "\n",
    "    output_block_3 = keras.layers.add([shortcut_y, conv_z])\n",
    "    output_block_3 = keras.layers.Activation('relu')(output_block_3)\n",
    "    output_block_3 = keras.layers.Dropout(0.4)(output_block_3)\n",
    "\n",
    "    den = keras.layers.Bidirectional(keras.layers.LSTM(num_feature_maps, stateful= True, return_sequences= True))(output_block_3)\n",
    "    den = keras.layers.Dropout(0.1)(den)\n",
    "    den = keras.layers.BatchNormalization()(den)\n",
    "\n",
    "    inter = keras.layers.Dense(100, activation= 'relu')(den)\n",
    "    den = keras.layers.TimeDistributed(keras.layers.Dense(128, activation='relu'))(inter)\n",
    "    gap = keras.layers.GlobalAveragePooling1D()(den)\n",
    "    inter = keras.layers.Dense(100, activation='relu')(gap)\n",
    "\n",
    "    output_layer = keras.layers.Dense(num_classes, activation='softmax')(inter)\n",
    "    concat = keras.layers.Concatenate(axis =-1)([tf.expand_dims(output_layer,1), input_layer])\n",
    "    model = keras.models.Model(inputs = [input_layer], outputs = [concat, output_layer])\n",
    "    print(model.summary())\n",
    "\n",
    "    return model\n",
    "    \n",
    "  def time_loss():\n",
    "    @tf.function\n",
    "    def loss(y_true, y_pred):\n",
    "        pred = y_pred[:, :, 0]\n",
    "        dat = y_pred[:, :, 1:]\n",
    "\n",
    "        arr = tf.TensorArray(tf.float32, size=y_pred.shape[0])\n",
    "        for i in range(pred.shape[0]):\n",
    "            x = pred[i]\n",
    "            x_dat = dat[i]\n",
    "            res = K.min(tf.where(x_dat != 0))\n",
    "            if res.shape != (1,1):\n",
    "                if y_true[i] == 1 and x > THRESHOLD_up:\n",
    "                    arr = arr.write(i, K.cast(res, tf.float32))\n",
    "                elif y_true[i] == 0 and x < THRESHOLD_down:\n",
    "                    arr = arr.write(i, K.cast(res, tf.float32))\n",
    "            else:\n",
    "                arr = arr.write(i, 1.043)\n",
    "        return tf.reduce_sum(arr.stack())\n",
    "    return \n",
    "\n",
    "  def fit(self, X_train, y_train, x_val, y_val, batch_size, epochs):\n",
    "    \n",
    "  def predict(self, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bzyyfAotaYru"
   },
   "outputs": [],
   "source": [
    "class ResNet:\n",
    "\n",
    "  def __init__(self, output_directory, input_shape, nb_classes, overfit_patience):\n",
    "    self.output_directory = output_directory\n",
    "    self.model = self.build_model(input_shape, nb_classes,64,overfit_patience)\n",
    "    # print(self.model.summary())\n",
    "    self.model.save_weights(self.output_directory+'model_init.hdf5')\n",
    "    return\n",
    "  \n",
    "  # def build_model(self, input_shape, num_classes, num_feature_maps, overfit_patience):\n",
    "  \n",
    "  #   input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "  #   # BLOCK 1\n",
    "\n",
    "  #   conv_x = keras.layers.Conv1D(filters=num_feature_maps, kernel_size=8, padding='same')(input_layer)\n",
    "  #   conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "  #   conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "\n",
    "  #   conv_y = keras.layers.Conv1D(filters=num_feature_maps, kernel_size=5, padding='same')(conv_x)\n",
    "  #   conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "  #   conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "\n",
    "  #   conv_z = keras.layers.Conv1D(filters=num_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
    "  #   conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "  #   # expand channels for the sum\n",
    "  #   shortcut_y = keras.layers.Conv1D(filters=num_feature_maps, kernel_size=1, padding='same')(input_layer)\n",
    "  #   shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "\n",
    "  #   output_block_1 = keras.layers.add([shortcut_y, conv_z])\n",
    "  #   output_block_1 = keras.layers.Activation('relu')(output_block_1)\n",
    "\n",
    "  #   # BLOCK 2\n",
    "\n",
    "  #   conv_x = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=8, padding='same')(output_block_1)\n",
    "  #   conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "  #   conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "\n",
    "  #   conv_y = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
    "  #   conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "  #   conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "\n",
    "  #   conv_z = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
    "  #   conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "  #   # expand channels for the sum\n",
    "  #   shortcut_y = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n",
    "  #   shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "\n",
    "  #   output_block_2 = keras.layers.add([shortcut_y, conv_z])\n",
    "  #   output_block_2 = keras.layers.Activation('relu')(output_block_2)\n",
    "\n",
    "  #   # BLOCK 3\n",
    "\n",
    "  #   conv_x = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n",
    "  #   conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "  #   conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "\n",
    "  #   conv_y = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
    "  #   conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "  #   conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "\n",
    "  #   conv_z = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
    "  #   conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "  #   # no need to expand channels because they are equal\n",
    "  #   shortcut_y = keras.layers.BatchNormalization()(output_block_2)\n",
    "\n",
    "  #   output_block_3 = keras.layers.add([shortcut_y, conv_z])\n",
    "  #   output_block_3 = keras.layers.Activation('relu')(output_block_3)\n",
    "\n",
    "  #   # FINAL\n",
    "\n",
    "  #   gap_layer = keras.layers.GlobalAveragePooling1D()(output_block_3)\n",
    "\n",
    "  #   output_layer = keras.layers.Dense(num_classes, activation='softmax')(gap_layer)\n",
    "\n",
    "  #   model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "  #   model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(),\n",
    "  #                 metrics=['accuracy'])\n",
    "\n",
    "  #   reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=overfit_patience, min_lr=0.0001)\n",
    "\n",
    "  #   overfit_callback = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience= overfit_patience,verbose = True)\n",
    "\n",
    "  #   file_path = self.output_directory + 'best_model.hdf5'\n",
    "\n",
    "  #   model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss',\n",
    "  #                                                       save_best_only=True)\n",
    "\n",
    "  #   self.callbacks = [reduce_lr, model_checkpoint,overfit_callback]\n",
    "\n",
    "  #   return model\n",
    "\n",
    "  def build_model(self, input_shape, num_classes, num_feature_maps, overfit_patience):\n",
    "  \n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    # BLOCK 1\n",
    "\n",
    "    conv_x = keras.layers.Conv1D(filters=num_feature_maps, kernel_size=8, padding='same')(input_layer)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "\n",
    "    conv_y = keras.layers.Conv1D(filters=num_feature_maps, kernel_size=5, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "\n",
    "    conv_z = keras.layers.Conv1D(filters=num_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "    # expand channels for the sum\n",
    "    shortcut_y = keras.layers.Conv1D(filters=num_feature_maps, kernel_size=1, padding='same')(input_layer)\n",
    "    shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "\n",
    "    output_block_1 = keras.layers.add([shortcut_y, conv_z])\n",
    "    output_block_1 = keras.layers.Activation('relu')(output_block_1)\n",
    "\n",
    "    # BLOCK 2\n",
    "\n",
    "    # conv_x = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=8, padding='same')(output_block_1)\n",
    "    # conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    # conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "\n",
    "    # conv_y = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
    "    # conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    # conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "\n",
    "    # conv_z = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
    "    # conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "    # # expand channels for the sum\n",
    "    # shortcut_y = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n",
    "    # shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "\n",
    "    # output_block_2 = keras.layers.add([shortcut_y, conv_z])\n",
    "    # output_block_2 = keras.layers.Activation('relu')(output_block_2)\n",
    "\n",
    "    # # BLOCK 3\n",
    "\n",
    "    # conv_x = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n",
    "    # conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    # conv_x = keras.layers.Activation('relu')(conv_x)\n",
    "\n",
    "    # conv_y = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
    "    # conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    # conv_y = keras.layers.Activation('relu')(conv_y)\n",
    "\n",
    "    # conv_z = keras.layers.Conv1D(filters=num_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
    "    # conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "    # # no need to expand channels because they are equal\n",
    "    # shortcut_y = keras.layers.BatchNormalization()(output_block_2)\n",
    "\n",
    "    # output_block_3 = keras.layers.add([shortcut_y, conv_z])\n",
    "    # output_block_3 = keras.layers.Activation('relu')(output_block_3)\n",
    "\n",
    "    # FINAL\n",
    "\n",
    "    flatten_layer = keras.layers.Flatten()(output_block_1)\n",
    "    \n",
    "    fc1 = keras.layers.Dense(units = 64, activation = 'relu')(flatten_layer)\n",
    "    fc2 = keras.layers.Dense(units = 16, activation = 'relu')(fc1)\n",
    "\n",
    "    output_layer = keras.layers.Dense(units=num_classes,activation='softmax')(fc2)\n",
    "\n",
    "    # output_layer = keras.layers.Dense(num_classes, activation='softmax')(gap_layer)\n",
    "\n",
    "    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=overfit_patience, min_lr=0.0001)\n",
    "\n",
    "    overfit_callback = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience= overfit_patience,verbose = True)\n",
    "\n",
    "    file_path = self.output_directory + 'best_model.hdf5'\n",
    "\n",
    "    model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss',\n",
    "                                                        save_best_only=True)\n",
    "\n",
    "    self.callbacks = [reduce_lr, model_checkpoint,overfit_callback]\n",
    "\n",
    "    return model\n",
    "\n",
    "  def fit(self, x_train, y_train, x_val, y_val, batch_size, epochs):\n",
    "    \n",
    "    mini_batch_size = int(min(x_train.shape[0]/10, batch_size))\n",
    "\n",
    "    hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=epochs,\n",
    "      verbose=False, validation_data=(x_val,y_val), callbacks=self.callbacks)\n",
    "    \n",
    "    self.model.save(self.output_directory+'last_model.hdf5')\n",
    "\n",
    "\n",
    "\n",
    "    # convert the predicted from binary to integer \n",
    "    \n",
    "\n",
    "    # save_logs(self.output_directory, hist, y_pred, y_true, duration)\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "  def predict(self, X):\n",
    "    model_path = self.output_directory + 'best_model.hdf5'\n",
    "    model = keras.models.load_model(model_path)\n",
    "    return model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCuyDp48Ixdq"
   },
   "outputs": [],
   "source": [
    "def create_turbPP_dataset(fDOM_data, stage_data, turb_data, window_size = 15):\n",
    "    \n",
    "    # Get turb candidate peaks - these were the values used to get the set of turb candidate peaks\n",
    "    params = {'prom' : [6,None],\n",
    "                    'width': [None, None],\n",
    "                    'wlen' : 200,\n",
    "                    'dist' : 1,\n",
    "                    'rel_h': .6}\n",
    "    \n",
    "    turb_peaks, turb_props = find_peaks(turb_data[:,1], \n",
    "                                        prominence = params['prom'],\n",
    "                                        width = params['width'],\n",
    "                                        wlen = params['wlen'],\n",
    "                                        distance = params['dist'],\n",
    "                                        rel_height = params['rel_h'])\n",
    "    \n",
    "    turb_peaks, turb_props = delete_missing_data_peaks(turb_data, turb_peaks, turb_props, '/content/drive/MyDrive/NSF_Research/NN/Data/flat_plat_ranges.txt')\n",
    "\n",
    "    # Import ground truths for these peaks\n",
    "    gt_fname_t = '/content/drive/MyDrive/NSF_Research/NN/Data/turb_pp_0k-300k-2_labeled'\n",
    "    with open(gt_fname_t, 'r', newline = '') as f:\n",
    "        reader = csv.reader(f, delimiter = ',')\n",
    "        # gt entries in form: ['timestamp_of_peak', 'value_of_peak','label_of_peak','idx_of_peak']\n",
    "        next(reader)\n",
    "        ground_truth = [0 if row[2] == 'NPP' else 1 for row in reader] \n",
    "        f.close()  \n",
    "\n",
    "    # Reshape data \n",
    "    fDOM_data = fDOM_data[:,1].reshape(-1,1)\n",
    "    stage_data = stage_data[:,1].reshape(-1,1)\n",
    "    turb_data = turb_data[:,1].reshape(-1,1)\n",
    "                                         \n",
    "        \n",
    "    # Use Robust scaler to scale data\n",
    "    fDOM_scaler = RobustScaler().fit(fDOM_data)\n",
    "    fDOM_data_scaled = fDOM_scaler.transform(fDOM_data)\n",
    "    \n",
    "    turb_scaler = RobustScaler().fit(turb_data)\n",
    "    turb_data_scaled = turb_scaler.transform(turb_data)\n",
    "    \n",
    "    stage_scaler = RobustScaler().fit(stage_data)\n",
    "    stage_data_scaled = stage_scaler.transform(stage_data)\n",
    "    \n",
    "    # Created \"sequenced\" data, where middle point is the peak --> sequence is (window_size * 2) + 1\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for i,peak in enumerate(turb_peaks):\n",
    "        if peak - window_size > 0 and peak + window_size < len(fDOM_data): \n",
    "            sample = np.hstack((fDOM_data_scaled[peak - window_size:peak + window_size + 1],\n",
    "                                stage_data_scaled[peak - window_size:peak + window_size + 1],\n",
    "                                turb_data_scaled[peak - window_size:peak + window_size + 1])).T\n",
    "            y.append(ground_truth[i])\n",
    "            X.append(sample.T)\n",
    "    \n",
    "    return X, y\n",
    "# X,y = create_turbPP_dataset(fDOM_data, stage_data, turb_data, window_size = 30)\n",
    "# X = np.array(X)\n",
    "# y = np.array(y)\n",
    "# print(X.shape)\n",
    "# print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VaCpC9qXwKDV"
   },
   "outputs": [],
   "source": [
    "f = fDOM_data[0:30,1].reshape(-1,1)\n",
    "t = turb_data[0:30,1].reshape(-1,1)\n",
    "s = stage_data[0:30,1].reshape(-1,1)\n",
    "\n",
    "x = np.hstack((f,t,s)).T\n",
    "print(x.shape)\n",
    "print(x[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0AUbfD7BLMzq"
   },
   "outputs": [],
   "source": [
    "def datetime_to_julian(date : datetime.datetime) -> float:\n",
    "    \"\"\"\n",
    "    Convert datetime object to julian time using algorithm outlined by Fliegel and van Flandern (1968):\n",
    "    \n",
    "    date:   date to convert\n",
    "\n",
    "    return: julian time equivalent\n",
    "    \"\"\"\n",
    "    interm1 = math.floor((14-date.month)/12)\n",
    "    interm2 = date.year + 4800 - interm1\n",
    "    interm3 = date.month + 12*interm1 - 3\n",
    "\n",
    "    jdn = (date.day + math.floor((153*interm3 + 2)/5) + 365*interm2 + \n",
    "           math.floor(interm2/4) - math.floor(interm2/100) + math.floor(interm2/400) - 32045)\n",
    "\n",
    "    jd = jdn + (date.hour - 12) / 24 + date.minute / 1440 + date.second / 86400 + date.microsecond / 86400000000\n",
    "    return jd\n",
    "\n",
    "def delete_missing_data_peaks(data, peaks, props, missing_file_path):\n",
    "    \"\"\" \n",
    "    Delete peaks that occur during time periods designated as \"missing data\"\n",
    "    \n",
    "    data:              timeseries that peaks occured in\n",
    "    peaks:             indices of peaks detected\n",
    "    props:             properties associated with each peak\n",
    "    missing_file_path: file path of missing date ranges\n",
    "    return:            filtered peaks and props\n",
    "    \"\"\"\n",
    "    with open(missing_file_path,newline='') as file:\n",
    "        reader = csv.reader(file, delimiter = ',')\n",
    "        time_list = []\n",
    "        for row in reader:\n",
    "            time_list.append([datetime_to_julian(datetime.datetime.strptime(row[0],'%Y-%m-%d %H:%M:%S')),\n",
    "                              datetime_to_julian(datetime.datetime.strptime(row[1],'%Y-%m-%d %H:%M:%S'))])\n",
    "            \n",
    "        # Identify and remove violating peaks \n",
    "        keep_indices = list(np.linspace(0,peaks.shape[0]-1,peaks.shape[0]))\n",
    "        for i,idx in enumerate(peaks): \n",
    "            time = data[idx,0]\n",
    "            for row in time_list: \n",
    "                if time >= row[0] and time <= row[1]:\n",
    "                    keep_indices.remove(i)\n",
    "                    break\n",
    "        \n",
    "        peaks = np.take(peaks,keep_indices,0)  \n",
    "        \n",
    "        # Remove properties for violating peaks\n",
    "        for key in props:\n",
    "            props[key] = np.take(props[key], keep_indices,0)\n",
    "        \n",
    "        return peaks, props"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NNExperimentTF.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
