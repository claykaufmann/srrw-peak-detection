{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc8621b2-fced-4453-8ad6-4bb864e72d63",
   "metadata": {},
   "source": [
    "# Detect fDOM Plummeting Peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494cff5c-6d46-47c1-bac0-ac5ccac63edb",
   "metadata": {},
   "source": [
    "## Rules for plummeting peaks\n",
    "\n",
    "Downward peak where: \n",
    "- Base width smaller than threshold \n",
    "- Prominence larger than a threshold \n",
    "- No corresponding peak in turbidity in a certain range \n",
    "- No other downward peaks in certain proximity (this would make the plummeting peak local fluctuation...) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03dcbb1-0bc3-4819-9275-0df0c1e9e3e7",
   "metadata": {},
   "source": [
    "## Import Tools, Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd747e72-5140-4804-b0e8-204db2f5de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and data \n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import copy\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from os.path import dirname, join as pjoin\n",
    "import datetime\n",
    "import csv\n",
    "import math\n",
    "import sys\n",
    "sys.path.insert(1,'../')\n",
    "import Tools.data_processing as dp\n",
    "import Tools.data_movement as dm \n",
    "from auxiliary_functions import get_candidates, detect_flat_plat, detect_stage_rises\n",
    "\n",
    "fDOM_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv')\n",
    "stage_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/stage_10.1.11-1.1.19.csv')\n",
    "turb_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv')\n",
    "stage_data = dp.align_stage_to_fDOM(fDOM_data, stage_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48b725a-cf30-440c-8020-9e62dfc9b05d",
   "metadata": {},
   "source": [
    "## Flip fDOM timeseries to detect plummeting peaks. Get candidate set for plummeting peaks\n",
    "#### Flipping fDOM is surely inefficient and \"peaks\" could be detected in the unflipped timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c42a3a57-8b9c-47e4-80e5-7a0e0fed5a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hh/m6vvts714wdcv6sngjwj41zr0000gn/T/ipykernel_25502/31462634.py:12: PeakPropertyWarning: some peaks have a prominence of 0\n",
      "  peaks, props = find_peaks(flipped_fDOM[:,1],\n"
     ]
    }
   ],
   "source": [
    "# Flip timeseries \n",
    "flipped_fDOM = dp.flip_timeseries(copy.deepcopy(fDOM_data))\n",
    "\n",
    "# Get fDOM plummeting peak candidate set using scipy find_peaks()\n",
    "prominence_range = [3,None] # peaks must have at least prominence 3\n",
    "width_range = [None,10] # peaks cannot have a base width of more than 5\n",
    "wlen = 100 \n",
    "distance = 1 \n",
    "rel_height =.6\n",
    "\n",
    "# Get list of all peaks that could possibly be plummeting peaks\n",
    "peaks, props = find_peaks(flipped_fDOM[:,1],\n",
    "                          height = (None, None),\n",
    "                          threshold = (None,None),\n",
    "                          distance = distance,\n",
    "                          prominence = prominence_range,\n",
    "                          width = width_range,\n",
    "                          wlen = wlen,\n",
    "                          rel_height = rel_height)\n",
    "\n",
    "# Form candidate set from returned information\n",
    "cands = [[peak, math.floor(props['left_ips'][i]), math.ceil(props['right_ips'][i]),props['prominences'][i]] for i,peak in enumerate(peaks)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c077d21-b778-45bc-87a5-3ea29429dee4",
   "metadata": {},
   "source": [
    "## Import ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6be6b075-8522-4e67-a394-4759d23c6d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and process ground truth\n",
    "truth_fname = '../Data/labeled_data/ground_truths/fDOM/fDOM_Plummeting_Peaks/julian_time/fDOM_PLP_0k-300k.csv'\n",
    "\n",
    "with open(truth_fname, 'r', newline = '') as f:\n",
    "    \n",
    "        reader = csv.reader(f, delimiter = ',')\n",
    "        # truth entries in form: ['timestamp_of_peak', 'value_of_peak','label_of_peak','idx_of_peak']\n",
    "        next(reader)\n",
    "        truths = [[float(row[0]), float(row[1]), row[2], int(row[3])] for row in reader] \n",
    "        f.close()   \n",
    "\n",
    "assert(len(truths) == len(cands))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2326d3f3-7388-420f-b577-949e5a4100df",
   "metadata": {},
   "source": [
    "## Detect turbidity peaks - necessary to distinquish between plummeting peak and interference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16fda0af-7e67-48c6-9908-d70a2e8dcd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "turb_peak_params = {'prom' : [6,None],\n",
    "                    'width': [None, None],\n",
    "                    'wlen' : 200,\n",
    "                    'dist' : 1,\n",
    "                    'rel_h': .6}\n",
    "\n",
    "# Get fDOM and turb candiate peaks\n",
    "turb_peaks, _ = get_candidates(turb_data, turb_peak_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505c2f11-7b95-44aa-a753-48e94352ed15",
   "metadata": {},
   "source": [
    "## Define helper functions and set of training parameters, create training/testing splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f719765-f78f-4bda-a99a-0941dca8d890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameter ranges and training parameters and helper function\n",
    "iterations = 70000\n",
    "num_splits = 5\n",
    "\n",
    "basewidth_range = (1, 10)\n",
    "prominence_range= (5, 20)\n",
    "\n",
    "peak_proximity_bounds = (1,20)\n",
    "turb_interference_bounds = (-10,10)\n",
    "\n",
    "\n",
    "accumulated_test_metrics = {}\n",
    "\n",
    "accumulated_test_results = {}\n",
    "\n",
    "accumulated_best_params = {}\n",
    "\n",
    "train_test_split_indices = TimeSeriesSplit(num_splits).split(cands)\n",
    "\n",
    "def classify_candidate_peaks(peaks, params):\n",
    "    \n",
    "    def check_turb_interference():\n",
    "        pass \n",
    "    def check_peak_proximity():\n",
    "        pass \n",
    "    \n",
    "    results = []\n",
    "    for peak in peaks:\n",
    "        prominence_condition = peak[3] >= params['min_prominence']\n",
    "        basewidth_condition = abs(peak[1] - peak[2]) <= params['max_basewidth']\n",
    "        turb_inteference_condition = check_turb_interference()\n",
    "        peak_proximity_condition = check_peak_proximity()\n",
    "        if prominence_condition and basewidth_condition and turb_interference_condition and peak_proximity_condition:\n",
    "            results.append((peak[0], 'PLP'))\n",
    "        else:\n",
    "            results.append((peak[0], 'NPLP'))\n",
    "    return results\n",
    "\n",
    "def label_positives_negatives(predictions, truths):\n",
    "    TP = TN = FP = FN = 0\n",
    "    results = []\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        prediction = predictions[i][1]\n",
    "        truth = truths[i][2]\n",
    "        \n",
    "        if prediction == 'PLP':\n",
    "            if truth == 'NPLP':\n",
    "                FP +=1\n",
    "                results.append(predictions[i].append('FP'))\n",
    "            else: \n",
    "                TP +=1\n",
    "                results.append(predictions[i].append('TP'))\n",
    "        else:\n",
    "            if truth == 'NPLP':\n",
    "                TN +=1\n",
    "                results.append(predictions[i].append('TN'))\n",
    "            else: \n",
    "                FN +=1\n",
    "                results.append(predictions[i].append('FN'))\n",
    "        \n",
    "    return (TP,TN,FP,FN,results)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f435be-2919-4a3a-b573-0692f663410b",
   "metadata": {},
   "source": [
    "## Nested Cross Validation Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4667fb20-dff7-453f-9491-9639092154bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_start = datetime.datetime.now()\n",
    "\n",
    "split = 1\n",
    "\n",
    "for train_val_indices, test_indices in train_test_split_indices:\n",
    "    X_train, y_train = [cands[i] for i in train_val_indices], [truths[i] for i in train_val_indices]\n",
    "    X_test, y_test = [cands[i] for i in test_indices], [truths[i] for i in test_indices]\n",
    "    \n",
    "    max_fold_metric = 0 \n",
    "    max_result = None \n",
    "    \n",
    "    print(\"Split: \",split)\n",
    "    \n",
    "    split_start = datetime.datetime.now()\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        \n",
    "        # Random grid search for hyperparams \n",
    "        params = {}\n",
    "        \n",
    "        params['max_basewidth'] = np.random.randint(basewidth_range[0], basewidth_range[1]+1)\n",
    "        params['min_prominence'] = np.random.uniform(prominence_range[0], prominence_range[1])\n",
    "        \n",
    "        temp = np.random.randint(turb_interference_bounds[0], turb_interference_bounds[1])\n",
    "        params['turb_interference_range'] = (temp, np.random.randint(temp, turb_interference_bounds[1]) + 2)\n",
    "        \n",
    "        params['peak_proximity_threshold']= np.random.randint(peak_proximity_bounds[0], peak_proximity_bounds[1])\n",
    "      \n",
    "        predictions = classify_candidate_peaks(X_train, params)\n",
    "        \n",
    "        TP,TN,FP,FN,results = label_positives_negatives(predictions, y_train)\n",
    "        \n",
    "        TPR = TP/(TP + FN)\n",
    "        TNR = TN/(TN + FP)\n",
    "        \n",
    "        bal_acc = (TPR + TNR)/2\n",
    "        \n",
    "        f1_score = (2 * TP)/((2 * TP) + FP + FN)\n",
    "\n",
    "        acc = f1_score\n",
    "        if iteration and iteration % int(iterations/10) == 0: print(\" {}/{} \".format(iteration, iterations), end = \"\")\n",
    "        if acc > max_fold_metric: \n",
    "            max_fold_metric = acc\n",
    "            max_result = copy.deepcopy(results)\n",
    "            best_params = copy.deepcopy(params)\n",
    "            \n",
    "    # Test best parameters on testing data \n",
    "    test_predictions = classify_candidate_peaks(X_test, best_params)\n",
    "    TP,TN,FP,FN,results = label_positives_negatives(test_predictions, y_test)\n",
    "    \n",
    "    TPR = TP/(TP + FN)\n",
    "    TNR = TN/(TN + FP)\n",
    "    \n",
    "    bal_acc = (TPR + TNR)/2 \n",
    "    f1_score = (2 * TP)/((2 * TP) + FP + FN)\n",
    "    \n",
    "    print('\\nSplit: {}  F1: {:.4f} BA: {:.4f}  Params: {}  TP: {} TN: {} FP: {} FN: {}  Time: {}'.format(split, f1_score, bal_acc, best_params, TP, TN, FP, FN, datetime.datetime.now() - split_start))\n",
    "    accumulated_test_metrics[split] = [f1_score, bal_acc] # Record test metrics of each split\n",
    "    accumulated_test_results[split] = copy.deepcopy(results) # Record test results (FP,FN,TP,TN for each datapoint) for each split\n",
    "    accumulated_best_params[split] = copy.deepcopy(best_params) # Record params uses in testing for each split\n",
    "    \n",
    "    split+=1\n",
    "\n",
    "# Display Resuls \n",
    "mean_f1 = 0\n",
    "mean_ba = 0 \n",
    "\n",
    "for key in accumulated_test_metrics:\n",
    "    metrics = accumulated_test_metrics[key]\n",
    "    mean_f1+=metrics[0]\n",
    "    mean_ba+=metrics[1]\n",
    "\n",
    "print(\"Mean Test F1: \", mean_f1/len(accumulated_test_metrics))\n",
    "print(\"Mean Test BA: \", mean_ba/len(accumulated_test_metrics))\n",
    "\n",
    "print(\"Training time: \", datetime.datetime.now() - overall_start)\n",
    "\n",
    "# Pickle params from last fold\n",
    "with open('./Experiments/fDOM_PLP/best_params.pkl', 'wb') as pck_file:\n",
    "    pickle.dump(accumulated_best_params[num_splits], pck_file)\n",
    "    pck_file.close()\n",
    "    \n",
    "# Pickle results from last fold \n",
    "with open('./Experiments/fDOM_PLP/test_results.pkl', 'wb') as pck_file:\n",
    "    pickle.dump(accumulated_test_results, pck_file)\n",
    "    pck_file.close()\n",
    "\n",
    "# Pickle results from last fold \n",
    "with open('./Experiments/fDOM_PLP/test_metrics.pkl', 'wb') as pck_file:\n",
    "    pickle.dump(accumulated_test_metrics, pck_file)\n",
    "    pck_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf216a29-c9b0-41e3-b512-dfef624c1408",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ada5b320-08c3-417e-99c7-a38b39887f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "turb_cand_params = {'prom' : [6,None],\n",
    "                    'width': [None, None],\n",
    "                    'wlen' : 200,\n",
    "                    'dist' : 1,\n",
    "                    'rel_h': .6}\n",
    "\n",
    "# Get fDOM and turb candiate peaks\n",
    "turb_peaks, turb_props = get_candidates(turb_data, turb_cand_params)\n",
    "\n",
    "# Iterate through peaks and turn into short 3 point \"events\" by flagging the data point to either side of a peak\n",
    "fDOM_events = []\n",
    "fDOM_lb = []\n",
    "fDOM_rb = []\n",
    "\n",
    "for i,cand in enumerate(cands):\n",
    "            fDOM_events.append(np.array((flipped_fDOM[cand[0]])))\n",
    "            fDOM_lb.append(flipped_fDOM[math.floor(cand[1]),0])\n",
    "            fDOM_rb.append(flipped_fDOM[math.ceil(cand[2]),0])\n",
    "            \n",
    "fDOM_lb = list(set(fDOM_lb))\n",
    "fDOM_lb.sort()\n",
    "fDOM_rb = list(set(fDOM_rb))\n",
    "fDOM_rb.sort()\n",
    "\n",
    "turb_events = []\n",
    "turb_lb = []\n",
    "turb_rb = []\n",
    "\n",
    "for i,peak in enumerate(turb_peaks):\n",
    "            turb_events.append(np.array((turb_data[peak])))\n",
    "            turb_lb.append(turb_data[math.floor(turb_props['left_ips'][i]),0])\n",
    "            turb_rb.append(turb_data[math.ceil(turb_props['right_ips'][i]),0])\n",
    "            \n",
    "turb_lb = list(set(turb_lb))\n",
    "turb_lb.sort()\n",
    "turb_rb = list(set(turb_rb))\n",
    "turb_rb.sort()            \n",
    "\n",
    "fDOM_merged = dp.merge_data(flipped_fDOM, fDOM_events, 'intf', '')\n",
    "turb_merged = dp.merge_data(turb_data, turb_events, 't_opp', '')\n",
    "\n",
    "fDOM_merged = dp.merge_additional_data(fDOM_merged, fDOM_lb, 'left_base')\n",
    "fDOM_merged = dp.merge_additional_data(fDOM_merged, fDOM_rb, 'right_base')\n",
    "\n",
    "turb_merged = dp.merge_additional_data(turb_merged, turb_lb, 'left_base')\n",
    "turb_merged = dp.merge_additional_data(turb_merged, turb_rb, 'right_base')\n",
    "\n",
    "\n",
    "stage_data_merged = dp.merge_data(stage_data, [], '','')\n",
    "\n",
    "dm.write_data_to_trainset(fDOM_merged,\n",
    "                          stage_data_merged,\n",
    "                          turb_merged,\n",
    "                          '../Data/temp_plotting/fDOM_plum_200k-300k.csv',\n",
    "                          True,\n",
    "                          True,\n",
    "                          200000,\n",
    "                          300000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
