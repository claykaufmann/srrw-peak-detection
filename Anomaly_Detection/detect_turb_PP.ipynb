{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5522f49d-3188-4054-8c49-9102f3717c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and data \n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import copy\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from os.path import dirname, join as pjoin\n",
    "import datetime\n",
    "import csv\n",
    "import math\n",
    "import sys\n",
    "sys.path.insert(1,'../')\n",
    "import Tools.data_processing as dp\n",
    "import Tools.data_movement as dm \n",
    "from auxiliary_functions import get_candidates, detect_flat_plat, detect_stage_rises\n",
    "\n",
    "\n",
    "fDOM_raw_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv')\n",
    "stage_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/stage_10.1.11-1.1.19.csv')\n",
    "turb_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv')\n",
    "stage_data = dp.align_stage_to_fDOM(fDOM_raw_data, stage_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a4822c-62b6-4c20-b352-e4a493eb6668",
   "metadata": {},
   "source": [
    "### Detect Stage Rises and Process \n",
    "##### Process stage rises so that each index contains value representing distance to next stage rise in both positive and negative directioin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1a7258f-3f12-4531-8a5c-c0fd160b1e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stage rises\n",
    "s_indices = detect_stage_rises(stage_data[:,1])\n",
    "\n",
    "# Process stage rises so that each index displays distance to next stage rise in positive and negative direction\n",
    "y = s_indices.shape[0] -1 \n",
    "s_indexed = np.zeros((s_indices.shape[0],2))\n",
    "x_count = -1 \n",
    "y_count = -1\n",
    "for x in range(s_indices.shape[0]):\n",
    "    # X Block \n",
    "    \n",
    "    # When x encounters first stage rise, start x counter\n",
    "    if x_count == -1 and s_indices[x] == 1:\n",
    "        x_count = 0\n",
    "    if x_count != -1:\n",
    "        if s_indices[x] == 1:\n",
    "            x_count = 0\n",
    "            s_indexed[x,0] = x_count\n",
    "        else:\n",
    "            x_count += 1\n",
    "            s_indexed[x,0] = x_count\n",
    "    else:\n",
    "        s_indexed[x,0] = -1\n",
    "            \n",
    "    # Y Block\n",
    "    if y_count == -1 and s_indices[y] == 1:\n",
    "        y_count = 0\n",
    "    if y_count != -1:\n",
    "        if s_indices[y] == 1:\n",
    "            y_count = 0\n",
    "            s_indexed[y,1] = y_count\n",
    "        else:\n",
    "            y_count += 1\n",
    "            s_indexed[y,1] = y_count\n",
    "    else: \n",
    "        s_indexed[y,1] = -1\n",
    "        \n",
    "    y-=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780c79e1-a1e5-4002-9a7f-cd718b05f09b",
   "metadata": {},
   "source": [
    "### Get Candidate Turb PP Peaks and Process \n",
    "##### For each peak, determine how far away from a stage rise is is in both the negative and postive temporal directione\n",
    "##### Import ground truth values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95ffd79f-e112-4391-b419-914186750af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Get turb and fDOM peaks\n",
    "fDOM_cand_params = {'prom' : [8,None],\n",
    "                    'width': [5, None],\n",
    "                    'wlen' : 300,\n",
    "                    'dist' : 20,\n",
    "                    'rel_h': .6}\n",
    "\n",
    "turb_cand_params = {'prom' : [6,None],\n",
    "                    'width': [None, None],\n",
    "                    'wlen' : 200,\n",
    "                    'dist' : 1,\n",
    "                    'rel_h': .6}\n",
    "\n",
    "fDOM_peaks, fDOM_props = get_candidates(fDOM_raw_data, fDOM_cand_params)\n",
    "turb_peaks, turb_props = get_candidates(turb_data, turb_cand_params)\n",
    "\n",
    "# Remove peaks that occur during a flat plateau \n",
    "\n",
    "turb_peaks, turb_props = dp.delete_missing_data_peaks(turb_data, turb_peaks, turb_props, '/Users/zachfogg/Desktop/DB-SRRW/Data/misc/flat_plat_ranges.txt')\n",
    "# Convert peaks and props to useable structure and assign values from s_indexed\n",
    "# Each entry = [index_of_peak, left_ips, right_ips, X, Y, flag]\n",
    "turb_cand = [[peak, math.floor(turb_props['left_ips'][i]), math.ceil(turb_props['right_ips'][i]),s_indexed[peak,0], s_indexed[peak,1] ,turb_props['prominences'][i]] for i,peak in enumerate(turb_peaks)]\n",
    "\n",
    "# Import ground truth values \n",
    "truth_fname = '/Users/zachfogg/Desktop/DB-SRRW/Data/manual_annotating_data/processed_data/turb/julian_time/turb_pp_0k-300k-2_labeled'\n",
    "\n",
    "with open(truth_fname, 'r', newline = '') as f:\n",
    "    \n",
    "        reader = csv.reader(f, delimiter = ',')\n",
    "        # truth entries in form: ['timestamp_of_peak', 'value_of_peak','label_of_peak','idx_of_peak']\n",
    "        next(reader)\n",
    "        truths = [[float(row[0]), float(row[1]), row[2], int(row[3])] for row in reader] \n",
    "        f.close()   \n",
    "\n",
    "assert(len(truths) == len(turb_cand))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c881cf-dce5-44bf-bb74-bb028d7bcca9",
   "metadata": {},
   "source": [
    "### Hyperparameter Search While Preforming K-Fold Nested Cross Validation on Rule Based Classifier\n",
    "- parameter \"x\" refers to the number of data points before the next stage rise a point is\n",
    "- parameter \"y\" refers to the number of data points after the previous stage rise a point is \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "121e14a5-a68f-440e-b917-bcb59500fa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameter ranges and training parameters and helper function\n",
    "iterations = 70000\n",
    "num_splits = 5\n",
    "\n",
    "turb_peak_prom_threshold = 50 # Threshold for turb peak at which we must see fDOM interference \n",
    "\n",
    "x_bounds = [0, 30]\n",
    "y_bounds = [0, 30]\n",
    "\n",
    "intf_range_1_bounds = [-10,2]\n",
    "intf_range_2_bounds = [-5,3]\n",
    "intf_range_3_bounds = [-2,10]\n",
    "\n",
    "intf_threshold_bounds = [-2,2]\n",
    "\n",
    "\n",
    "accumulated_test_metrics = {}\n",
    "\n",
    "accumulated_test_results = {}\n",
    "\n",
    "accumulated_best_params = {}\n",
    "\n",
    "train_test_split_indices = TimeSeriesSplit(num_splits).split(turb_cand)\n",
    "\n",
    "def check_for_fDOM_interference(fDOM, peak_indx, intf_params):\n",
    "    \"\"\"\n",
    "    Given the index of a turb peak, check fDOM for \n",
    "    interference using given parameters \n",
    "    \"\"\"\n",
    "    # Calculate mean tangent line across range_1\n",
    "    mean_tan_1 = np.mean(np.diff(fDOM[(peak_indx + intf_params['inft_range_1'][0]):(peak_indx + intf_params['inft_range_1'][1]),1]))\n",
    "    # Calculate mean tangent line across range_2\n",
    "    mean_tan_2 = np.mean(np.diff(fDOM[(peak_indx + intf_params['inft_range_2'][0]):(peak_indx + intf_params['inft_range_2'][1]),1]))\n",
    "    # Calculate mean tangent line across range_3\n",
    "    mean_tan_3 = np.mean(np.diff(fDOM[(peak_indx + intf_params['inft_range_3'][0]):(peak_indx + intf_params['inft_range_3'][1]),1])) \n",
    "    # Compare means \n",
    "    if (mean_tan_1 * intf_params['intf_t1']) > (mean_tan_2) or (mean_tan_2) < (mean_tan_3 * intf_params['intf_t2']):\n",
    "        return True \n",
    "    return False\n",
    "\n",
    "def classify_turb_peaks(peaks, params):\n",
    "    results = []\n",
    "    for peak in peaks:\n",
    "        if(peak[3] != -1 and peak[3] <= params['x']) or (peak[4] !=-1 and peak[4] <= params['y']):\n",
    "            if peak[5] > turb_peak_prom_threshold: \n",
    "                intf_result = check_for_fDOM_interference(fDOM_raw_data, peak[0], params)\n",
    "                if intf_result: \n",
    "                    results.append([peak[0], 'NPP'])\n",
    "                else: \n",
    "                    results.append([peak[0], 'PP'])\n",
    "            else: \n",
    "                results.append([peak[0], 'NPP'])\n",
    "        else: \n",
    "            results.append([peak[0], 'PP'])\n",
    "    return results\n",
    "\n",
    "def label_positives_negatives(predictions, truths):\n",
    "    TP = TN = FP = FN = 0\n",
    "    results = []\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        prediction = predictions[i][1]\n",
    "        truth = truths[i][2]\n",
    "        \n",
    "        if prediction == 'NPP':\n",
    "            if truth == 'NPP':\n",
    "                TN +=1\n",
    "                results.append(predictions[i].append('TN'))\n",
    "            else: \n",
    "                FN+=1\n",
    "                results.append(predictions[i].append('FN'))\n",
    "        else:\n",
    "            if truth == 'NPP':\n",
    "                FP+=1\n",
    "                results.append(predictions[i].append('FP'))\n",
    "            else: #TODO: Comeback and evaluate if this makes sense: Algo could predict PP because X/Y was not optimal, while truth was PPP because of interference\n",
    "                TP+=1\n",
    "                results.append(predictions[i].append('TP'))\n",
    "        \n",
    "    return (TP,TN,FP,FN,results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e1803e5e-7bac-40e9-af8a-dbcf52f88289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split:  1\n",
      " 7000/70000  14000/70000  21000/70000  28000/70000  35000/70000  42000/70000  49000/70000  56000/70000  63000/70000 \n",
      "Split: 1  F1: 0.9570 BA: 0.9529  Params: {'x': 9, 'y': 8, 'inft_range_1': (-8, -2), 'inft_range_2': (-5, 1), 'inft_range_3': (8, 11), 'intf_t1': -0.8066638028104025, 'intf_t2': -1.5181968305986993}  TP: 89 TN: 77 FP: 6 FN: 2  Time: 0:00:38.908799\n",
      "Split:  2\n",
      " 7000/70000  14000/70000  21000/70000  28000/70000  35000/70000  42000/70000  49000/70000  56000/70000  63000/70000 \n",
      "Split: 2  F1: 0.9565 BA: 0.9523  Params: {'x': 18, 'y': 1, 'inft_range_1': (-8, 3), 'inft_range_2': (-5, 1), 'inft_range_3': (-2, 10), 'intf_t1': -0.36255907688326783, 'intf_t2': 1.2122392329914708}  TP: 99 TN: 66 FP: 2 FN: 7  Time: 0:01:36.471527\n",
      "Split:  3\n",
      " 7000/70000  14000/70000  21000/70000  28000/70000  35000/70000  42000/70000  49000/70000  56000/70000  63000/70000 \n",
      "Split: 3  F1: 0.9365 BA: 0.9610  Params: {'x': 12, 'y': 2, 'inft_range_1': (-6, 1), 'inft_range_2': (-5, -2), 'inft_range_3': (0, 6), 'intf_t1': -1.2942512822091623, 'intf_t2': 1.4456383652839357}  TP: 59 TN: 107 FP: 7 FN: 1  Time: 0:02:13.221275\n",
      "Split:  4\n",
      " 7000/70000  14000/70000  21000/70000  28000/70000  35000/70000  42000/70000  49000/70000  56000/70000  63000/70000 \n",
      "Split: 4  F1: 0.9420 BA: 0.9570  Params: {'x': 13, 'y': 1, 'inft_range_1': (0, 2), 'inft_range_2': (-5, -3), 'inft_range_3': (4, 7), 'intf_t1': 0.8454681841390039, 'intf_t2': -0.19683731961573248}  TP: 65 TN: 101 FP: 6 FN: 2  Time: 0:03:11.962972\n",
      "Split:  5\n",
      " 7000/70000  14000/70000  21000/70000  28000/70000  35000/70000  42000/70000  49000/70000  56000/70000  63000/70000 \n",
      "Split: 5  F1: 0.9315 BA: 0.9395  Params: {'x': 13, 'y': 2, 'inft_range_1': (0, 2), 'inft_range_2': (-4, 1), 'inft_range_3': (9, 11), 'intf_t1': 0.7100971913695409, 'intf_t2': 0.0656586836339943}  TP: 68 TN: 96 FP: 4 FN: 6  Time: 0:04:13.532925\n",
      "Mean Test F1:  0.9447109515545028\n",
      "Mean Test BA:  0.9525206551697059\n",
      "Training time:  0:11:54.100646\n"
     ]
    }
   ],
   "source": [
    "overall_start = datetime.datetime.now()\n",
    "\n",
    "split = 1\n",
    "\n",
    "for train_val_indices, test_indices in train_test_split_indices:\n",
    "    X_train, y_train = [turb_cand[i] for i in train_val_indices], [truths[i] for i in train_val_indices]\n",
    "    X_test, y_test = [turb_cand[i] for i in test_indices], [truths[i] for i in test_indices]\n",
    "    \n",
    "    max_fold_metric = 0 \n",
    "    max_result = None \n",
    "    \n",
    "    print(\"Split: \",split)\n",
    "    \n",
    "    split_start = datetime.datetime.now()\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        \n",
    "        # Random grid search for hyperparams \n",
    "        params = {}\n",
    "        \n",
    "        params['x'] = np.random.randint(x_bounds[0], x_bounds[1]+1)\n",
    "        params['y'] = np.random.randint(y_bounds[0], y_bounds[1]+1)\n",
    "#         params['x'] = 1\n",
    "#         params['y'] = 11\n",
    "        \n",
    "        temp = np.random.randint(intf_range_1_bounds[0], intf_range_1_bounds[1])\n",
    "        params['inft_range_1'] = (temp, np.random.randint(temp, intf_range_1_bounds[1]) + 2)\n",
    "        \n",
    "        temp = np.random.randint(intf_range_2_bounds[0], intf_range_2_bounds[1])\n",
    "        params['inft_range_2'] = (temp, np.random.randint(temp, intf_range_2_bounds[1]) + 2)\n",
    "        \n",
    "        temp = np.random.randint(intf_range_3_bounds[0], intf_range_3_bounds[1])\n",
    "        params['inft_range_3'] = (temp, np.random.randint(temp, intf_range_3_bounds[1]) + 2)\n",
    "        \n",
    "        params['intf_t1'] = np.random.uniform(intf_threshold_bounds[0], intf_threshold_bounds[1])\n",
    "        params['intf_t2'] = np.random.uniform(intf_threshold_bounds[0], intf_threshold_bounds[1])\n",
    "        \n",
    "        predictions = classify_turb_peaks(X_train, params)\n",
    "        \n",
    "        TP,TN,FP,FN,results = label_positives_negatives(predictions, y_train)\n",
    "        \n",
    "        TPR = TP/(TP + FN)\n",
    "        TNR = TN/(TN + FP)\n",
    "        \n",
    "        bal_acc = (TPR + TNR)/2\n",
    "        \n",
    "        f1_score = (2 * TP)/((2 * TP) + FP + FN)\n",
    "\n",
    "        acc = f1_score\n",
    "        if iteration and iteration % int(iterations/10) == 0: print(\" {}/{} \".format(iteration, iterations), end = \"\")\n",
    "        if acc > max_fold_metric: \n",
    "            max_fold_metric = acc\n",
    "            max_result = copy.deepcopy(results)\n",
    "            best_params = copy.deepcopy(params)\n",
    "            \n",
    "    # Test best parameters on testing data \n",
    "    test_predictions = classify_turb_peaks(X_test, best_params)\n",
    "    TP,TN,FP,FN,results = label_positives_negatives(test_predictions, y_test)\n",
    "    \n",
    "    TPR = TP/(TP + FN)\n",
    "    TNR = TN/(TN + FP)\n",
    "    \n",
    "    bal_acc = (TPR + TNR)/2 \n",
    "    f1_score = (2 * TP)/((2 * TP) + FP + FN)\n",
    "    \n",
    "    print('\\nSplit: {}  F1: {:.4f} BA: {:.4f}  Params: {}  TP: {} TN: {} FP: {} FN: {}  Time: {}'.format(split, f1_score, bal_acc, best_params, TP, TN, FP, FN, datetime.datetime.now() - split_start))\n",
    "    accumulated_test_metrics[split] = [f1_score, bal_acc] # Record test metrics of each split\n",
    "    accumulated_test_results[split] = copy.deepcopy(results) # Record test results (FP,FN,TP,TN for each datapoint) for each split\n",
    "    accumulated_best_params[split] = copy.deepcopy(best_params) # Record params uses in testing for each split\n",
    "    \n",
    "    split+=1\n",
    "\n",
    "# Display Resuls \n",
    "mean_f1 = 0\n",
    "mean_ba = 0 \n",
    "\n",
    "for key in accumulated_test_metrics:\n",
    "    metrics = accumulated_test_metrics[key]\n",
    "    mean_f1+=metrics[0]\n",
    "    mean_ba+=metrics[1]\n",
    "\n",
    "print(\"Mean Test F1: \", mean_f1/len(accumulated_test_metrics))\n",
    "print(\"Mean Test BA: \", mean_ba/len(accumulated_test_metrics))\n",
    "\n",
    "print(\"Training time: \", datetime.datetime.now() - overall_start)\n",
    "\n",
    "# Pickle params from last fold\n",
    "with open('./Experiments/turb_PP/best_params.pkl', 'wb') as pck_file:\n",
    "    pickle.dump(accumulated_best_params[num_splits], pck_file)\n",
    "    pck_file.close()\n",
    "    \n",
    "# Pickle results from last fold \n",
    "with open('./Experiments/turb_PP/test_results.pkl', 'wb') as pck_file:\n",
    "    pickle.dump(accumulated_test_results, pck_file)\n",
    "    pck_file.close()\n",
    "\n",
    "# Pickle results from last fold \n",
    "with open('./Experiments/turb_PP/test_metrics.pkl', 'wb') as pck_file:\n",
    "    pickle.dump(accumulated_test_metrics, pck_file)\n",
    "    pck_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd225c4-6835-4db0-87dc-c568e59e8b33",
   "metadata": {},
   "source": [
    "### Plot results to look for inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b770632-cd70-44cd-a0d2-2317ac30f92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9665711556829035\n",
      "[191, 'NPP', 'TN']\n",
      "[191, 186, 195, 0.0, 0.0, 8.4696597116]\n",
      "FP:  21 FN:  14 TN:  515  TP:  497\n"
     ]
    }
   ],
   "source": [
    "# Mean Test F1:  0.9447109515545028\n",
    "# Mean Test BA:  0.9525206551697059\n",
    "# Training time:  0:11:54.100646\n",
    "# Best: Params: {'x': 13, 'y': 2, 'inft_range_1': (0, 2), 'inft_range_2': (-4, 1), 'inft_range_3': (9, 11), \n",
    "# 'intf_t1': 0.7100971913695409, 'intf_t2': 0.0656586836339943}\n",
    "\n",
    "test_params = {'x': 13, 'y': 2, 'inft_range_1': (0, 2), 'inft_range_2': (-4, 1), 'inft_range_3': (9, 11), 'intf_t1': 0.7100971913695409, 'intf_t2': 0.0656586836339943}\n",
    "\n",
    "predictions = classify_turb_peaks(turb_cand, test_params)\n",
    "        \n",
    "TP,TN,FP,FN,results = label_positives_negatives(predictions, truths)\n",
    "\n",
    "f1_score = (2 * TP)/((2 * TP) + FP + FN)\n",
    "print((TP + TN)/(TP + TN + FN + FP))\n",
    "print(predictions[0])\n",
    "print(turb_cand[0])\n",
    "print(\"FP: \", FP, \"FN: \", FN, \"TN: \", TN, \" TP: \", TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "040ccdf5-e9fb-4acb-a191-16c44927fffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_map = {}\n",
    "for pred in predictions:\n",
    "    peak_map[pred[0]] = [pred[1], pred[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7031a6f9-a0ad-408e-885f-b0db3256daa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stage rises\n",
    "s_indices = detect_stage_rises(stage_data[:,1])\n",
    "# fDOM_cand_params = {'prom' : [4,None],\n",
    "#                     'width': [None, None],   These are the params for fDOM pp cand\n",
    "#                     'wlen' : 200,\n",
    "#                     'dist' : 1,\n",
    "#                     'rel_h': .6}\n",
    "\n",
    "fDOM_cand_params = {'prom' : [4,None],\n",
    "                    'width': [None, 2],\n",
    "                    'wlen' : 200,\n",
    "                    'dist' : 1,\n",
    "                    'rel_h': .6}\n",
    "\n",
    "# turb_cand_params = {'prom' : [6,None],       These are the params that were used to label turb cand 0-100k\n",
    "#                     'width': [None, None],\n",
    "#                     'wlen' : 200,\n",
    "#                     'dist' : 1,\n",
    "#                     'rel_h': .6}\n",
    "\n",
    "turb_cand_params = {'prom' : [6,None],\n",
    "                    'width': [None, 2],\n",
    "                    'wlen' : 200,\n",
    "                    'dist' : 1,\n",
    "                    'rel_h': .5}\n",
    "\n",
    "# Get fDOM and turb candiate peaks\n",
    "fDOM_peaks, fDOM_props = get_candidates(fDOM_raw_data, fDOM_cand_params)\n",
    "turb_peaks, turb_props = get_candidates(turb_data, turb_cand_params)\n",
    "\n",
    "# Remove peaks that occur during a flat plateau \n",
    "turb_flat_plat = detect_flat_plat(turb_data, 100, 40)\n",
    "turb_flat_plat_indxs = []\n",
    "for i in range(turb_flat_plat.shape[0]):\n",
    "    if turb_flat_plat[i] == 1:\n",
    "        turb_flat_plat_indxs.append(i)\n",
    "\n",
    "take_indices = []\n",
    "for i,peak in enumerate(turb_peaks):\n",
    "    if peak not in turb_flat_plat_indxs:\n",
    "        take_indices.append(i)\n",
    "\n",
    "turb_peaks = np.take(turb_peaks, take_indices)\n",
    "for key in turb_props:\n",
    "    turb_props[key] = np.take(turb_props[key], take_indices)\n",
    "\n",
    "# Iterate through peaks and turn into short 3 point \"events\" by flagging the data point to either side of a peak\n",
    "# fDOM_events = []\n",
    "# for peak in fDOM_peaks:\n",
    "#             fDOM_events.append(np.array((fDOM_raw_data[peak-1], fDOM_raw_data[peak], fDOM_raw_data[peak+1])))\n",
    "fDOM_events = []\n",
    "fDOM_lb = []\n",
    "fDOM_rb = []\n",
    "\n",
    "for i,peak in enumerate(fDOM_peaks):\n",
    "            fDOM_events.append(np.array((fDOM_raw_data[peak])))\n",
    "#             fDOM_lb.append(fDOM_raw_data[fDOM_props['left_bases'][i],0])\n",
    "#             fDOM_rb.append(fDOM_raw_data[fDOM_props['right_bases'][i],0])\n",
    "            fDOM_lb.append(fDOM_raw_data[math.floor(fDOM_props['left_ips'][i]),0])\n",
    "            fDOM_rb.append(fDOM_raw_data[math.ceil(fDOM_props['right_ips'][i]),0])\n",
    "            \n",
    "fDOM_lb = list(set(fDOM_lb))\n",
    "fDOM_lb.sort()\n",
    "fDOM_rb = list(set(fDOM_rb))\n",
    "fDOM_rb.sort()\n",
    "\n",
    "turb_events = []\n",
    "turb_lb = []\n",
    "turb_rb = []\n",
    "for i,peak in enumerate(turb_peaks):\n",
    "            turb_events.append(np.array((turb_data[peak])))\n",
    "            turb_lb.append(turb_data[math.floor(turb_props['left_ips'][i]),0])\n",
    "            turb_rb.append(turb_data[math.ceil(turb_props['right_ips'][i]),0])\n",
    "            \n",
    "turb_lb = list(set(turb_lb))\n",
    "turb_lb.sort()\n",
    "turb_rb = list(set(turb_rb))\n",
    "turb_rb.sort()            \n",
    "\n",
    "fDOM_merged = dp.merge_data(fDOM_raw_data, fDOM_events, 'f_opp', '')\n",
    "turb_merged = dp.merge_data(turb_data, turb_events, 't_opp', '')\n",
    "\n",
    "fDOM_merged = dp.merge_additional_data(fDOM_merged, fDOM_lb, 'left_base')\n",
    "fDOM_merged = dp.merge_additional_data(fDOM_merged, fDOM_rb, 'right_base')\n",
    "\n",
    "turb_merged = dp.merge_additional_data(turb_merged, turb_lb, 'left_base')\n",
    "turb_merged = dp.merge_additional_data(turb_merged, turb_rb, 'right_base')\n",
    "\n",
    "\n",
    "stage_edge_data = dp.stage_rises_to_data(s_indices, stage_data)\n",
    "stage_data_merged = dp.merge_data(stage_data, stage_edge_data, 'rise','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cb6bfc5-d45e-4c27-98b8-fa877747b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for peak in peak_map.keys():\n",
    "    turb_merged[peak][2] = peak_map[peak][0] + \"_\" + peak_map[peak][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc8bf9f6-0f11-49be-b770-87b0e0779fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.write_data_to_trainset(fDOM_merged,\n",
    "                          stage_data_merged,\n",
    "                          turb_merged,\n",
    "                          '../Data/plot/pred_turb_pp_0k-100k.csv',\n",
    "                          True,\n",
    "                          True,\n",
    "                          0,\n",
    "                          100000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
