{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d0315618-62c9-4b38-bb86-95cd7705dbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and data \n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "import copy\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from os.path import dirname, join as pjoin\n",
    "import datetime\n",
    "import csv\n",
    "import math\n",
    "import sys\n",
    "sys.path.insert(1,'../')\n",
    "import Tools.data_processing as dp\n",
    "import Tools.data_movement as dm \n",
    "from auxiliary_functions import get_candidates, detect_flat_plat, detect_stage_rises\n",
    "\n",
    "fDOM_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv')\n",
    "stage_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/stage_10.1.11-1.1.19.csv')\n",
    "turb_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv')\n",
    "stage_data = dp.align_stage_to_fDOM(fDOM_data, stage_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d014c238-2633-4ce4-8f69-44ef776bb992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stage rises\n",
    "s_indices = detect_stage_rises(stage_data[:,1])\n",
    "\n",
    "# Process stage rises so that each index displays distance to next stage rise in positive and negative direction\n",
    "y = s_indices.shape[0] -1 \n",
    "s_indexed = np.zeros((s_indices.shape[0],2))\n",
    "x_count = -1 \n",
    "y_count = -1\n",
    "for x in range(s_indices.shape[0]):\n",
    "    # X Block \n",
    "    \n",
    "    # When x encounters first stage rise, start x counter\n",
    "    if x_count == -1 and s_indices[x] == 1:\n",
    "        x_count = 0\n",
    "    if x_count != -1:\n",
    "        if s_indices[x] == 1:\n",
    "            x_count = 0\n",
    "            s_indexed[x,0] = x_count\n",
    "        else:\n",
    "            x_count += 1\n",
    "            s_indexed[x,0] = x_count\n",
    "    else:\n",
    "        s_indexed[x,0] = -1\n",
    "            \n",
    "    # Y Block\n",
    "    if y_count == -1 and s_indices[y] == 1:\n",
    "        y_count = 0\n",
    "    if y_count != -1:\n",
    "        if s_indices[y] == 1:\n",
    "            y_count = 0\n",
    "            s_indexed[y,1] = y_count\n",
    "        else:\n",
    "            y_count += 1\n",
    "            s_indexed[y,1] = y_count\n",
    "    else: \n",
    "        s_indexed[y,1] = -1\n",
    "        \n",
    "    y-=1\n",
    "\n",
    "# Sharp peaks algo\n",
    "\n",
    "# Manually determine some generous ranges for prominence, width, wlen, possibly threshold, possibly distance to segment out fluctuations, rel_height for basewidth\n",
    "# Prominence range: (2,None), rel_height = 1.0 (maybe .9 but not .5), width: (None, 20), wlen = 100 (to save on efficieny), distance = 5 (this will prevent fluctuations from being detected)\n",
    "# For distance we also have to consider that a sharp peak could occur mid event? \n",
    "# Threshold will not be useful it seems like\n",
    "prominence_range = [5,None]\n",
    "width_range = [None,None]\n",
    "wlen = 100\n",
    "distance = 1\n",
    "rel_height =.6\n",
    "\n",
    "# data = fDOM_raw_data[:100000][:,1]\n",
    "\n",
    "# Get list of all peaks that could possibly be sharp peaks\n",
    "fDOM_peaks, fDOM_props = find_peaks(fDOM_data[:,1],\n",
    "                          height = (None, None),\n",
    "                          threshold = (None,None),\n",
    "                          distance = distance,\n",
    "                          prominence = prominence_range,\n",
    "                          width = width_range,\n",
    "                          wlen = wlen,\n",
    "                          rel_height = rel_height)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ca4efb9c-d949-48ca-b75b-d0965d2a5dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "637\n"
     ]
    }
   ],
   "source": [
    "fDOM_cands = [[peak, math.floor(fDOM_props['left_ips'][i]), math.ceil(fDOM_props['right_ips'][i]),s_indexed[peak,0], s_indexed[peak,1] ,fDOM_props['prominences'][i]] for i,peak in enumerate(fDOM_peaks)]\n",
    "print(len(fDOM_cands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "51568e55-9265-43cb-a07a-e659fa488fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# Filter out skyrocketing peak candidates whose left_ips or right_ips are also a plummeting peak, as we don't want to detect these\n",
    "removed = []\n",
    "not_removed = []\n",
    "for cand in fDOM_cands: \n",
    "    if cand[1] in possible_plum_peaks or cand[2] in possible_plum_peaks:\n",
    "        removed.append(cand)\n",
    "    elif cand[1]+1 in possible_plum_peaks or cand[2]+1 in possible_plum_peaks:\n",
    "        removed.append(cand)\n",
    "    elif cand[1]-1 in possible_plum_peaks or cand[2]-1 in possible_plum_peaks:\n",
    "        removed.append(cand)\n",
    "    else:\n",
    "        not_removed.append(cand)\n",
    "        \n",
    "print(len(removed))\n",
    "fDOM_cands = not_removed\n",
    "print(len(not_removed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "92f411cd-910b-4676-a4db-04fb6f289bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "turb_cand_params = {'prom' : [6,None],\n",
    "                    'width': [None, None],\n",
    "                    'wlen' : 200,\n",
    "                    'dist' : 1,\n",
    "                    'rel_h': .6}\n",
    "\n",
    "# Get fDOM and turb candiate peaks\n",
    "turb_peaks, turb_props = get_candidates(turb_data, turb_cand_params)\n",
    "\n",
    "# Remove peaks that occur during a flat plateau \n",
    "turb_flat_plat = detect_flat_plat(turb_data, 100, 40)\n",
    "turb_flat_plat_indxs = []\n",
    "for i in range(turb_flat_plat.shape[0]):\n",
    "    if turb_flat_plat[i] == 1:\n",
    "        turb_flat_plat_indxs.append(i)\n",
    "\n",
    "take_indices = []\n",
    "for i,peak in enumerate(turb_peaks):\n",
    "    if peak not in turb_flat_plat_indxs:\n",
    "        take_indices.append(i)\n",
    "\n",
    "turb_peaks = np.take(turb_peaks, take_indices)\n",
    "for key in turb_props:\n",
    "    turb_props[key] = np.take(turb_props[key], take_indices)\n",
    "\n",
    "# Iterate through peaks and turn into short 3 point \"events\" by flagging the data point to either side of a peak\n",
    "fDOM_events = []\n",
    "fDOM_lb = []\n",
    "fDOM_rb = []\n",
    "\n",
    "for i,cand in enumerate(fDOM_cands):\n",
    "            fDOM_events.append(np.array((fDOM_data[cand[0]])))\n",
    "            fDOM_lb.append(fDOM_data[math.floor(cand[1]),0])\n",
    "            fDOM_rb.append(fDOM_data[math.ceil(cand[2]),0])\n",
    "            \n",
    "fDOM_lb = list(set(fDOM_lb))\n",
    "fDOM_lb.sort()\n",
    "fDOM_rb = list(set(fDOM_rb))\n",
    "fDOM_rb.sort()\n",
    "\n",
    "turb_events = []\n",
    "turb_lb = []\n",
    "turb_rb = []\n",
    "for i,peak in enumerate(turb_peaks):\n",
    "            turb_events.append(np.array((turb_data[peak])))\n",
    "            turb_lb.append(turb_data[math.floor(turb_props['left_ips'][i]),0])\n",
    "            turb_rb.append(turb_data[math.ceil(turb_props['right_ips'][i]),0])\n",
    "            \n",
    "turb_lb = list(set(turb_lb))\n",
    "turb_lb.sort()\n",
    "turb_rb = list(set(turb_rb))\n",
    "turb_rb.sort()            \n",
    "\n",
    "fDOM_merged = dp.merge_data(fDOM_data, fDOM_events, 'not_sky_peak', '')\n",
    "turb_merged = dp.merge_data(turb_data, turb_events, 't_opp', '')\n",
    "\n",
    "fDOM_merged = dp.merge_additional_data(fDOM_merged, fDOM_lb, 'left_base')\n",
    "fDOM_merged = dp.merge_additional_data(fDOM_merged, fDOM_rb, 'right_base')\n",
    "\n",
    "turb_merged = dp.merge_additional_data(turb_merged, turb_lb, 'left_base')\n",
    "turb_merged = dp.merge_additional_data(turb_merged, turb_rb, 'right_base')\n",
    "\n",
    "\n",
    "stage_edge_data = dp.stage_rises_to_data(s_indices, stage_data)\n",
    "stage_data_merged = dp.merge_data(stage_data, stage_edge_data, 'rise','')\n",
    "\n",
    "dm.write_data_to_trainset(fDOM_merged,\n",
    "                          stage_data_merged,\n",
    "                          turb_merged,\n",
    "                          '../Data/temp_plotting/fDOM_sky_0k-100k.csv',\n",
    "                          True,\n",
    "                          True,\n",
    "                          0,\n",
    "                          100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6038c5b7-42b0-4068-9631-d3398e769544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_sky_peaks_fDOM(candidates: list[np.ndarray], \n",
    "                            properties: list, prom_range: \n",
    "                            list[float], base_width_range: \n",
    "                            list[float]) -> list[list[list]]:\n",
    "    \"\"\"\n",
    "    Detect fDOM sharp peak anomalies from given set of possible candidates, given hyper parameters \n",
    "    \"\"\"\n",
    "    sharp_peaks = []\n",
    "    not_sharp_peaks = []\n",
    "    \n",
    "    for i, peak in candidates:\n",
    "        # Uses bases to calculate base_width. Or just use width. Is width a float or in\n",
    "        base_width = properties['right_bases'][i] - properties['left_bases'][i]\n",
    "        base_width = properties['widths'][i]\n",
    "        prominence = properties['prominences'][i]\n",
    "        start_timestamp = data[properties['left_bases'][i],0]\n",
    "        end_timestamp = data[properties['right_bases'][i],0]\n",
    "        peak_timestamp = data[peak][0]\n",
    "        \n",
    "        # Check if base_width violates\n",
    "        if ((base_width > base_width_range[0] and base_width < base_width_range[1])\n",
    "            and prominence > prom_range[0] and prominence < prom_range[1]):\n",
    "            \n",
    "            # Append [start, end, peak, label]\n",
    "            sharp_peaks.append([start_timestamp,end_timestamp,peak_timestamp,'sharp_peak_fDOM'])\n",
    "            \n",
    "        else: \n",
    "            not_sharp_peaks.append([start_timestamp,end_timestamp,peak_timestamp,'rejected_sharp_peak_fDOM'])\n",
    "    return [sharp_peaks, not_sharp_peaks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc6c6500-881b-401e-871f-17fc7ab7bd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hh/m6vvts714wdcv6sngjwj41zr0000gn/T/ipykernel_1671/2046036813.py:16: PeakPropertyWarning: some peaks have a prominence of 0\n",
      "  possible_plum_peaks, props = find_peaks(data,\n"
     ]
    }
   ],
   "source": [
    "# flip timeseries \n",
    "flipped_fDOM = np.array(fDOM_data)\n",
    "flipped_fDOM = dp.flip_timeseries(flipped_fDOM)\n",
    "\n",
    "# get candidates with find_peaks \n",
    "prominence_range_plum = [3,None]\n",
    "width_range_plum = [None,5]\n",
    "wlen_plum = 100\n",
    "distance_plum = 1 \n",
    "rel_height_plum =.6\n",
    "\n",
    "# data = fDOM_raw_data[:100000][:,1]\n",
    "data = flipped_fDOM[:,1]\n",
    "\n",
    "# Get list of all peaks that could possibly be plummeting peaks\n",
    "possible_plum_peaks, props = find_peaks(data,\n",
    "                          height = (None, None),\n",
    "                          threshold = (None,None),\n",
    "                          distance = distance_plum,\n",
    "                          prominence = prominence_range_plum,\n",
    "                          width = width_range_plum,\n",
    "                          wlen = wlen_plum,\n",
    "                          rel_height = rel_height_plum)\n",
    "\n",
    "fDOM_cands = [[peak, math.floor(props['left_ips'][i]), math.ceil(props['right_ips'][i]),s_indexed[peak,0], s_indexed[peak,1] ,props['prominences'][i]] for i,peak in enumerate(possible_plum_peaks)]\n",
    "\n",
    "# get list of turb_peaks: these turb peaks look similar to skyrocketing peaks in fDOM. \n",
    "# prominence_range_turb = [2,None]\n",
    "# width_range_turb = [None,10]\n",
    "# wlen_turb = 100\n",
    "# distance_turb = 1 \n",
    "# rel_height_turb =.8\n",
    "\n",
    "# data = fDOM_raw_data[:100000][:,1]\n",
    "\n",
    "# # Get list of all peaks that could possibly be sharp peaks\n",
    "# turb_peaks, props = find_peaks(data,\n",
    "#                           height = (None, None),\n",
    "#                           threshold = (None,None),\n",
    "#                           distance = distance_turb,\n",
    "#                           prominence = prominence_range_turb,\n",
    "#                           width = width_range_turb,\n",
    "#                           wlen = wlen_turb,\n",
    "#                           rel_height = rel_height_turb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3522b40-4400-4494-8208-7a7c5466c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "turb_cand_params = {'prom' : [6,None],\n",
    "                    'width': [None, None],\n",
    "                    'wlen' : 200,\n",
    "                    'dist' : 1,\n",
    "                    'rel_h': .6}\n",
    "\n",
    "# Get fDOM and turb candiate peaks\n",
    "turb_peaks, turb_props = get_candidates(turb_data, turb_cand_params)\n",
    "\n",
    "# Remove peaks that occur during a flat plateau \n",
    "turb_flat_plat = detect_flat_plat(turb_data, 100, 40)\n",
    "turb_flat_plat_indxs = []\n",
    "for i in range(turb_flat_plat.shape[0]):\n",
    "    if turb_flat_plat[i] == 1:\n",
    "        turb_flat_plat_indxs.append(i)\n",
    "\n",
    "take_indices = []\n",
    "for i,peak in enumerate(turb_peaks):\n",
    "    if peak not in turb_flat_plat_indxs:\n",
    "        take_indices.append(i)\n",
    "\n",
    "turb_peaks = np.take(turb_peaks, take_indices)\n",
    "for key in turb_props:\n",
    "    turb_props[key] = np.take(turb_props[key], take_indices)\n",
    "\n",
    "# Iterate through peaks and turn into short 3 point \"events\" by flagging the data point to either side of a peak\n",
    "fDOM_events = []\n",
    "fDOM_lb = []\n",
    "fDOM_rb = []\n",
    "\n",
    "for i,cand in enumerate(fDOM_cands):\n",
    "            fDOM_events.append(np.array((flipped_fDOM[cand[0]])))\n",
    "            fDOM_lb.append(flipped_fDOM[math.floor(cand[1]),0])\n",
    "            fDOM_rb.append(flipped_fDOM[math.ceil(cand[2]),0])\n",
    "            \n",
    "fDOM_lb = list(set(fDOM_lb))\n",
    "fDOM_lb.sort()\n",
    "fDOM_rb = list(set(fDOM_rb))\n",
    "fDOM_rb.sort()\n",
    "\n",
    "turb_events = []\n",
    "turb_lb = []\n",
    "turb_rb = []\n",
    "for i,peak in enumerate(turb_peaks):\n",
    "            turb_events.append(np.array((turb_data[peak])))\n",
    "            turb_lb.append(turb_data[math.floor(turb_props['left_ips'][i]),0])\n",
    "            turb_rb.append(turb_data[math.ceil(turb_props['right_ips'][i]),0])\n",
    "            \n",
    "turb_lb = list(set(turb_lb))\n",
    "turb_lb.sort()\n",
    "turb_rb = list(set(turb_rb))\n",
    "turb_rb.sort()            \n",
    "\n",
    "fDOM_merged = dp.merge_data(flipped_fDOM, fDOM_events, 'f_opp', '')\n",
    "turb_merged = dp.merge_data(turb_data, turb_events, 't_opp', '')\n",
    "\n",
    "fDOM_merged = dp.merge_additional_data(fDOM_merged, fDOM_lb, 'left_base')\n",
    "fDOM_merged = dp.merge_additional_data(fDOM_merged, fDOM_rb, 'right_base')\n",
    "\n",
    "turb_merged = dp.merge_additional_data(turb_merged, turb_lb, 'left_base')\n",
    "turb_merged = dp.merge_additional_data(turb_merged, turb_rb, 'right_base')\n",
    "\n",
    "\n",
    "stage_edge_data = dp.stage_rises_to_data(s_indices, stage_data)\n",
    "stage_data_merged = dp.merge_data(stage_data, stage_edge_data, 'rise','')\n",
    "\n",
    "dm.write_data_to_trainset(fDOM_merged,\n",
    "                          stage_data_merged,\n",
    "                          dp.merge_data(fDOM_data, [], '',''),\n",
    "                          '../Data/temp_plotting/fDOM_plum_0k-100k.csv',\n",
    "                          True,\n",
    "                          True,\n",
    "                          0,\n",
    "                          100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59460771-1a1a-4d5d-b022-bcfb2974edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "julian_hour = 0.04166666651144624\n",
    "interference_hour_range = [julian_hour * 2]\n",
    "prom_range = []\n",
    "base_width_range = []\n",
    "\n",
    "plummeting_peaks, not_plummeting_peaks = detect_plum_peaks_fDOM(possible_plum_peaks,\n",
    "                                                                turb_peaks,\n",
    "                                                                interference_hour_range,\n",
    "                                                                props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff23cff-1df6-47c5-aa8f-fd0b2f1eddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get turb peaks \n",
    "def detect_plum_peaks_fDOM(candidates: list[np.ndarray],\n",
    "                           turb_peaks: list[list[list]],\n",
    "                           interference_hour_range: list[int]\n",
    "                           properties: list, \n",
    "                           prom_range: list[float], \n",
    "                           base_width_range: list[float]):\n",
    "    \"\"\"\n",
    "    Detect fDOM plummeting peak anomalies from given set of possible candidates, given hyper parameters and turb peaks\n",
    "    \"\"\"\n",
    "    plum_peaks = []\n",
    "    not_plum_peaks = []\n",
    "    interference_peaks = [] # label these so we can see if it is rejecting correctly\n",
    "    \n",
    "    for i, peak in candidates:\n",
    "        # Uses bases to calculate base_width. Or just use width. Is width a float or in\n",
    "        base_width = properties['right_bases'][i] - properties['left_bases'][i]\n",
    "        base_width = properties['widths'][i]\n",
    "        prominence = properties['prominences'][i]\n",
    "        start_timestamp = data[properties['left_bases'][i],0]\n",
    "        end_timestamp = data[properties['right_bases'][i],0]\n",
    "        peak_timestamp = data[peak][0]\n",
    "        \n",
    "        # Check if base_width violates\n",
    "        if ((base_width > base_width_range[0] and base_width < base_width_range[1])\n",
    "            and prominence > prom_range[0] and prominence < prom_range[1]):\n",
    "            \n",
    "            # Check for interference\n",
    "            for peak in turb_peaks: \n",
    "                # Check if fDOM peak is within x hours of turb peak\n",
    "                if condition: \n",
    "                    # Append to interference_peaks\n",
    "                    break\n",
    "            # Append [start, end, peak, label]\n",
    "            plum_peaks.append([start_timestamp,end_timestamp,peak_timestamp,'plum_peak_fDOM'])\n",
    "            \n",
    "        else: \n",
    "            not_sharp_peaks.append([start_timestamp,end_timestamp,peak_timestamp,'rejected_plum_peak_fDOM'])\n",
    "    return sharp_peaks, not_sharp_peaks\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d82bf2-3cd6-4934-ada8-2d02a28afbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plum_peaks = []\n",
    "def filter_fDOM_cands(peaks, props,fDOM_data):\n",
    "    \"\"\"\n",
    "    Filter out candidates with an extremely small ending slope \n",
    "    Filter out candidates whose start and end occur on a plummenting peak/interference\n",
    "    \"\"\"\n",
    "    take_indices = []\n",
    "    \n",
    "    end_thresh = .2\n",
    "    for i, peak in enumerate(peaks): \n",
    "        if peak > 100000:\n",
    "            break\n",
    "        end_run = abs(peak - math.ceil(props['right_ips'][i]))\n",
    "        end_rise = abs(fDOM_data[peak,1] - fDOM_data[math.ceil(props['right_ips'][i]),1])  \n",
    "        end_slope = end_rise/end_run\n",
    "\n",
    "        if end_slope < end_thresh:\n",
    "            print('Date: {} ES: {}  Prom: {}'.format(dp.julian_to_datetime(fDOM_data[peak,0]), round(end_slope,3),round(props['prominences'][i],2)))\n",
    "            print('Left IPS: {}'.format(dp.julian_to_datetime(fDOM_data[math.ceil(props['right_ips'][i]),0])))\n",
    "            print('\\n')\n",
    "        \n",
    "        if math.ceil(props['right_ips'][i]) in plum_peaks and math.floor(props['left_ips'][i]) in plum_peaks:\n",
    "            print('DQP: Data: {}'.format(dp.julian_to_datetime(fDOM_data[peak,0])))\n",
    "        \n",
    "filter_fDOM_cands(fDOM_peaks, fDOM_props, fDOM_raw_data)\n",
    "\n",
    "flipped_fDOM = np.array(fDOM_raw_data)\n",
    "flipped_fDOM = dp.flip_timeseries(flipped_fDOM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
