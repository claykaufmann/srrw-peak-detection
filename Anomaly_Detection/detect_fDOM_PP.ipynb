{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0e86851-71cb-4325-9962-e4c30f9b978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and data \n",
    "import scipy.io as sio\n",
    "from scipy import ndimage\n",
    "import copy\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from os.path import dirname, join as pjoin\n",
    "import datetime\n",
    "import csv\n",
    "import math\n",
    "import sys\n",
    "sys.path.insert(1,'../')\n",
    "import Tools.data_processing as dp\n",
    "import Tools.data_movement as dm \n",
    "from auxiliary_functions import get_candidates, detect_flat_plat, detect_stage_rises\n",
    "\n",
    "fDOM_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv')\n",
    "stage_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/stage_10.1.11-1.1.19.csv')\n",
    "turb_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv')\n",
    "stage_data = dp.align_stage_to_fDOM(fDOM_data, stage_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee3e758a-24ed-4eb5-93b8-ab315d4ff78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fDOM_data_copy = copy.deepcopy(fDOM_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6001492-7035-4d9f-b917-289207e7634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fDOM_data = fDOM_data_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dd1dd0-e3aa-44fa-a68c-c1967b14018d",
   "metadata": {},
   "source": [
    "# Attempt to smooth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fded6ec8-7ab0-4b9d-a0d0-e21ff0f1e869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_pass_filter(data, window_len):\n",
    "    \"\"\"\n",
    "    (2 * window_len) + 1 is the size of the window that determines the values that \n",
    "    influence the current measurement (middle of window)\n",
    "    \"\"\"\n",
    "    kernel = np.lib.pad(np.linspace(1,3,window_len), (0,window_len -1), 'reflect')\n",
    "    kernel = np.divide(kernel, np.sum(kernel))\n",
    "    return ndimage.convolve(data, kernel)\n",
    "\n",
    "smoothed_signal = low_pass_filter(fDOM_data[:,1], 7)\n",
    "fDOM_data = np.column_stack((fDOM_data[:,0],smoothed_signal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c75a7-266d-427e-93b2-c29511391080",
   "metadata": {},
   "source": [
    "### Detect and process stage rises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c0d3353-8353-43ff-8906-ec0c5d5e17fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stage rises\n",
    "s_indices = detect_stage_rises(stage_data[:,1])\n",
    "\n",
    "# Process stage rises so that each index displays distance to next stage rise in positive and negative direction\n",
    "y = s_indices.shape[0] -1 \n",
    "s_indexed = np.zeros((s_indices.shape[0],2))\n",
    "x_count = -1 \n",
    "y_count = -1\n",
    "for x in range(s_indices.shape[0]):\n",
    "    # X Block \n",
    "    \n",
    "    # When x encounters first stage rise, start x counter\n",
    "    if x_count == -1 and s_indices[x] == 1:\n",
    "        x_count = 0\n",
    "    if x_count != -1:\n",
    "        if s_indices[x] == 1:\n",
    "            x_count = 0\n",
    "            s_indexed[x,0] = x_count\n",
    "        else:\n",
    "            x_count += 1\n",
    "            s_indexed[x,0] = x_count\n",
    "    else:\n",
    "        s_indexed[x,0] = -1\n",
    "            \n",
    "    # Y Block\n",
    "    if y_count == -1 and s_indices[y] == 1:\n",
    "        y_count = 0\n",
    "    if y_count != -1:\n",
    "        if s_indices[y] == 1:\n",
    "            y_count = 0\n",
    "            s_indexed[y,1] = y_count\n",
    "        else:\n",
    "            y_count += 1\n",
    "            s_indexed[y,1] = y_count\n",
    "    else: \n",
    "        s_indexed[y,1] = -1\n",
    "        \n",
    "    y-=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1666c9-1294-4df9-96fc-68af6e6281ae",
   "metadata": {},
   "source": [
    "### Get optimal fDOM candidate sets\n",
    "\n",
    "Methods to reduce fDOM candidate: <br>\n",
    "\n",
    "- Combat detecting shallow peaks by making two calls to find_peaks with different parameters\n",
    "\n",
    "- Combat detecting \"peaks\" formed by plummeting peaks by not considering peaks where both start and end are significantly below floating mean of data within recent past\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11bb57a1-a0f8-44eb-97e3-34f18e86ca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# large_peaks_params = {'prom' : [8,None],\n",
    "#                     'width': [15, None],\n",
    "#                     'wlen' : 200,\n",
    "#                     'dist' : 1,\n",
    "#                     'rel_h': .6}\n",
    "\n",
    "# small_peaks_params = {'prom' : [3,None],\n",
    "#                     'width': [0, 15],\n",
    "#                     'wlen' : 200,\n",
    "#                     'dist' : 1,\n",
    "#                     'rel_h': .6}\n",
    "\n",
    "# large_peaks_params = {'prom' : [8,None],\n",
    "#                     'width': [50, None],\n",
    "#                     'wlen' : 200,\n",
    "#                     'dist' : 1,\n",
    "#                     'rel_h': .6}\n",
    "\n",
    "small_peaks_params = {'prom' : [3,None],\n",
    "                    'width': [None, None],\n",
    "                    'wlen' : 200,\n",
    "                    'dist' : 1,\n",
    "                    'rel_h': .6}\n",
    "\n",
    "remove_ranges = [[17816, 17849], [108170,108200],[111364, 111381]]\n",
    "def isInRange(indx):\n",
    "    for rng in remove_ranges:\n",
    "        if rng[0] <= indx and indx <= rng[1]:\n",
    "#             print(dp.julian_to_datetime(fDOM_data[indx,0]))\n",
    "            return True\n",
    "    return False \n",
    "\n",
    "# large_peaks, large_props = get_candidates(fDOM_data, large_peaks_params)\n",
    "fDOM_peaks, fDOM_props = get_candidates(fDOM_data, small_peaks_params)\n",
    "\n",
    "# Remove peaks from missing data error range: 12-10-27 14:00:00 -> 12-10-31 18:00 \n",
    "\n",
    "# Peaks will be deleted, so get the relevant properties right now\n",
    "# temp_large = []\n",
    "# large_indices = set()\n",
    "# for i in range(len(large_peaks)):\n",
    "#     temp_large.append([large_peaks[i], large_props['prominences'][i], large_props['widths'][i], large_props['left_ips'][i],large_props['right_ips'][i]])\n",
    "#     large_indices.add(large_peaks[i])\n",
    "\n",
    "fDOM_cands = [[peak, math.floor(fDOM_props['left_ips'][i]), math.ceil(fDOM_props['right_ips'][i]),s_indexed[peak,0], s_indexed[peak,1] ,fDOM_props['prominences'][i]] for i,peak in enumerate(fDOM_peaks)]\n",
    "# large_peaks = temp_large\n",
    "# turb_cand = [[peak, math.floor(turb_props['left_ips'][i]), math.ceil(turb_props['right_ips'][i]),s_indexed[peak,0], s_indexed[peak,1] ,turb_props['prominences'][i]] for i,peak in enumerate(turb_peaks)]\n",
    "# temp_small = []\n",
    "# small_indices = set()\n",
    "# for i in range(len(small_peaks)):\n",
    "#     temp_small.append([small_peaks[i], math.floor(small_props['left_ips'][i]), math.ceil(small_props['right_ips'][i]), small_props['left_ips'][i],small_props['right_ips'][i]])\n",
    "#     small_indices.add(small_peaks[i])\n",
    "# small_peaks = temp_small\n",
    "# temp_small = []\n",
    "# small_indices = set()\n",
    "# for i in range(len(small_peaks)):\n",
    "#     temp_small.append([small_peaks[i], small_props['prominences'][i], small_props['widths'][i], small_props['left_bases'][i],small_props['right_bases'][i]])\n",
    "#     small_indices.add(small_peaks[i])\n",
    "# small_peaks = temp_small\n",
    "\n",
    "# Combine both lists of peak, if there are duplicates, prefer the information from the larger peak\n",
    "# del_indices = large_indices.intersection(small_indices)\n",
    "# if len(del_indices):\n",
    "#     print(len(del_indices))\n",
    "#     raise Exception(\"Sets should not overlap\")\n",
    "    \n",
    "# fDOM_cands = large_peaks + small_peaks\n",
    "# fDOM_cands = small_peaks\n",
    "\n",
    "# def sortCands(cand):\n",
    "#     return cand[0]\n",
    "# fDOM_cands.sort(key = sortCands)\n",
    "\n",
    "# print(len(fDOM_cands))\n",
    "\n",
    "temp = []\n",
    "for peak in fDOM_cands:\n",
    "    if not(isInRange(peak[0])):\n",
    "        temp.append(peak)\n",
    "fDOM_cands = copy.deepcopy(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b22e4a0-77ed-4a37-bc3c-18429f0f613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert peaks and props to useable structure and assign values from s_indexed\n",
    "disp_peaks = fDOM_cands\n",
    "fDOM_cands = [[peak[0], peak[3],peak[4],s_indexed[peak[0],0], s_indexed[peak[0],1]] for peak in disp_peaks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2254e7-ac0d-41d9-8b5a-c8f2ce513df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ground truth values \n",
    "truth_fname = '../Data/labeled_data/ground_truths/turb/turb_pp/julian_time/fDOM_pp_0k-300k_labeled'\n",
    "\n",
    "with open(truth_fname, 'r', newline = '') as f:\n",
    "    \n",
    "        reader = csv.reader(f, delimiter = ',')\n",
    "        # truth entries in form: ['timestamp_of_peak', 'value_of_peak','label_of_peak','idx_of_peak']\n",
    "        next(reader)\n",
    "        truths = [[float(row[0]), float(row[1]), row[2], int(row[3])] for row in reader] \n",
    "        f.close()   \n",
    "\n",
    "assert(len(truths) == len(fDOM_cand))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d48dc3-2d1a-41fe-8266-4a53cca70520",
   "metadata": {},
   "source": [
    "# Nested Cross Validation for fDOM PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "64dcebd8-ed69-49fb-b6c1-a9ed33899e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562\n"
     ]
    }
   ],
   "source": [
    "# Define Hyperparameter ranges and training parameters and helper function\n",
    "iterations = 7000\n",
    "num_splits = 5\n",
    "\n",
    "x_bounds = [0, 30]\n",
    "y_bounds = [0, 30]\n",
    "\n",
    "\n",
    "accumulated_test_metrics = {}\n",
    "\n",
    "accumulated_test_results = {}\n",
    "\n",
    "accumulated_best_params = {}\n",
    "\n",
    "train_test_split_indices = TimeSeriesSplit(num_splits).split(fDOM_cand)\n",
    "\n",
    "\n",
    "def classify_fDOM_peaks(peaks, params):\n",
    "    results = []\n",
    "    for peak in peaks:\n",
    "        if(peak[3] != -1 and peak[3] <= params['x']) or (peak[4] !=-1 and peak[4] <= params['y']):\n",
    "            results.append([peak[0], 'NPP'])\n",
    "        else: \n",
    "            results.append([peak[0], 'PP'])\n",
    "    return results\n",
    "\n",
    "def label_positives_negatives(predictions, truths):\n",
    "    TP = TN = FP = FN = 0\n",
    "    results = []\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        prediction = predictions[i][1]\n",
    "        truth = truths[i][2]\n",
    "        \n",
    "        if prediction == 'NPP':\n",
    "            if truth == 'NPP':\n",
    "                TN +=1\n",
    "                results.append(predictions[i].append('TN'))\n",
    "            else: \n",
    "                FN+=1\n",
    "                results.append(predictions[i].append('FN'))\n",
    "        else:\n",
    "            if truth == 'NPP':\n",
    "                FP+=1\n",
    "                results.append(predictions[i].append('FP'))\n",
    "            else: #TODO: Comeback and evaluate if this makes sense: Algo could predict PP because X/Y was not optimal, while truth was PPP because of interference\n",
    "                TP+=1\n",
    "                results.append(predictions[i].append('TP'))\n",
    "        \n",
    "    return (TP,TN,FP,FN,results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be65e711-dbf8-47b6-9023-b4930412589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_start = datetime.datetime.now()\n",
    "\n",
    "split = 1\n",
    "\n",
    "for train_val_indices, test_indices in train_test_split_indices:\n",
    "    X_train, y_train = [fDOM_cand[i] for i in train_val_indices], [truths[i] for i in train_val_indices]\n",
    "    X_test, y_test = [fDOM_cand[i] for i in test_indices], [truths[i] for i in test_indices]\n",
    "    \n",
    "    max_fold_metric = 0 \n",
    "    max_result = None \n",
    "    \n",
    "    print(\"Split: \",split)\n",
    "    \n",
    "    split_start = datetime.datetime.now()\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        \n",
    "        # Random grid search for hyperparams \n",
    "        params = {}\n",
    "        \n",
    "        params['x'] = np.random.randint(x_bounds[0], x_bounds[1]+1)\n",
    "        params['y'] = np.random.randint(y_bounds[0], y_bounds[1]+1)\n",
    "                \n",
    "        predictions = classify_fDOM_peaks(X_train, params)\n",
    "        \n",
    "        TP,TN,FP,FN,results = label_positives_negatives(predictions, y_train)\n",
    "        \n",
    "        TPR = TP/(TP + FN)\n",
    "        TNR = TN/(TN + FP)\n",
    "        \n",
    "        bal_acc = (TPR + TNR)/2\n",
    "        \n",
    "        f1_score = (2 * TP)/((2 * TP) + FP + FN)\n",
    "\n",
    "        acc = f1_score\n",
    "        if iteration and iteration % int(iterations/10) == 0: print(\" {}/{} \".format(iteration, iterations), end = \"\")\n",
    "        if acc > max_fold_metric: \n",
    "            max_fold_metric = acc\n",
    "            max_result = copy.deepcopy(results)\n",
    "            best_params = copy.deepcopy(params)\n",
    "            \n",
    "    # Test best parameters on testing data \n",
    "    test_predictions = classify_fDOM_peaks(X_test, best_params)\n",
    "    TP,TN,FP,FN,results = label_positives_negatives(test_predictions, y_test)\n",
    "    \n",
    "    TPR = TP/(TP + FN)\n",
    "    TNR = TN/(TN + FP)\n",
    "    \n",
    "    bal_acc = (TPR + TNR)/2 \n",
    "    f1_score = (2 * TP)/((2 * TP) + FP + FN)\n",
    "    \n",
    "    print('\\nSplit: {}  F1: {:.4f} BA: {:.4f}  Params: {}  TP: {} TN: {} FP: {} FN: {}  Time: {}'.format(split, f1_score, bal_acc, best_params, TP, TN, FP, FN, datetime.datetime.now() - split_start))\n",
    "    accumulated_test_metrics[split] = [f1_score, bal_acc] # Record test metrics of each split\n",
    "    accumulated_test_results[split] = copy.deepcopy(results) # Record test results (FP,FN,TP,TN for each datapoint) for each split\n",
    "    accumulated_best_params[split] = copy.deepcopy(best_params) # Record params uses in testing for each split\n",
    "    \n",
    "    split+=1\n",
    "\n",
    "# Display Resuls \n",
    "mean_f1 = 0\n",
    "mean_ba = 0 \n",
    "\n",
    "for key in accumulated_test_metrics:\n",
    "    metrics = accumulated_test_metrics[key]\n",
    "    mean_f1+=metrics[0]\n",
    "    mean_ba+=metrics[1]\n",
    "\n",
    "print(\"Mean Test F1: \", mean_f1/len(accumulated_test_metrics))\n",
    "print(\"Mean Test BA: \", mean_ba/len(accumulated_test_metrics))\n",
    "\n",
    "print(\"Training time: \", datetime.datetime.now() - overall_start)\n",
    "\n",
    "# Pickle params from last fold\n",
    "with open('./Experiments/fDOM_PP/best_params.pkl', 'wb') as pck_file:\n",
    "    pickle.dump(accumulated_best_params[num_splits], pck_file)\n",
    "    pck_file.close()\n",
    "    \n",
    "# Pickle results from last fold \n",
    "with open('./Experiments/fDOM_PP/test_results.pkl', 'wb') as pck_file:\n",
    "    pickle.dump(accumulated_test_results, pck_file)\n",
    "    pck_file.close()\n",
    "\n",
    "# Pickle results from last fold \n",
    "with open('./Experiments/fDOM_PP/test_metrics.pkl', 'wb') as pck_file:\n",
    "    pickle.dump(accumulated_test_metrics, pck_file)\n",
    "    pck_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca78d92d-5349-4a54-bade-8bea572d3347",
   "metadata": {},
   "outputs": [],
   "source": [
    "turb_cand_params = {'prom' : [6,None],\n",
    "                    'width': [None, None],\n",
    "                    'wlen' : 200,\n",
    "                    'dist' : 1,\n",
    "                    'rel_h': .6}\n",
    "\n",
    "# Get fDOM and turb candiate peaks\n",
    "turb_peaks, turb_props = get_candidates(turb_data, turb_cand_params)\n",
    "\n",
    "# Remove peaks that occur during a flat plateau \n",
    "turb_flat_plat = detect_flat_plat(turb_data, 100, 40)\n",
    "turb_flat_plat_indxs = []\n",
    "for i in range(turb_flat_plat.shape[0]):\n",
    "    if turb_flat_plat[i] == 1:\n",
    "        turb_flat_plat_indxs.append(i)\n",
    "\n",
    "take_indices = []\n",
    "for i,peak in enumerate(turb_peaks):\n",
    "    if peak not in turb_flat_plat_indxs:\n",
    "        take_indices.append(i)\n",
    "\n",
    "turb_peaks = np.take(turb_peaks, take_indices)\n",
    "for key in turb_props:\n",
    "    turb_props[key] = np.take(turb_props[key], take_indices)\n",
    "\n",
    "# Iterate through peaks and turn into short 3 point \"events\" by flagging the data point to either side of a peak\n",
    "fDOM_events = []\n",
    "fDOM_lb = []\n",
    "fDOM_rb = []\n",
    "\n",
    "for i,cand in enumerate(fDOM_cands):\n",
    "            fDOM_events.append(np.array((fDOM_data[cand[0]])))\n",
    "            fDOM_lb.append(fDOM_data[math.floor(cand[1]),0])\n",
    "            fDOM_rb.append(fDOM_data[math.ceil(cand[2]),0])\n",
    "            \n",
    "fDOM_lb = list(set(fDOM_lb))\n",
    "fDOM_lb.sort()\n",
    "fDOM_rb = list(set(fDOM_rb))\n",
    "fDOM_rb.sort()\n",
    "\n",
    "turb_events = []\n",
    "turb_lb = []\n",
    "turb_rb = []\n",
    "for i,peak in enumerate(turb_peaks):\n",
    "            turb_events.append(np.array((turb_data[peak])))\n",
    "            turb_lb.append(turb_data[math.floor(turb_props['left_ips'][i]),0])\n",
    "            turb_rb.append(turb_data[math.ceil(turb_props['right_ips'][i]),0])\n",
    "            \n",
    "turb_lb = list(set(turb_lb))\n",
    "turb_lb.sort()\n",
    "turb_rb = list(set(turb_rb))\n",
    "turb_rb.sort()            \n",
    "\n",
    "fDOM_merged = dp.merge_data(fDOM_data, fDOM_events, 'f_opp', '')\n",
    "turb_merged = dp.merge_data(turb_data, turb_events, 't_opp', '')\n",
    "\n",
    "fDOM_merged = dp.merge_additional_data(fDOM_merged, fDOM_lb, 'left_base')\n",
    "fDOM_merged = dp.merge_additional_data(fDOM_merged, fDOM_rb, 'right_base')\n",
    "\n",
    "turb_merged = dp.merge_additional_data(turb_merged, turb_lb, 'left_base')\n",
    "turb_merged = dp.merge_additional_data(turb_merged, turb_rb, 'right_base')\n",
    "\n",
    "\n",
    "stage_edge_data = dp.stage_rises_to_data(s_indices, stage_data)\n",
    "stage_data_merged = dp.merge_data(stage_data, stage_edge_data, 'rise','')\n",
    "\n",
    "dm.write_data_to_trainset(fDOM_merged,\n",
    "                          stage_data_merged,\n",
    "                          dp.merge_data(fDOM_data_copy, [], '',''),\n",
    "                          '../Data/temp_plotting/fDOM_PP_smoothed_200k-300k.csv',\n",
    "                          True,\n",
    "                          True,\n",
    "                          200000,\n",
    "                          300000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
