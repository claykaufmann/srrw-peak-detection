{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f22bf48d-ec15-4db9-87a1-4f85d9a7d545",
   "metadata": {},
   "source": [
    "# Detect fDOM Phantom Peaks\n",
    "\n",
    "## Rules for fDOM Phantom Peaks\n",
    "\n",
    "Upward peak where: \n",
    "- There is a corresponding preceding or succeeding stage rise in a given interval\n",
    "- The peak has a prominence to basewidth ratio above a certain threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0e86851-71cb-4325-9962-e4c30f9b978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and data \n",
    "import scipy.io as sio\n",
    "import matplotlib as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import copy\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from os.path import dirname, join as pjoin\n",
    "import datetime\n",
    "import csv\n",
    "import math\n",
    "import sys\n",
    "sys.path.insert(1,'../')\n",
    "import Tools.data_processing as dp\n",
    "import Tools.data_movement as dm \n",
    "from auxiliary_functions import get_candidates, detect_flat_plat, detect_stage_rises\n",
    "\n",
    "fDOM_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv')\n",
    "stage_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/stage_10.1.11-1.1.19.csv')\n",
    "turb_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv')\n",
    "stage_data = dp.align_stage_to_fDOM(fDOM_data, stage_data)\n",
    "fDOM_data_unsmoothed = copy.deepcopy(fDOM_data)\n",
    "# fDOM_data = fDOM_data_unsmoothed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dd1dd0-e3aa-44fa-a68c-c1967b14018d",
   "metadata": {},
   "source": [
    "# Attempt to smooth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fded6ec8-7ab0-4b9d-a0d0-e21ff0f1e869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_pass_filter(data, window_len):\n",
    "    \"\"\"\n",
    "    (2 * window_len) + 1 is the size of the window that determines the values that \n",
    "    influence the current measurement (middle of window)\n",
    "    \"\"\"\n",
    "    kernel = np.lib.pad(np.linspace(1,3,window_len), (0,window_len -1), 'reflect')\n",
    "    kernel = np.divide(kernel, np.sum(kernel))\n",
    "    return ndimage.convolve(data, kernel)\n",
    "\n",
    "smoothed_signal = low_pass_filter(fDOM_data[:,1], 7)\n",
    "fDOM_data = np.column_stack((fDOM_data[:,0],smoothed_signal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c75a7-266d-427e-93b2-c29511391080",
   "metadata": {},
   "source": [
    "### Detect and process stage rises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c0d3353-8353-43ff-8906-ec0c5d5e17fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stage rises\n",
    "s_indices = detect_stage_rises(stage_data[:,1])\n",
    "\n",
    "# Process stage rises so that each index displays distance to next stage rise in positive and negative direction\n",
    "y = s_indices.shape[0] -1 \n",
    "s_indexed = np.zeros((s_indices.shape[0],2))\n",
    "x_count = -1 \n",
    "y_count = -1\n",
    "for x in range(s_indices.shape[0]):\n",
    "    # X Block \n",
    "    \n",
    "    # When x encounters first stage rise, start x counter\n",
    "    if x_count == -1 and s_indices[x] == 1:\n",
    "        x_count = 0\n",
    "    if x_count != -1:\n",
    "        if s_indices[x] == 1:\n",
    "            x_count = 0\n",
    "            s_indexed[x,0] = x_count\n",
    "        else:\n",
    "            x_count += 1\n",
    "            s_indexed[x,0] = x_count\n",
    "    else:\n",
    "        s_indexed[x,0] = -1\n",
    "            \n",
    "    # Y Block\n",
    "    if y_count == -1 and s_indices[y] == 1:\n",
    "        y_count = 0\n",
    "    if y_count != -1:\n",
    "        if s_indices[y] == 1:\n",
    "            y_count = 0\n",
    "            s_indexed[y,1] = y_count\n",
    "        else:\n",
    "            y_count += 1\n",
    "            s_indexed[y,1] = y_count\n",
    "    else: \n",
    "        s_indexed[y,1] = -1\n",
    "        \n",
    "    y-=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1666c9-1294-4df9-96fc-68af6e6281ae",
   "metadata": {},
   "source": [
    "## Get fDOM PP candidate set\n",
    "\n",
    "There are areas of missing data that cause peaks to be detected where there are no peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11bb57a1-a0f8-44eb-97e3-34f18e86ca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_params = {'prom' : [3,None],\n",
    "                    'width': [None, None],\n",
    "                    'wlen' : 200,\n",
    "                    'dist' : 1,\n",
    "                    'rel_h': .6}\n",
    "\n",
    "remove_ranges = [[17816, 17849], [108170,108200],[111364, 111381]]\n",
    "\n",
    "def isInRange(indx):\n",
    "    for rng in remove_ranges:\n",
    "        if rng[0] <= indx and indx <= rng[1]:\n",
    "            return True\n",
    "    return False \n",
    "\n",
    "peaks, props = get_candidates(fDOM_data, candidate_params)\n",
    "\n",
    "cands = [[peak, math.floor(props['left_ips'][i]), math.ceil(props['right_ips'][i]),s_indexed[peak,0], s_indexed[peak,1] ,props['prominences'][i]] for i,peak in enumerate(peaks)]\n",
    "\n",
    "# Remove erroneously detected peaks\n",
    "temp = []\n",
    "for peak in cands:\n",
    "    if not(isInRange(peak[0])):\n",
    "        temp.append(peak)\n",
    "cands = copy.deepcopy(temp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e11fbcf-7390-4995-b2f2-171dabe998ce",
   "metadata": {},
   "source": [
    "## Import ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b2254e7-ac0d-41d9-8b5a-c8f2ce513df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ground truth values \n",
    "truth_fname = '../Data/labeled_data/ground_truths/fDOM/fDOM_PP/julian_time/fDOM_PP_0k-300k.csv'\n",
    "\n",
    "with open(truth_fname, 'r', newline = '') as f:\n",
    "    \n",
    "        reader = csv.reader(f, delimiter = ',')\n",
    "        # truth entries in form: ['timestamp_of_peak', 'value_of_peak','label_of_peak','idx_of_peak']\n",
    "        next(reader)\n",
    "        truths = [[float(row[0]), float(row[1]), row[2], int(row[3])] for row in reader] \n",
    "        f.close()   \n",
    "\n",
    "assert(len(truths) == len(cands))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4190e3d9-73c8-45f3-ab17-1e354f5f26ab",
   "metadata": {},
   "source": [
    "## Avoid Repeated Work \n",
    "\n",
    "Avoid repeated work by adding a boolean attribute to each candidate indictating whether or not \n",
    "it falls between Sept15 - Nov1.  FL == 'Fall', NFL = 'Not Fall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d94ab9f1-20ea-4fbb-bef1-7d5b81fee16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cand in cands: \n",
    "    dt = dp.julian_to_datetime(fDOM_data[cand[0],0])\n",
    "    if (dt.month == 10) or (dt.month == 9 and dt.day >= 20):\n",
    "        cand.append('FL')\n",
    "    else: \n",
    "        cand.append('NFL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af75376c-ac9d-4038-8886-d41f65bb1335",
   "metadata": {},
   "source": [
    "## Define helper functions and set of training parameters, create training/testing splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64dcebd8-ed69-49fb-b6c1-a9ed33899e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameter ranges and training parameters and helper function\n",
    "iterations = 70000\n",
    "num_splits = 5\n",
    "\n",
    "x_bounds = (0, 50)\n",
    "y_bounds = (0, 50)\n",
    "\n",
    "ratio_threshold_range = (1, 100)\n",
    "\n",
    "accumulated_test_metrics = {}\n",
    "\n",
    "accumulated_test_results = {}\n",
    "\n",
    "accumulated_best_params = {}\n",
    "\n",
    "accumulated_cfmxs = {}\n",
    "\n",
    "train_test_split_indices = TimeSeriesSplit(num_splits).split(cands)\n",
    "\n",
    "def classify_candidate_peaks(peaks, params):\n",
    "    results = []\n",
    "    for peak in peaks:\n",
    "        \n",
    "        stage_rise_condition = (peak[3] != -1 and peak[3] <= params['x']) or (peak[4] !=-1 and peak[4] <= params['y'])\n",
    "        fall_range_condition = peaks[6] == 'NFL'\n",
    "        prominence_condition = peak[5] > params['min_prominence']\n",
    "        \n",
    "        basewidth_promince_ratio_condition = (abs(peak[1] - peak[2]) / peak[5]) < params['ratio_threshold']\n",
    "        \n",
    "        if stage_rise_condition and fall_range_condition and prominence_condition and basewidth_prominence_ratio_condition: \n",
    "            results.append([peak[0], 'NPP'])\n",
    "        else: \n",
    "            results.append([peak[0], 'PP'])\n",
    "    return results\n",
    "\n",
    "def label_positives_negatives(predictions, truths):\n",
    "    TP = TN = FP = FN = 0\n",
    "    results = []\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        prediction = predictions[i][1]\n",
    "        truth = truths[i][2]\n",
    "        \n",
    "        if prediction == 'NPP':\n",
    "            if truth == 'NPP':\n",
    "                TN +=1\n",
    "                results.append(predictions[i].append('TN'))\n",
    "            else: \n",
    "                FN+=1\n",
    "                results.append(predictions[i].append('FN'))\n",
    "        else:\n",
    "            if truth == 'NPP':\n",
    "                FP+=1\n",
    "                results.append(predictions[i].append('FP'))\n",
    "            else: #TODO: Comeback and evaluate if this makes sense: Algo could predict PP because X/Y was not optimal, while truth was PPP because of interference\n",
    "                TP+=1\n",
    "                results.append(predictions[i].append('TP'))\n",
    "        \n",
    "    return (TP,TN,FP,FN,results)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d48dc3-2d1a-41fe-8266-4a53cca70520",
   "metadata": {},
   "source": [
    "## Nested Cross Validation for fDOM PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be65e711-dbf8-47b6-9023-b4930412589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_start = datetime.datetime.now()\n",
    "\n",
    "split = 1\n",
    "\n",
    "for train_val_indices, test_indices in train_test_split_indices:\n",
    "    X_train, y_train = [fDOM_cand[i] for i in train_val_indices], [truths[i] for i in train_val_indices]\n",
    "    X_test, y_test = [fDOM_cand[i] for i in test_indices], [truths[i] for i in test_indices]\n",
    "    \n",
    "    max_fold_metric = 0 \n",
    "    max_result = None \n",
    "    \n",
    "    print(\"Split: \",split)\n",
    "    \n",
    "    split_start = datetime.datetime.now()\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        \n",
    "        # Random grid search for hyperparams \n",
    "        params = {}\n",
    "        \n",
    "        params['x'] = np.random.randint(x_bounds[0], x_bounds[1]+1)\n",
    "        params['y'] = np.random.randint(y_bounds[0], y_bounds[1]+1)\n",
    "        params['min_prominence'] = np.random.uniform(prominence_range[0], prominence_range[1])\n",
    "        params['ratio_threshold'] = np.random.uniform(ratio_threshold_range[0], ratio_threshold_range[1])\n",
    "                \n",
    "        predictions = classify_candidate_peaks(X_train, params)\n",
    "        \n",
    "        TP,TN,FP,FN,results = label_positives_negatives(predictions, y_train)\n",
    "        \n",
    "        TPR = TP/(TP + FN)\n",
    "        TNR = TN/(TN + FP)\n",
    "        \n",
    "        bal_acc = (TPR + TNR)/2\n",
    "        \n",
    "        f1_score = (2 * TP)/((2 * TP) + FP + FN)\n",
    "\n",
    "        acc = f1_score\n",
    "        if iteration and iteration % int(iterations/10) == 0: print(\" {}/{} \".format(iteration, iterations), end = \"\")\n",
    "        if acc > max_fold_metric: \n",
    "            max_fold_metric = acc\n",
    "            max_result = copy.deepcopy(results)\n",
    "            best_params = copy.deepcopy(params)\n",
    "            \n",
    "    # Test best parameters on testing data \n",
    "    test_predictions = classify_candidate_peaks(X_test, best_params)\n",
    "    TP,TN,FP,FN,results = label_positives_negatives(test_predictions, y_test)\n",
    "    \n",
    "    cfmx = confusion_matrix([row[2] for row in y_test],[row[1] for row in test_predictions], labels = ['NPP', 'PP'])\n",
    "    print(cfmx)\n",
    "    accumulated_cfmxs[split] = copy.deepcopy(cfmx)\n",
    "    \n",
    "    TPR = TP/(TP + FN)\n",
    "    TNR = TN/(TN + FP)\n",
    "    \n",
    "    bal_acc = (TPR + TNR)/2 \n",
    "    f1_score = (2 * TP)/((2 * TP) + FP + FN)\n",
    "    \n",
    "    print('\\nSplit: {}  F1: {:.4f} BA: {:.4f}  Params: {}  TP: {} TN: {} FP: {} FN: {}  Time: {}'.format(split, f1_score, bal_acc, best_params, TP, TN, FP, FN, datetime.datetime.now() - split_start))\n",
    "    accumulated_test_metrics[split] = [f1_score, bal_acc] # Record test metrics of each split\n",
    "    accumulated_test_results[split] = copy.deepcopy(results) # Record test results (FP,FN,TP,TN for each datapoint) for each split\n",
    "    accumulated_best_params[split] = copy.deepcopy(best_params) # Record params uses in testing for each split\n",
    "    \n",
    "    split+=1\n",
    "\n",
    "# Display Resuls \n",
    "mean_f1 = 0\n",
    "mean_ba = 0 \n",
    "\n",
    "for key in accumulated_test_metrics:\n",
    "    metrics = accumulated_test_metrics[key]\n",
    "    mean_f1+=metrics[0]\n",
    "    mean_ba+=metrics[1]\n",
    "\n",
    "print(\"Mean Test F1: \", mean_f1/len(accumulated_test_metrics))\n",
    "print(\"Mean Test BA: \", mean_ba/len(accumulated_test_metrics))\n",
    "\n",
    "print(\"Training time: \", datetime.datetime.now() - overall_start)\n",
    "\n",
    "# Pickle params from last fold\n",
    "with open('./Experimental_Results/fDOM_PP/best_params.pkl', 'wb') as pck_file:\n",
    "    pickle.dump(accumulated_best_params[num_splits], pck_file)\n",
    "    pck_file.close()\n",
    "    \n",
    "# Pickle results from last fold \n",
    "with open('./Experimental_Results/fDOM_PP/test_results.pkl', 'wb') as pck_file:\n",
    "    pickle.dump(accumulated_test_results, pck_file)\n",
    "    pck_file.close()\n",
    "\n",
    "# Pickle results from last fold \n",
    "with open('./Experimental_Results/fDOM_PP/test_metrics.pkl', 'wb') as pck_file:\n",
    "    pickle.dump(accumulated_test_metrics, pck_file)\n",
    "    pck_file.close()\n",
    "    \n",
    "mean_cfmx = np.zeros((2,2))\n",
    "for key in accumulated_cfmxs.keys():\n",
    "    mean_cfmx += accumulated_cfmxs[key]\n",
    "mean_cfmx = mean_cfmx / num_splits\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.title(label = 'fDOM Phantom Peak')\n",
    "\n",
    "sn.set(font_scale = 1.5)\n",
    "sn.heatmap( pd.DataFrame(mean_cfmx.astype('float') / mean_cfmx.sum(axis=1)[:, np.newaxis],index = ['Negative', 'Positive'], columns = ['Negative','Positive']), annot = True, annot_kws ={\"size\": 16})\n",
    "plt.xlabel('Ground Truths')\n",
    "plt.ylabel('Predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca78d92d-5349-4a54-bade-8bea572d3347",
   "metadata": {},
   "outputs": [],
   "source": [
    "turb_cand_params = {'prom' : [6,None],\n",
    "                    'width': [None, None],\n",
    "                    'wlen' : 200,\n",
    "                    'dist' : 1,\n",
    "                    'rel_h': .6}\n",
    "\n",
    "# Get fDOM and turb candiate peaks\n",
    "turb_peaks, turb_props = get_candidates(turb_data, turb_cand_params)\n",
    "\n",
    "# Remove peaks that occur during a flat plateau \n",
    "turb_flat_plat = detect_flat_plat(turb_data, 100, 40)\n",
    "turb_flat_plat_indxs = []\n",
    "for i in range(turb_flat_plat.shape[0]):\n",
    "    if turb_flat_plat[i] == 1:\n",
    "        turb_flat_plat_indxs.append(i)\n",
    "\n",
    "take_indices = []\n",
    "for i,peak in enumerate(turb_peaks):\n",
    "    if peak not in turb_flat_plat_indxs:\n",
    "        take_indices.append(i)\n",
    "\n",
    "turb_peaks = np.take(turb_peaks, take_indices)\n",
    "for key in turb_props:\n",
    "    turb_props[key] = np.take(turb_props[key], take_indices)\n",
    "\n",
    "# Iterate through peaks and turn into short 3 point \"events\" by flagging the data point to either side of a peak\n",
    "fDOM_events = []\n",
    "fDOM_lb = []\n",
    "fDOM_rb = []\n",
    "\n",
    "for i,cand in enumerate(fDOM_cands):\n",
    "            fDOM_events.append(np.array((fDOM_data[cand[0]])))\n",
    "            fDOM_lb.append(fDOM_data[math.floor(cand[1]),0])\n",
    "            fDOM_rb.append(fDOM_data[math.ceil(cand[2]),0])\n",
    "            \n",
    "fDOM_lb = list(set(fDOM_lb))\n",
    "fDOM_lb.sort()\n",
    "fDOM_rb = list(set(fDOM_rb))\n",
    "fDOM_rb.sort()\n",
    "\n",
    "turb_events = []\n",
    "turb_lb = []\n",
    "turb_rb = []\n",
    "for i,peak in enumerate(turb_peaks):\n",
    "            turb_events.append(np.array((turb_data[peak])))\n",
    "            turb_lb.append(turb_data[math.floor(turb_props['left_ips'][i]),0])\n",
    "            turb_rb.append(turb_data[math.ceil(turb_props['right_ips'][i]),0])\n",
    "            \n",
    "turb_lb = list(set(turb_lb))\n",
    "turb_lb.sort()\n",
    "turb_rb = list(set(turb_rb))\n",
    "turb_rb.sort()            \n",
    "\n",
    "fDOM_merged = dp.merge_data(fDOM_data, fDOM_events, 'NPP', '')\n",
    "turb_merged = dp.merge_data(turb_data, turb_events, 't_opp', '')\n",
    "\n",
    "fDOM_merged = dp.merge_additional_data(fDOM_merged, fDOM_lb, 'left_base')\n",
    "fDOM_merged = dp.merge_additional_data(fDOM_merged, fDOM_rb, 'right_base')\n",
    "\n",
    "turb_merged = dp.merge_additional_data(turb_merged, turb_lb, 'left_base')\n",
    "turb_merged = dp.merge_additional_data(turb_merged, turb_rb, 'right_base')\n",
    "\n",
    "\n",
    "stage_edge_data = dp.stage_rises_to_data(s_indices, stage_data)\n",
    "stage_data_merged = dp.merge_data(stage_data, stage_edge_data, 'rise','')\n",
    "\n",
    "dm.write_data_to_trainset(fDOM_merged,\n",
    "                          stage_data_merged,\n",
    "                          dp.merge_data(fDOM_data_copy, [], '',''),\n",
    "                          '../Data/temp_plotting/fDOM_PP_smoothed_100k-200k.csv',\n",
    "                          True,\n",
    "                          True,\n",
    "                          100000,\n",
    "                          200000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
