{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79b4decd-2500-42d6-aff8-fbf8a639ca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import time \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split \n",
    "import scipy.io as sio\n",
    "import copy\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from os.path import dirname, join as pjoin\n",
    "import datetime\n",
    "import csv\n",
    "import math\n",
    "import sys\n",
    "sys.path.insert(1,'../')\n",
    "import Tools.data_processing as dp\n",
    "import Tools.data_movement as dm \n",
    "from auxiliary_functions import get_candidates, detect_flat_plat, outdated_detect_stage_rises\n",
    "\n",
    "fDOM_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv')\n",
    "stage_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/stage_10.1.11-1.1.19.csv')\n",
    "turb_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv')\n",
    "stage_data = dp.align_stage_to_fDOM(fDOM_data, stage_data)\n",
    "\n",
    "\n",
    "# same approach but with stage: give window before and after single stage point with label, only takes one timeseries... \n",
    "# so can be flattened input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f4209c-f835-4aed-a385-6c5b35825c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 61, 3])\n"
     ]
    }
   ],
   "source": [
    "def create_turbPP_dataset(fDOM_data, stage_data, turb_data, window_size = 15):\n",
    "    \n",
    "    # Get turb candidate peaks - these were the values used to get the set of turb candidate peaks\n",
    "    turb_cand_params = {'prom' : [6,None],\n",
    "                    'width': [None, None],\n",
    "                    'wlen' : 200,\n",
    "                    'dist' : 1,\n",
    "                    'rel_h': .6}\n",
    "    \n",
    "    turb_peaks, turb_props = get_candidates(turb_data, turb_cand_params)\n",
    "    turb_peaks, turb_props = dp.delete_missing_data_peaks(turb_data, turb_peaks, turb_props, '/Users/zachfogg/Desktop/DB-SRRW/Data/misc/flat_plat_ranges.txt')\n",
    "\n",
    "    # Import ground truths for these peaks\n",
    "    gt_fname_t = '/Users/zachfogg/Desktop/DB-SRRW/Data/manual_annotating_data/processed_data/turb/julian_time/turb_pp_0k-300k-2_labeled'\n",
    "    with open(gt_fname_t, 'r', newline = '') as f:\n",
    "        reader = csv.reader(f, delimiter = ',')\n",
    "        # gt entries in form: ['timestamp_of_peak', 'value_of_peak','label_of_peak','idx_of_peak']\n",
    "        next(reader)\n",
    "        ground_truth = [0 if row[2] == 'NPP' else 1 for row in reader] \n",
    "        f.close()  \n",
    "\n",
    "    # Reshape data \n",
    "    fDOM_data = fDOM_data[:,1].reshape(-1,1)\n",
    "    stage_data = stage_data[:,1].reshape(-1,1)\n",
    "    turb_data = turb_data[:,1].reshape(-1,1)\n",
    "                                         \n",
    "        \n",
    "    # Use Robust scaler to scale data\n",
    "    fDOM_scaler = RobustScaler().fit(fDOM_data)\n",
    "    fDOM_data_scaled = fDOM_scaler.transform(fDOM_data)\n",
    "    \n",
    "    turb_scaler = RobustScaler().fit(turb_data)\n",
    "    turb_data_scaled = turb_scaler.transform(turb_data)\n",
    "    \n",
    "    stage_scaler = RobustScaler().fit(stage_data)\n",
    "    stage_data_scaled = stage_scaler.transform(stage_data)\n",
    "    \n",
    "    # Created \"sequenced\" data, where middle point is the peak --> sequence is (window_size * 2) + 1\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for i,peak in enumerate(turb_peaks):\n",
    "        if peak - window_size > 0 and peak + window_size < len(fDOM_data): \n",
    "            sample = np.vstack((fDOM_data_scaled[peak - window_size:peak + window_size + 1].T,\n",
    "                                stage_data_scaled[peak - window_size:peak + window_size + 1].T,\n",
    "                                turb_data_scaled[peak - window_size:peak + window_size + 1].T)).T\n",
    "            y.append(ground_truth[i])\n",
    "            X.append(sample)\n",
    "    \n",
    "    return X, y\n",
    "    # Create dataset where X is shape (num_candidates, window_size+1, 3) and Y is shape (num_candidates, 1)\n",
    "    # Each X samples is a window around a candidate peak for each turb, fDOM, stage, where the center point is the peak -> (window_size+1, 3)\n",
    "    # Each Y sample is 0 if peak is not PP and 1 if peak is PP \n",
    "batch_size = 8 \n",
    "\n",
    "X,y = create_turbPP_dataset(fDOM_data, stage_data, turb_data, window_size = 30)\n",
    "X = torch.Tensor(X)\n",
    "y = torch.Tensor(y)\n",
    "y = y.type(torch.LongTensor)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# train_dataloader = DataLoader(TensorDataset(torch.Tensor(X_train),torch.Tensor(y_train)), batch_size = batch_size)\n",
    "# test_dataloader = DataLoader(TensorDataset(torch.Tensor(X_test),torch.Tensor(y_test)), batch_size = batch_size)\n",
    "train_dataloader = DataLoader(TensorDataset(X_train,y_train), batch_size = batch_size)\n",
    "test_dataloader = DataLoader(TensorDataset(X_test,y_test), batch_size = batch_size)\n",
    "\n",
    "for X, y in train_dataloader: \n",
    "    print(X.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73360849-3ab3-4c45-877d-42ae118a5eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.566\n",
      "[8.79020267]\n",
      "10428.23\n",
      "[8839.98657214]\n"
     ]
    }
   ],
   "source": [
    "x = 50\n",
    "y = 75\n",
    "# print(stage_data[x:y,1])\n",
    "stage_data1 = stage_data[:,1].reshape(-1,1)\n",
    "stage_scaler = RobustScaler().fit(stage_data1)\n",
    "stage_data_scaled = stage_scaler.transform(stage_data1)\n",
    "# print(stage_data_scaled[x:y])\n",
    "print(max(stage_data[:,1]))\n",
    "print(max(stage_data_scaled))\n",
    "\n",
    "turb_data1 = turb_data[:,1].reshape(-1,1)\n",
    "turb_scaler = RobustScaler().fit(turb_data1)\n",
    "turb_data_scaled = turb_scaler.transform(turb_data1)\n",
    "# print(stage_data_scaled[x:y])\n",
    "print(max(turb_data[:,1]))\n",
    "print(max(turb_data_scaled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25d65e68-f103-43aa-84f6-6db291a3d919",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network that is comprised of three convolutional layers\n",
    "    No dimensionality reduction until the pooling layer\n",
    "    Softmax ouput for binary output \n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, features):\n",
    "        super(FCN, self).__init__()\n",
    "        in_channels = 30 * 2 + 1\n",
    "        self.conv1 = nn.Conv1d(in_channels = in_channels, out_channels = 128, kernel_size = 8, padding = \"same\")\n",
    "        self.batchNorm1 = nn.BatchNorm1d(128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels = 128 , out_channels = 256 , kernel_size = 5, padding = \"same\")\n",
    "        self.batchNorm2 = nn.BatchNorm1d(256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels = 256, out_channels = 128, kernel_size = 3, padding = \"same\")\n",
    "        self.batchNorm3 = nn.BatchNorm1d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.pool = nn.AvgPool1d(kernel_size = 3)\n",
    "        \n",
    "        self.fc = nn.Linear(in_features = 128, out_features = num_classes)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc2 = nn.Linear(in_features = 128, out_features = 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "#         print(\"C1 \", x.shape)\n",
    "        x = self.batchNorm1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "#         print(\"C2 \", x.shape)        \n",
    "        x = self.batchNorm2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "#         print(\"C3 \", x.shape)  \n",
    "        x = self.batchNorm3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.pool(x)\n",
    "#         print(\"P \", x.shape)\n",
    "        x = self.flatten(x)\n",
    "#         print(\"F \", x.shape)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc(x)\n",
    "#         print(x.shape)\n",
    "#         print(x)\n",
    "        logits = self.softmax(x)\n",
    "#         print(logits.shape)\n",
    "#         print(logits)\n",
    "        return logits\n",
    "    \n",
    "# class FCN(nn.Module):\n",
    "#     def __init__(self, window_size, features):\n",
    "#         super(FCN, self).__init__()\n",
    "# #         self.conv1 = nn.Conv1d(in_channels = )\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear_relu_stack = nn.Sequential(\n",
    "#             nn.Linear(window_size * features, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128,1)\n",
    "#         )\n",
    "#     def forward(self, x):\n",
    "#         x = self.flatten(x)\n",
    "#         logits = self.linear_relu_stack(x)\n",
    "#         return logits\n",
    "\n",
    "def train(dataLoader, model, loss_fn, optimizer):\n",
    "    size = len(dataLoader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X,y) in enumerate(dataLoader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Compute prediction error \n",
    "        pred = model(X)\n",
    "#         loss = loss_fn(pred,y.unsqueeze(1))\n",
    "\n",
    "        loss = loss_fn(pred,y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         if batch % 10 == 0:\n",
    "#             loss, current = loss.item(), batch * len(X)\n",
    "#             print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "def test(dataLoader, model, loss_fn):\n",
    "    size = len(dataLoader.dataset)\n",
    "    num_batches = len(dataLoader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0,0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X,y in dataLoader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred,y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "        test_loss /= num_batches \n",
    "        correct /= size \n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b9c35a5-b90a-4758-b1e9-14489323ecae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.623195 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 67.6%, Avg loss: 0.595769 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 0.659419 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 0.676276 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 0.636171 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.659903 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.569327 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 67.1%, Avg loss: 0.614442 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.602227 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Avg loss: 0.660541 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 66.2%, Avg loss: 0.627858 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 72.4%, Avg loss: 0.564212 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.540870 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.592267 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.591506 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.578167 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 90.0%, Avg loss: 0.408881 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 0.621037 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.463570 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.1%, Avg loss: 0.437011 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.536345 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 79.0%, Avg loss: 0.501320 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.537334 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.624524 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.480730 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.454899 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 0.491922 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.471342 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.461491 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.473905 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.494287 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.431533 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.517572 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.535436 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.407821 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.0%, Avg loss: 0.420004 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.382688 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.0%, Avg loss: 0.412432 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.425030 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.429411 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.488315 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.430487 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.432384 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.429052 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.0%, Avg loss: 0.410762 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 85.7%, Avg loss: 0.437766 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.0%, Avg loss: 0.419326 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.424122 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.441035 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 89.5%, Avg loss: 0.409988 \n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "num_channels = 3\n",
    "model = FCN(num_classes,num_channels).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 50\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
