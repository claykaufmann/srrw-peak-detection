{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c86af3aa-2fd0-47c2-bdb5-c860dd007be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and data \n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import pandas as pd\n",
    "import copy\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from os.path import dirname, join as pjoin\n",
    "import datetime\n",
    "import csv\n",
    "import math\n",
    "import sys\n",
    "sys.path.insert(1,'../')\n",
    "import Tools.data_processing as dp\n",
    "import Tools.data_movement as dm \n",
    "from Tools.auxiliary_functions import get_candidates, detect_flat_plat\n",
    "\n",
    "fDOM_raw_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv')\n",
    "stage_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/stage_10.1.11-1.1.19.csv')\n",
    "turb_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv')\n",
    "stage_data = dp.align_stage_to_fDOM(fDOM_raw_data, stage_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9358fb6b-4bda-4185-a1f2-6e4e4dc8ef93",
   "metadata": {},
   "source": [
    "### Define the two stage rise detection algorithms being tested and helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d94e7610-f8f8-4b8f-9cda-afa0592f10c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_stage_rises_one_window(data, params):\n",
    "    slope_threshold = params['slope_threshold']\n",
    "    duration_threshold = params['duration_threshold']\n",
    "    \n",
    "    signals = np.zeros(len(data))\n",
    "    slopes = np.diff(data)\n",
    "    continuous_rises = np.zeros(len(slopes))\n",
    "    \n",
    "    # For each point, how long has there been a sustained rise\n",
    "    length = 0\n",
    "    for i in range(len(slopes)):\n",
    "        if slopes[i] >= slope_threshold:\n",
    "            length +=1\n",
    "        else:\n",
    "            length = 0\n",
    "        continuous_rises[i] = length\n",
    "        \n",
    "    # For each point, if the rise has been sustained for a sufficient period, label all points in that rise\n",
    "    for i in range(len(continuous_rises)):\n",
    "        rise_length = continuous_rises[i]\n",
    "        if rise_length >= duration_threshold:\n",
    "            signals[int(i - rise_length + 1):i+1] = 1\n",
    "\n",
    "    return signals\n",
    "                       \n",
    "def detect_stage_rises_two_window(data, params):\n",
    "    w1_slope_threshold = params['w1_slope_threshold']\n",
    "    w2_slope_threshold = params['w2_slope_threshold']\n",
    "    w1_duration_threshold = params['w1_duration_threshold']\n",
    "    w2_duration_threshold = params['w2_duration_threshold']\n",
    "\n",
    "    signals = np.zeros(len(data))\n",
    "    slopes = np.diff(data)\n",
    "    \n",
    "    w1_continuous_rises = np.zeros(len(slopes))\n",
    "    w2_continuous_rises = np.zeros(len(slopes))\n",
    "    \n",
    "    # For each point, how long has there been a sustained rise\n",
    "    w1_length = 0\n",
    "    w2_length = 0\n",
    "    for i in range(len(slopes)):\n",
    "        # Window 1\n",
    "        if slopes[i] >= w1_slope_threshold:\n",
    "            w1_length +=1\n",
    "        else:\n",
    "            w1_length = 0\n",
    "        w1_continuous_rises[i] = w1_length\n",
    "        \n",
    "        # Window 2 \n",
    "        if slopes[i] >= w2_slope_threshold:\n",
    "            w2_length +=1\n",
    "        else:\n",
    "            w2_length = 0\n",
    "            \n",
    "        w2_continuous_rises[i] = w2_length\n",
    "    \n",
    "    # For each point, if the rise has been sustained for a sufficient period, label all points in that rise\n",
    "    for i in range(len(w1_continuous_rises)):\n",
    "        # Window 1\n",
    "        rise_length = w1_continuous_rises[i]\n",
    "        if rise_length >= w1_duration_threshold:\n",
    "            signals[int(i - rise_length + 1):i+2] = 1\n",
    "            \n",
    "        # Window 2\n",
    "        rise_length = w2_continuous_rises[i]\n",
    "        if rise_length >= w2_duration_threshold:\n",
    "            signals[int(i - rise_length + 1):i+2] = 1\n",
    "    \n",
    "    return signals\n",
    "\n",
    "def detect_stage_rises_two_window_height(data, params):\n",
    "    w1_slope_threshold = params['w1_slope_threshold']\n",
    "    w2_slope_threshold = params['w2_slope_threshold']\n",
    "    w1_duration_threshold = params['w1_duration_threshold']\n",
    "    w2_duration_threshold = params['w2_duration_threshold']\n",
    "    min_height = params['min_height']\n",
    "\n",
    "    signals = np.zeros(len(data))\n",
    "    slopes = np.diff(data)\n",
    "    \n",
    "    w1_continuous_rises = np.zeros(len(slopes))\n",
    "    w2_continuous_rises = np.zeros(len(slopes))\n",
    "    \n",
    "    # For each point, how long has there been a sustained rise\n",
    "    w1_length = 0\n",
    "    w2_length = 0\n",
    "    for i in range(len(slopes)):\n",
    "        # Window 1\n",
    "        if slopes[i] >= w1_slope_threshold:\n",
    "            w1_length +=1\n",
    "        else:\n",
    "            w1_length = 0\n",
    "        w1_continuous_rises[i] = w1_length\n",
    "        \n",
    "        # Window 2 \n",
    "        if slopes[i] >= w2_slope_threshold:\n",
    "            w2_length +=1\n",
    "        else:\n",
    "            w2_length = 0\n",
    "            \n",
    "        w2_continuous_rises[i] = w2_length\n",
    "    \n",
    "    # For each point, if the rise has been sustained for a sufficient period, label all points in that rise\n",
    "    for i in range(len(w1_continuous_rises)):\n",
    "        # Window 1\n",
    "        rise_length = w1_continuous_rises[i]\n",
    "        height_condition = abs(data[int(i - rise_length +1)] - data[i + 1]) >= min_height\n",
    "        \n",
    "        if (rise_length >= w1_duration_threshold) and height_condition:\n",
    "            signals[int(i - rise_length + 1):i+2] = 1\n",
    "            \n",
    "        # Window 2\n",
    "        rise_length = w2_continuous_rises[i]\n",
    "        height_condition = abs(data[int(i - rise_length +1)] - data[i + 1]) >= min_height\n",
    "        \n",
    "        if (rise_length >= w2_duration_threshold) and height_condition:\n",
    "            signals[int(i - rise_length + 1):i+2] = 1\n",
    "    \n",
    "    return signals\n",
    "\n",
    "def label_positives_negatives(predictions, ground_truths):\n",
    "    TP = TN = FP = FN = 0\n",
    "    results = []\n",
    "    for i in range(len(predictions)):\n",
    "        prediction = predictions[i]\n",
    "        ground_truth = ground_truths[i]\n",
    "\n",
    "        if prediction == 1 and ground_truth == 1:\n",
    "            results.append([prediction, 'TP'])\n",
    "            TP +=1 \n",
    "        elif prediction == 1 and ground_truth == 0:\n",
    "            results.append([prediction, 'FP'])\n",
    "            FP +=1 \n",
    "        elif prediction == 0 and ground_truth == 1:\n",
    "            results.append([prediction,'FN'])\n",
    "            FN +=1 \n",
    "        else:\n",
    "            results.append([prediction, 'TN'])\n",
    "            TN +=1\n",
    "    return (TP,TN,FP,FN,results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39efb56d-445c-47b0-9b31-357fd9764fbf",
   "metadata": {},
   "source": [
    "### Import ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b2bfc23-8d8a-42e4-9e6b-d16ad7a56ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground_truth_path = '/Users/zachfogg/Desktop/newnew/stage_rise_labeled_0k-300k.csv'\n",
    "ground_truth_path = \"../Data/labeled_data/ground_truths/stage/stage_rise/stage_rise_labeled_0k-300k.csv\"\n",
    "ground_truth_signals = np.zeros(len(stage_data))\n",
    "with open(ground_truth_path, 'r', newline = '') as gt_file:\n",
    "    reader = csv.reader(gt_file, delimiter = ',')\n",
    "    idx = 0 \n",
    "    for row in reader:\n",
    "        if row[0] == 'Stage':\n",
    "            if row[3] == 'TP':\n",
    "                ground_truth_signals[idx] = 1\n",
    "            idx+=1\n",
    "\n",
    "for i in range(1,len(ground_truth_signals)-1):\n",
    "    if ground_truth_signals[i-1] == ground_truth_signals[i+1] == 0 and ground_truth_signals[i] == 1:\n",
    "        ground_truth_signals[i] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39817508-bc0d-4bce-80df-d115ef877b44",
   "metadata": {},
   "source": [
    "### Define Hyperparam bounds and training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6242bcb-5859-46fc-b7c1-eef743bb3bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Bounds \n",
    "slope_threshold_bounds = (0,.1)\n",
    "duration_threshold_bounds = (0,20)\n",
    "min_height_bounds = (0,0.1)\n",
    "\n",
    "best_params = {}\n",
    "\n",
    "iterations = 688\n",
    "\n",
    "num_splits = 5 # Number of train/val/test folds\n",
    "\n",
    "train_test_split = TimeSeriesSplit(num_splits).split(stage_data[:,1])\n",
    "\n",
    "accumulated_test_metrics = {}\n",
    "\n",
    "accumulated_test_results = {}\n",
    "\n",
    "overall_start = datetime.datetime.now()\n",
    "\n",
    "accumulated_best_params = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9ac950e-13e1-42d4-9307-839fc21a04c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split:  1\n",
      " 68/688  136/688  204/688  272/688  340/688  408/688  476/688  544/688  612/688  680/688 \n",
      "Split: 1  F1: 0.9035 BA: 0.9887  Params: {'w1_slope_threshold': 0.09973508451591305, 'w1_duration_threshold': 9, 'w2_slope_threshold': 0.0027122093054720354, 'w2_duration_threshold': 2}  TP: 1123 TN: 36907 FP: 221 FN: 19  Time: 0:01:07.738097\n",
      "Split:  2\n",
      " 68/688  136/688  204/688  272/688  340/688  408/688  476/688  544/688  612/688  680/688 \n",
      "Split: 2  F1: 0.9034 BA: 0.9666  Params: {'w1_slope_threshold': 0.002071041670009577, 'w1_duration_threshold': 5, 'w2_slope_threshold': 0.02960505743036176, 'w2_duration_threshold': 2}  TP: 888 TN: 37192 FP: 130 FN: 60  Time: 0:02:15.969185\n",
      "Split:  3\n",
      " 68/688  136/688  204/688  272/688  340/688  408/688  476/688  544/688  612/688  680/688 \n",
      "Split: 3  F1: 0.9033 BA: 0.9858  Params: {'w1_slope_threshold': 0.002692193427462497, 'w1_duration_threshold': 3, 'w2_slope_threshold': 0.03481963497731524, 'w2_duration_threshold': 2}  TP: 1004 TN: 37051 FP: 191 FN: 24  Time: 0:03:23.609266\n",
      "Split:  4\n",
      " 68/688  136/688  204/688  272/688  340/688  408/688  476/688  544/688  612/688  680/688 \n",
      "Split: 4  F1: 0.9452 BA: 0.9683  Params: {'w1_slope_threshold': 0.0027973820056520827, 'w1_duration_threshold': 4, 'w2_slope_threshold': 0.018505985197238586, 'w2_duration_threshold': 2}  TP: 1216 TN: 36913 FP: 61 FN: 80  Time: 0:51:50.788750\n",
      "Split:  5\n",
      " 68/688  136/688  204/688  272/688  340/688  408/688  476/688  544/688  612/688  680/688 \n",
      "Split: 5  F1: 0.9432 BA: 0.9556  Params: {'w1_slope_threshold': 0.0031429701658513, 'w1_duration_threshold': 7, 'w2_slope_threshold': 0.004675409136497811, 'w2_duration_threshold': 2}  TP: 1004 TN: 37145 FP: 24 FN: 97  Time: 0:28:29.979175\n",
      "Mean Test F1:  0.9196932508669077\n",
      "Mean Test BA:  0.9730035251600693\n",
      "1:27:08.762801\n"
     ]
    }
   ],
   "source": [
    "overall_start = datetime.datetime.now()\n",
    "\n",
    "split = 1 \n",
    "\n",
    "for train_val_indices, test_indices in train_test_split: # Outer loop performs the \"Forward-Chaining\"\n",
    "    # Get appropriate data subset\n",
    "    X_train, y_train = np.take(stage_data[:,1], train_val_indices), np.take(ground_truth_signals, train_val_indices)\n",
    "    X_test, y_test = np.take(stage_data[:,1], test_indices), np.take(ground_truth_signals, test_indices)\n",
    "\n",
    "   \n",
    "    max_fold_metric = 0\n",
    "    max_result = None  \n",
    "    max_acc = 0\n",
    "    print(\"Split: \",split)\n",
    "    # Optimize hyper parameters to the training data\n",
    "    split_start = datetime.datetime.now()\n",
    "    for iteration in range(iterations):\n",
    "        \n",
    "        # Random grid search\n",
    "#         params['slope_threshold'] = np.random.uniform(slope_threshold_bounds[0], slope_threshold_bounds[1])\n",
    "#         params['duration_threshold'] = np.random.randint(duration_threshold_bounds[0], duration_threshold_bounds[1]+1)        \n",
    "        params = {}\n",
    "        params['w1_slope_threshold'] = np.random.uniform(slope_threshold_bounds[0], slope_threshold_bounds[1])\n",
    "        params['w1_duration_threshold'] = np.random.randint(duration_threshold_bounds[0], duration_threshold_bounds[1]+1)\n",
    "        \n",
    "        params['w2_slope_threshold'] = np.random.uniform(slope_threshold_bounds[0], slope_threshold_bounds[1])\n",
    "#         params['w2_duration_threshold'] = np.random.randint(duration_threshold_bounds[0], duration_threshold_bounds[1]+1)\n",
    "        params['w2_duration_threshold'] = 2\n",
    "        \n",
    "        # Detect stage rises \n",
    "        detected_signals = detect_stage_rises_two_window(X_train, params)\n",
    "        \n",
    "        # Results will hold: [signal, result in comparison to ground truth]\n",
    "        TP,TN,FP,FN,results = label_positives_negatives(detected_signals, y_train)\n",
    "        \n",
    "        TPR = TP/(TP + FN)\n",
    "        TNR = TN/(TN + FP)\n",
    "        bal_acc = (TPR + TNR)/2 \n",
    "        f1_score = (2 * TP)/((2 * TP) + FP + FN)\n",
    "\n",
    "        acc = f1_score\n",
    "        if iteration and iteration % int(iterations/10) == 0: print(\" {}/{} \".format(iteration, iterations), end = \"\")\n",
    "        if acc > max_acc: \n",
    "            max_acc = acc\n",
    "            max_result = copy.deepcopy(results)\n",
    "            best_params = copy.deepcopy(params)\n",
    "#             print('I: {}  F1: {:.4f} BA: {:.4f}  Params: {}  TP: {} TN: {} FP: {} FN: {}'.format(iteration, f1_score, bal_acc, params, TP, TN, FP, FN))\n",
    "        \n",
    "    # Test best parameters on testing data \n",
    "    predicted_signals = detect_stage_rises_two_window(X_test, best_params)\n",
    "    \n",
    "    TP,TN,FP,FN,results = label_positives_negatives(predicted_signals, y_test)\n",
    "    \n",
    "    TPR = TP/(TP + FN)\n",
    "    TNR = TN/(TN + FP)\n",
    "    \n",
    "    bal_acc = (TPR + TNR)/2 \n",
    "    f1_score = (2 * TP)/((2 * TP) + FP + FN)\n",
    "    \n",
    "#     print(\"Split: {} test scores, f1: {} BA: {} Time: {}\".format(split, f1_score, bal_acc, datetime.datetime.now() - split_start))\n",
    "    print('\\nSplit: {}  F1: {:.4f} BA: {:.4f}  Params: {}  TP: {} TN: {} FP: {} FN: {}  Time: {}'.format(split, f1_score, bal_acc, best_params, TP, TN, FP, FN, datetime.datetime.now() - split_start))\n",
    "    accumulated_test_metrics[split] = [f1_score, bal_acc] # Record test metrics of each split\n",
    "    accumulated_test_results[split] = copy.deepcopy(results) # Record test results (FP,FN,TP,TN for each datapoint) for each split\n",
    "    accumulated_best_params[split] = copy.deepcopy(best_params) # Record params uses in testing for each split\n",
    "    \n",
    "    split+=1\n",
    "\n",
    "mean_f1 = 0\n",
    "mean_ba = 0 \n",
    "\n",
    "for key in accumulated_test_metrics:\n",
    "    metrics = accumulated_test_metrics[key]\n",
    "    mean_f1+=metrics[0]\n",
    "    mean_ba+=metrics[1]\n",
    "\n",
    "print(\"Mean Test F1: \", mean_f1/len(accumulated_test_metrics))\n",
    "print(\"Mean Test BA: \", mean_ba/len(accumulated_test_metrics))\n",
    "\n",
    "print(datetime.datetime.now() - overall_start)\n",
    "\n",
    "# Pickle params from last fold\n",
    "with open('./Past_Experiments/stage_algo_exp_results/algo3_best_params.pkl', 'wb') as pck_file:\n",
    "    pickle.dump(accumulated_best_params[num_splits], pck_file)\n",
    "    pck_file.close()\n",
    "    \n",
    "# Pickle results from last fold \n",
    "with open('./Past_Experiments/stage_algo_exp_results/algo3_test_results.pkl', 'wb') as pck_file:\n",
    "    pickle.dump(accumulated_test_results, pck_file)\n",
    "    pck_file.close()\n",
    "\n",
    "# Pickle results from last fold \n",
    "with open('./Past_Experiments/stage_algo_exp_results/algo3_test_metrics.pkl', 'wb') as pck_file:\n",
    "    pickle.dump(accumulated_test_metrics, pck_file)\n",
    "    pck_file.close()\n",
    "    \n",
    "# Algo 5 is when we don't allow window of size < 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06729f9b-eaf9-4c60-8ed2-adea14ec6f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9398932112890923\n",
      "FP:, 437  FN:  351\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Alg 4: {'w1_slope_threshold': 0.08070955276214427, 'w1_duration_threshold': 7, 'w2_slope_threshold': 0.0028358577231109906, 'w2_duration_threshold': 3, 'min_height': 0.029811829887152044}\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Algo 3: \n",
    "{'w1_slope_threshold': 0.0025548972795833794, 'w1_duration_threshold': 4, 'w2_slope_threshold': 0.024178567948212826, 'w2_duration_threshold': 12, 'min_height': 0.0007260027409820013}\n",
    "\n",
    "Algo 2: Mean Test F1:  0.874357384629945\n",
    "Mean Test BA:  0.9232266661025357\n",
    "\"\"\"\n",
    "params = {'w1_slope_threshold': 0.0027973820056520827, 'w1_duration_threshold': 4, 'w2_slope_threshold': 0.018505985197238586, 'w2_duration_threshold': 2}\n",
    "predicted_signals = detect_stage_rises_two_window(stage_data[:,1], params)\n",
    "    \n",
    "TP,TN,FP,FN,results = label_positives_negatives(predicted_signals, ground_truth_signals)\n",
    "\n",
    "print((2 * TP)/((2 * TP) + FP + FN))\n",
    "print(\"FP:,\", FP, \" FN: \", FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3718971f-4784-4d44-9ce5-1f8f5b3e56fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,2,3,4,-1],[1,2,3,4,-1]])\n",
    "y = np.argmax(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3f9c62-4fda-4b24-93c2-78843214f105",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'w1_slope_threshold': 0.0027973820056520827, 'w1_duration_threshold': 4, 'w2_slope_threshold': 0.018505985197238586, 'w2_duration_threshold': 2}\n",
    "with open('./Hyperparameters/Stage/detect_stage_rises_params.pkl', 'wb') as pck_file:\n",
    "    pickle.dump(params, pck_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "157f5fcc-476a-4b9d-b188-f5447327389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out signals to reconsider labels\n",
    "# Get stage rises\n",
    "# fDOM_cand_params = {'prom' : [4,None],\n",
    "#                     'width': [None, None],   These are the params for fDOM pp cand\n",
    "#                     'wlen' : 200,\n",
    "#                     'dist' : 1,\n",
    "#                     'rel_h': .6}\n",
    "\n",
    "fDOM_cand_params = {'prom' : [4,None],\n",
    "                    'width': [None, None],\n",
    "                    'wlen' : 200,\n",
    "                    'dist' : 1,\n",
    "                    'rel_h': .6}\n",
    "\n",
    "# turb_cand_params = {'prom' : [6,None],       These are the params that were used to label turb cand 0-100k\n",
    "#                     'width': [None, None],\n",
    "#                     'wlen' : 200,\n",
    "#                     'dist' : 1,\n",
    "#                     'rel_h': .6}\n",
    "\n",
    "turb_cand_params = {'prom' : [6,None],\n",
    "                    'width': [None, None],    # was 2 for skyrocketing\n",
    "                    'wlen' : 200,\n",
    "                    'dist' : 1,\n",
    "                    'rel_h': .6}   # was .5 for skyrocketing \n",
    "\n",
    "# Get fDOM and turb candiate peaks\n",
    "fDOM_peaks, fDOM_props = get_candidates(fDOM_raw_data, fDOM_cand_params)\n",
    "turb_peaks, turb_props = get_candidates(turb_data, turb_cand_params)\n",
    "\n",
    "# Remove peaks that occur during a flat plateau \n",
    "turb_flat_plat = detect_flat_plat(turb_data, 100, 40)\n",
    "turb_flat_plat_indxs = []\n",
    "for i in range(turb_flat_plat.shape[0]):\n",
    "    if turb_flat_plat[i] == 1:\n",
    "        turb_flat_plat_indxs.append(i)\n",
    "\n",
    "take_indices = []\n",
    "for i,peak in enumerate(turb_peaks):\n",
    "    if peak not in turb_flat_plat_indxs:\n",
    "        take_indices.append(i)\n",
    "\n",
    "turb_peaks = np.take(turb_peaks, take_indices)\n",
    "for key in turb_props:\n",
    "    turb_props[key] = np.take(turb_props[key], take_indices)\n",
    "\n",
    "# Iterate through peaks and turn into short 3 point \"events\" by flagging the data point to either side of a peak\n",
    "# fDOM_events = []\n",
    "# for peak in fDOM_peaks:\n",
    "#             fDOM_events.append(np.array((fDOM_raw_data[peak-1], fDOM_raw_data[peak], fDOM_raw_data[peak+1])))\n",
    "fDOM_events = []\n",
    "fDOM_lb = []\n",
    "fDOM_rb = []\n",
    "\n",
    "for i,peak in enumerate(fDOM_peaks):\n",
    "            fDOM_events.append(np.array((fDOM_raw_data[peak])))\n",
    "#             fDOM_lb.append(fDOM_raw_data[fDOM_props['left_bases'][i],0])\n",
    "#             fDOM_rb.append(fDOM_raw_data[fDOM_props['right_bases'][i],0])\n",
    "            fDOM_lb.append(fDOM_raw_data[math.floor(fDOM_props['left_ips'][i]),0])\n",
    "            fDOM_rb.append(fDOM_raw_data[math.ceil(fDOM_props['right_ips'][i]),0])\n",
    "            \n",
    "fDOM_lb = list(set(fDOM_lb))\n",
    "fDOM_lb.sort()\n",
    "fDOM_rb = list(set(fDOM_rb))\n",
    "fDOM_rb.sort()\n",
    "\n",
    "turb_events = []\n",
    "turb_lb = []\n",
    "turb_rb = []\n",
    "for i,peak in enumerate(turb_peaks):\n",
    "            turb_events.append(np.array((turb_data[peak])))\n",
    "            turb_lb.append(turb_data[math.floor(turb_props['left_ips'][i]),0])\n",
    "            turb_rb.append(turb_data[math.ceil(turb_props['right_ips'][i]),0])\n",
    "            \n",
    "turb_lb = list(set(turb_lb))\n",
    "turb_lb.sort()\n",
    "turb_rb = list(set(turb_rb))\n",
    "turb_rb.sort()            \n",
    "\n",
    "fDOM_merged = dp.merge_data(fDOM_raw_data, fDOM_events, 'f_opp', '')\n",
    "turb_merged = dp.merge_data(turb_data, turb_events, 't_opp', '')\n",
    "\n",
    "fDOM_merged = dp.merge_additional_data(fDOM_merged, fDOM_lb, 'left_base')\n",
    "fDOM_merged = dp.merge_additional_data(fDOM_merged, fDOM_rb, 'right_base')\n",
    "\n",
    "turb_merged = dp.merge_additional_data(turb_merged, turb_lb, 'left_base')\n",
    "turb_merged = dp.merge_additional_data(turb_merged, turb_rb, 'right_base')\n",
    "\n",
    "\n",
    "stage_data_merged = []\n",
    "for i in range(len(results)):\n",
    "    stage_data_merged.append([stage_data[i,0],stage_data[i,1],results[i][1]])\n",
    "\n",
    "dm.write_data_to_trainset(fDOM_merged,\n",
    "                          stage_data_merged,\n",
    "                          turb_merged,\n",
    "                          '../Data/plot/stage_rise_algo_0k-100k.csv',\n",
    "                          True,\n",
    "                          True,\n",
    "                          0,\n",
    "                          100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb1b48da-4693-4f90-9f81-7afd6272e410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w1_slope_threshold': 0.08638011677049756, 'w1_duration_threshold': 10, 'w2_slope_threshold': 0.0034103401817676352, 'w2_duration_threshold': 2}\n"
     ]
    }
   ],
   "source": [
    "print(accumulated_best_params[num_splits][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
