{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba9d5f56-2484-4121-b710-7b3beca2de00",
   "metadata": {},
   "source": [
    "# Detect fDOM Skyrockting Peaks\n",
    "\n",
    "## Rules for Skyrocketing Peaks\n",
    "\n",
    "Upward peak where: \n",
    "- Base width smaller than threshold \n",
    "- Prominence larger than a threshold \n",
    "- No adjacent upward peak in a certain range\n",
    "- Both bases of the peak are not also fDOM downward peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7237f96e-b576-4cf0-9872-da77380976e1",
   "metadata": {},
   "source": [
    "## Import Tools, Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0315618-62c9-4b38-bb86-95cd7705dbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and data \n",
    "import scipy.io as sio\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import copy\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from os.path import dirname, join as pjoin\n",
    "import datetime\n",
    "import csv\n",
    "import math\n",
    "import sys\n",
    "sys.path.insert(1,'../')\n",
    "import Tools.data_processing as dp\n",
    "import Tools.data_movement as dm \n",
    "from auxiliary_functions import get_candidates, detect_flat_plat, detect_stage_rises\n",
    "\n",
    "fDOM_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv')\n",
    "stage_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/stage_10.1.11-1.1.19.csv')\n",
    "turb_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv')\n",
    "stage_data = dp.align_stage_to_fDOM(fDOM_data, stage_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c7b376-888c-4144-a7e3-7de5aad5bbbf",
   "metadata": {},
   "source": [
    "## Get Candidate Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d014c238-2633-4ce4-8f69-44ef776bb992",
   "metadata": {},
   "outputs": [],
   "source": [
    "prominence_range = [5,None]\n",
    "width_range = [None,None]\n",
    "wlen = 100\n",
    "distance = 1\n",
    "rel_height =.6\n",
    "\n",
    "# Get list of all peaks that could possibly be plummeting peaks\n",
    "peaks, props = find_peaks(fDOM_data[:,1],\n",
    "                          height = (None, None),\n",
    "                          threshold = (None,None),\n",
    "                          distance = distance,\n",
    "                          prominence = prominence_range,\n",
    "                          width = width_range,\n",
    "                          wlen = wlen,\n",
    "                          rel_height = rel_height)\n",
    "\n",
    "# Form candidate set from returned information\n",
    "cands = [[peak, math.floor(props['left_ips'][i]), math.ceil(props['right_ips'][i]),props['prominences'][i]] for i,peak in enumerate(peaks)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa27f07-c10c-4c2e-9e1e-c16be052827a",
   "metadata": {},
   "source": [
    "## Process proximity to other peaks in usable form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c36e0be-d210-462b-b764-ceefa90a0a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append some data to the end of cands that tell how far a peak is from other peaks (I think min of left and right should be fine)\n",
    "# For each peak, determine how far it is from the closest adjacent peak\n",
    "proximity_to_adjacent = np.zeros((len(peaks)))\n",
    "\n",
    "for i in range(len(peaks)):\n",
    "    x = y = fDOM_data.shape[0] + 1 \n",
    "    if i > 0: \n",
    "        x = abs(peaks[i] - peaks[i-1])\n",
    "    if i < len(peaks) - 1:\n",
    "        y = abs(peaks[i] - peaks[i+1])\n",
    "        \n",
    "    proximity_to_adjacent[i] = min(x,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff570596-1910-41e5-9d6d-e3603444d5fe",
   "metadata": {},
   "source": [
    "## Get Downward Peaks (used in detection mechanism) and process to usable form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77bf8516-607e-4a8d-beb3-0b1f0618d726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hh/m6vvts714wdcv6sngjwj41zr0000gn/T/ipykernel_1446/2349065138.py:12: PeakPropertyWarning: some peaks have a prominence of 0\n",
      "  downward_peaks, _ = find_peaks(flipped_fDOM[:,1],\n"
     ]
    }
   ],
   "source": [
    "# Flip timeseries \n",
    "flipped_fDOM = dp.flip_timeseries(copy.deepcopy(fDOM_data))\n",
    "\n",
    "# Get fDOM plummeting peak candidate set using scipy find_peaks()\n",
    "prominence_range = [3,None] # peaks must have at least prominence 3\n",
    "width_range = [None,10] # peaks cannot have a base width of more than 5\n",
    "wlen = 100 \n",
    "distance = 1 \n",
    "rel_height =.6\n",
    "\n",
    "# Get list of all peaks that could possibly be plummeting peaks\n",
    "downward_peaks, _ = find_peaks(flipped_fDOM[:,1],\n",
    "                          height = (None, None),\n",
    "                          threshold = (None,None),\n",
    "                          distance = distance,\n",
    "                          prominence = prominence_range,\n",
    "                          width = width_range,\n",
    "                          wlen = wlen,\n",
    "                          rel_height = rel_height)\n",
    "\n",
    "# Process into usable form: This is not efficient, but this operation is only peformed once... so no sweat \n",
    "proximity_to_downward = np.zeros((len(cands),2))\n",
    "\n",
    "for i, cand in enumerate(cands): \n",
    "    x = y = fDOM_data.shape[0] + 1 \n",
    "    \n",
    "    for downward_peak in downward_peaks:\n",
    "        if downward_peak <= cand[1]: \n",
    "            x = min(abs(cand[1]-downward_peak),x)\n",
    "        elif downward_peak >= cand[2]:\n",
    "            y = min(abs(cand[2]-downward_peak),y)\n",
    "    proximity_to_downward[i,0] = x\n",
    "    proximity_to_downward[i,1] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078606d0-efd4-4fa1-a0af-6ddc0e3297e3",
   "metadata": {},
   "source": [
    "## Import and process ground truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1c32b43-a186-4496-bd21-5a34157ad2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and process ground truth\n",
    "truth_fname = '../Data/labeled_data/ground_truths/fDOM/fDOM_SKP/julian_time/fDOM_SKP_0k-300k.csv'\n",
    "\n",
    "with open(truth_fname, 'r', newline = '') as f:\n",
    "    \n",
    "        reader = csv.reader(f, delimiter = ',')\n",
    "        # truth entries in form: ['timestamp_of_peak', 'value_of_peak','label_of_peak','idx_of_peak']\n",
    "        next(reader)\n",
    "        truths = [[float(row[0]), float(row[1]), row[2], int(row[3])] for row in reader] \n",
    "        f.close()   \n",
    "\n",
    "assert(len(truths) == len(cands))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc185b2-8c09-4a26-a007-ed0c9a5f1419",
   "metadata": {},
   "source": [
    "## Define helper functions and set of training parameters, create training/testing splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "192377a8-d1f8-476a-8b97-424a30aa6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameter ranges and training parameters and helper function\n",
    "iterations = 7000\n",
    "num_splits = 5\n",
    "\n",
    "basewidth_range = (1, 10)\n",
    "prominence_range= (5, 20)\n",
    "\n",
    "peak_proximity_bounds = (0,20)\n",
    "downward_bases_range= (0,5) # sometimes the actual 'peak' of the downward peak is not exactly at the base of the skyrocketing peak \n",
    "\n",
    "accumulated_test_metrics = {}\n",
    "\n",
    "accumulated_test_results = {}\n",
    "\n",
    "accumulated_best_params = {}\n",
    "\n",
    "accumulated_cfmxs = {}\n",
    "\n",
    "train_test_split_indices = TimeSeriesSplit(num_splits).split(cands)\n",
    "\n",
    "def classify_candidate_peaks(peaks, params):\n",
    "    \n",
    "    def check_downward_peak_condition(index):\n",
    "        left = proximity_to_downward[index,0] <= params['downward_bases_threshold']\n",
    "        right = proximity_to_downward[index,1] <= params['downward_bases_threshold']\n",
    "    \n",
    "        if left and right: return False\n",
    "        if left and not right: return True \n",
    "        return True\n",
    "        \n",
    "    results = []\n",
    "    for i,peak in enumerate(peaks):\n",
    "        \n",
    "        prominence_condition = peak[3] >= params['min_prominence']\n",
    "        basewidth_condition = abs(peak[1] - peak[2]) <= params['max_basewidth']\n",
    "        downward_bases_condition = check_downward_peak_condition(i)\n",
    "        peak_proximity_condition = proximity_to_adjacent[i] >= params['proximity_threshold']\n",
    "        \n",
    "        if prominence_condition and basewidth_condition and downward_bases_condition and peak_proximity_condition:\n",
    "            results.append([peak[0], 'SKP'])\n",
    "        else:\n",
    "            results.append([peak[0], 'NSKP'])\n",
    "    return results\n",
    "\n",
    "def label_positives_negatives(predictions, truths):\n",
    "    TP = TN = FP = FN = 0\n",
    "    results = []\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        prediction = predictions[i][1]\n",
    "        truth = truths[i][2]\n",
    "        \n",
    "        if prediction == 'SKP':\n",
    "            if truth == 'NSKP':\n",
    "                FP +=1\n",
    "                results.append(predictions[i].append('FP'))\n",
    "            else: \n",
    "                TP +=1\n",
    "                results.append(predictions[i].append('TP'))\n",
    "        else:\n",
    "            if truth == 'NSKP':\n",
    "                TN +=1\n",
    "                results.append(predictions[i].append('TN'))\n",
    "            else: \n",
    "                FN +=1\n",
    "                results.append(predictions[i].append('FN'))\n",
    "        \n",
    "    return (TP,TN,FP,FN,results)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee6e7c6-9510-42f1-a816-6eaefa247e10",
   "metadata": {},
   "source": [
    "## Nested Cross Validation Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd4d44c-dbf3-4fcf-a057-57a464f5955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_start = datetime.datetime.now()\n",
    "\n",
    "split = 1\n",
    "\n",
    "for train_val_indices, test_indices in train_test_split_indices:\n",
    "    X_train, y_train = [cands[i] for i in train_val_indices], [truths[i] for i in train_val_indices]\n",
    "    X_test, y_test = [cands[i] for i in test_indices], [truths[i] for i in test_indices]\n",
    "    \n",
    "    num_pos_test= len(list(filter(lambda x: x[2] == \"SKP\", y_test)))\n",
    "    num_pos_train= len(list(filter(lambda x: x[2] == \"SKP\", y_train)))\n",
    "    \n",
    "    print(f'Num Pos in Test: {num_pos_test}')\n",
    "    print(f'Num Pos in Train: {num_pos_train}\\n')\n",
    "\n",
    "    if num_pos_test >= 1 and num_pos_train >= 1: \n",
    "    \n",
    "        max_fold_metric = 0 \n",
    "        max_result = None \n",
    "\n",
    "        print(\"Split: \",split)\n",
    "\n",
    "        split_start = datetime.datetime.now()\n",
    "\n",
    "        for iteration in range(iterations):\n",
    "\n",
    "            # Random grid search for hyperparams \n",
    "            params = {}\n",
    "\n",
    "            params['max_basewidth'] = np.random.randint(basewidth_range[0], basewidth_range[1]+1)\n",
    "            params['min_prominence'] = np.random.uniform(prominence_range[0], prominence_range[1])\n",
    "\n",
    "            params['downward_bases_threshold']= np.random.randint(downward_bases_range[0], downward_bases_range[1])\n",
    "\n",
    "            params['proximity_threshold']= np.random.randint(peak_proximity_bounds[0], peak_proximity_bounds[1])\n",
    "\n",
    "            predictions = classify_candidate_peaks(X_train, params)\n",
    "\n",
    "            TP,TN,FP,FN,results = label_positives_negatives(predictions, y_train)\n",
    "\n",
    "            TPR = 0 if TP == FN == 0 else TP/(TP + FN)\n",
    "            TNR = TN/(TN + FP)\n",
    "\n",
    "            bal_acc = (TPR + TNR)/2 \n",
    "\n",
    "            f1_score = 0 if TP == FP == FN == 0 else (2 * TP)/((2 * TP) + FP + FN)\n",
    "\n",
    "            acc = bal_acc\n",
    "            if iteration and iteration % int(iterations/10) == 0: print(\" {}/{} \".format(iteration, iterations), end = \"\")\n",
    "            if acc > max_fold_metric: \n",
    "                max_fold_metric = acc\n",
    "                max_result = copy.deepcopy(results)\n",
    "                best_params = copy.deepcopy(params)\n",
    "\n",
    "        # Test best parameters on testing data \n",
    "        test_predictions = classify_candidate_peaks(X_test, best_params)\n",
    "        TP,TN,FP,FN,results = label_positives_negatives(test_predictions, y_test)\n",
    "\n",
    "        cfmx = confusion_matrix([row[2] for row in y_test],[row[1] for row in test_predictions], labels = ['NSKP', 'SKP'])\n",
    "        accumulated_cfmxs[split] = copy.deepcopy(cfmx)\n",
    "\n",
    "        TPR = 0 if TP == FN == 0 else TP/(TP + FN)\n",
    "        TNR = TN/(TN + FP)\n",
    "\n",
    "        precision = 0 if TP == FP == 0 else TP/(TP + FP)\n",
    "\n",
    "        bal_acc = (TPR + TNR)/2 \n",
    "\n",
    "        f1_score = 0 if TP == FP == FN == 0 else (2 * TP)/((2 * TP) + FP + FN)\n",
    "\n",
    "        print('\\nSplit: {}  F1: {:.4f} BA: {:.4f}  Params: {}  TP: {} TN: {} FP: {} FN: {}  Time: {}'.format(split, f1_score, bal_acc, best_params, TP, TN, FP, FN, datetime.datetime.now() - split_start))\n",
    "        accumulated_test_metrics[split] = {'f1': f1_score, 'ba' : bal_acc, 'tpr': TPR, 'tnr' : TNR, 'precision': precision} # Record test metrics of each split\n",
    "        accumulated_test_results[split] = copy.deepcopy(results) # Record test results (FP,FN,TP,TN for each datapoint) for each split\n",
    "        accumulated_best_params[split] = copy.deepcopy(best_params) # Record params uses in testing for each split\n",
    "\n",
    "        split+=1\n",
    "\n",
    "# Display Resuls \n",
    "mean_f1 = 0\n",
    "mean_ba = 0 \n",
    "mean_sensitivity = 0\n",
    "mean_specificity = 0 \n",
    "mean_precision = 0\n",
    "\n",
    "for key in accumulated_test_metrics:\n",
    "    metrics = accumulated_test_metrics[key]\n",
    "    \n",
    "    mean_f1+=metrics['f1']\n",
    "    mean_ba+=metrics['ba']\n",
    "    mean_sensitivity += metrics['tpr']\n",
    "    mean_specificity += metrics['tnr']\n",
    "    mean_precision += metrics['precision']\n",
    "\n",
    "print(\"Mean Test F1: \", mean_f1/len(accumulated_test_metrics))\n",
    "print(\"Mean Test BA: \", mean_ba/len(accumulated_test_metrics))\n",
    "print(\"Mean Test TPR: \", mean_sensitivity/len(accumulated_test_metrics))\n",
    "print(\"Mean Test TNR: \", mean_specificity/len(accumulated_test_metrics))\n",
    "print(\"Mean Test Precision: \", mean_precision/len(accumulated_test_metrics))\n",
    "\n",
    "print(\"Training time: \", datetime.datetime.now() - overall_start)\n",
    "\n",
    "# Pickle best params\n",
    "with open('./Experimental_Results/fDOM_SKP/best_params.pkl', 'wb') as pck_file:\n",
    "    pickle.dump(accumulated_best_params, pck_file)\n",
    "    pck_file.close()\n",
    "    \n",
    "# Pickle test results\n",
    "with open('./Experimental_Results/fDOM_SKP/test_results.pkl', 'wb') as pck_file:\n",
    "    pickle.dump(accumulated_test_results, pck_file)\n",
    "    pck_file.close()\n",
    "\n",
    "# Pickle test metrics\n",
    "with open('./Experimental_Results/fDOM_SKP/test_metrics.pkl', 'wb') as pck_file:\n",
    "    pickle.dump(accumulated_test_metrics, pck_file)\n",
    "    pck_file.close()\n",
    "\n",
    "\n",
    "# Create and display confusion matrices\n",
    "mean_cfmx = np.zeros((2,2))\n",
    "for key in accumulated_cfmxs.keys():\n",
    "    mean_cfmx += accumulated_cfmxs[key]\n",
    "mean_cfmx = mean_cfmx / len(accumulated_cfmxs)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.title(label = 'fDOM Skyrocketing Peaks')\n",
    "sn.set(font_scale = 1.5)\n",
    "sn.heatmap( pd.DataFrame(mean_cfmx,index = ['Negative', 'Positive'], columns = ['Negative','Positive']), annot = True, annot_kws ={\"size\": 16})\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.title(label = 'fDOM Skyrocketing Peaks')\n",
    "\n",
    "sn.set(font_scale = 1.5)\n",
    "sn.heatmap( pd.DataFrame(mean_cfmx.astype('float') / mean_cfmx.sum(axis=1)[:, np.newaxis],index = ['Negative', 'Positive'], columns = ['Negative','Positive']), annot = True, annot_kws ={\"size\": 16})\n",
    "plt.xlabel('Ground Truths')\n",
    "plt.ylabel('Predictions')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92f411cd-910b-4676-a4db-04fb6f289bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fDOM_cands = cands\n",
    "s_indices = detect_stage_rises(stage_data[:,1])\n",
    "turb_cand_params = {'prom' : [6,None],\n",
    "                    'width': [None, None],\n",
    "                    'wlen' : 200,\n",
    "                    'dist' : 1,\n",
    "                    'rel_h': .6}\n",
    "\n",
    "# Get fDOM and turb candiate peaks\n",
    "turb_peaks, turb_props = get_candidates(turb_data, turb_cand_params)\n",
    "\n",
    "# Remove peaks that occur during a flat plateau \n",
    "turb_flat_plat = detect_flat_plat(turb_data, 100, 40)\n",
    "turb_flat_plat_indxs = []\n",
    "for i in range(turb_flat_plat.shape[0]):\n",
    "    if turb_flat_plat[i] == 1:\n",
    "        turb_flat_plat_indxs.append(i)\n",
    "\n",
    "take_indices = []\n",
    "for i,peak in enumerate(turb_peaks):\n",
    "    if peak not in turb_flat_plat_indxs:\n",
    "        take_indices.append(i)\n",
    "\n",
    "turb_peaks = np.take(turb_peaks, take_indices)\n",
    "for key in turb_props:\n",
    "    turb_props[key] = np.take(turb_props[key], take_indices)\n",
    "\n",
    "# Iterate through peaks and turn into short 3 point \"events\" by flagging the data point to either side of a peak\n",
    "fDOM_events = []\n",
    "fDOM_lb = []\n",
    "fDOM_rb = []\n",
    "\n",
    "for i,cand in enumerate(fDOM_cands):\n",
    "            fDOM_events.append(np.array((fDOM_data[cand[0]])))\n",
    "            fDOM_lb.append(fDOM_data[math.floor(cand[1]),0])\n",
    "            fDOM_rb.append(fDOM_data[math.ceil(cand[2]),0])\n",
    "            \n",
    "fDOM_lb = list(set(fDOM_lb))\n",
    "fDOM_lb.sort()\n",
    "fDOM_rb = list(set(fDOM_rb))\n",
    "fDOM_rb.sort()\n",
    "\n",
    "turb_events = []\n",
    "turb_lb = []\n",
    "turb_rb = []\n",
    "for i,peak in enumerate(turb_peaks):\n",
    "            turb_events.append(np.array((turb_data[peak])))\n",
    "            turb_lb.append(turb_data[math.floor(turb_props['left_ips'][i]),0])\n",
    "            turb_rb.append(turb_data[math.ceil(turb_props['right_ips'][i]),0])\n",
    "            \n",
    "turb_lb = list(set(turb_lb))\n",
    "turb_lb.sort()\n",
    "turb_rb = list(set(turb_rb))\n",
    "turb_rb.sort()            \n",
    "\n",
    "fDOM_merged = dp.merge_data(fDOM_data, fDOM_events, 'not_sky_peak', '')\n",
    "turb_merged = dp.merge_data(turb_data, turb_events, 't_opp', '')\n",
    "\n",
    "fDOM_merged = dp.merge_additional_data(fDOM_merged, fDOM_lb, 'left_base')\n",
    "fDOM_merged = dp.merge_additional_data(fDOM_merged, fDOM_rb, 'right_base')\n",
    "\n",
    "turb_merged = dp.merge_additional_data(turb_merged, turb_lb, 'left_base')\n",
    "turb_merged = dp.merge_additional_data(turb_merged, turb_rb, 'right_base')\n",
    "\n",
    "\n",
    "stage_edge_data = dp.stage_rises_to_data(s_indices, stage_data)\n",
    "stage_data_merged = dp.merge_data(stage_data, stage_edge_data, 'rise','')\n",
    "\n",
    "dm.write_data_to_trainset(fDOM_merged,\n",
    "                          stage_data_merged,\n",
    "                          turb_merged,\n",
    "                          '../Data/temp_plotting/fDOM_sky_0k-100k.csv',\n",
    "                          True,\n",
    "                          True,\n",
    "                          0,\n",
    "                          100000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
