{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc8621b2-fced-4453-8ad6-4bb864e72d63",
   "metadata": {},
   "source": [
    "# Detect fDOM Plummeting Peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494cff5c-6d46-47c1-bac0-ac5ccac63edb",
   "metadata": {},
   "source": [
    "## Rules for plummeting peaks\n",
    "\n",
    "Downward peak where: \n",
    "- Base width smaller than threshold \n",
    "- Prominence larger than a threshold \n",
    "- No corresponding peak in turbidity in a certain range \n",
    "- No other downward peaks in certain proximity (this would make the plummeting peak local fluctuation...) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03dcbb1-0bc3-4819-9275-0df0c1e9e3e7",
   "metadata": {},
   "source": [
    "## Import Tools, Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd747e72-5140-4804-b0e8-204db2f5de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and data \n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from os.path import dirname, join as pjoin\n",
    "import datetime\n",
    "import csv\n",
    "import math\n",
    "import Tools.data_processing as dp\n",
    "import Tools.data_movement as dm \n",
    "from Tools.auxiliary_functions import get_candidates, detect_flat_plat, detect_stage_rises\n",
    "\n",
    "fDOM_data = dm.read_in_preprocessed_timeseries('Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv')\n",
    "stage_data = dm.read_in_preprocessed_timeseries('Data/converted_data/julian_format/stage_10.1.11-1.1.19.csv')\n",
    "turb_data = dm.read_in_preprocessed_timeseries('Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv')\n",
    "stage_data = dp.align_stage_to_fDOM(fDOM_data, stage_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48b725a-cf30-440c-8020-9e62dfc9b05d",
   "metadata": {},
   "source": [
    "## Flip fDOM timeseries to detect plummeting peaks. Get candidate set for plummeting peaks\n",
    "#### Flipping fDOM is surely inefficient and \"peaks\" could be detected in the unflipped timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42a3a57-8b9c-47e4-80e5-7a0e0fed5a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip timeseries \n",
    "flipped_fDOM = dp.flip_timeseries(copy.deepcopy(fDOM_data))\n",
    "\n",
    "# Get fDOM plummeting peak candidate set using scipy find_peaks()\n",
    "prominence_range = [3,None] # peaks must have at least prominence 3\n",
    "width_range = [None,10] # peaks cannot have a base width of more than 5\n",
    "wlen = 100 \n",
    "distance = 1 \n",
    "rel_height =.6\n",
    "\n",
    "# Get list of all peaks that could possibly be plummeting peaks\n",
    "peaks, props = find_peaks(flipped_fDOM[:,1],\n",
    "                          height = (None, None),\n",
    "                          threshold = (None,None),\n",
    "                          distance = distance,\n",
    "                          prominence = prominence_range,\n",
    "                          width = width_range,\n",
    "                          wlen = wlen,\n",
    "                          rel_height = rel_height)\n",
    "\n",
    "# Form candidate set from returned information\n",
    "cands = [[peak, math.floor(props['left_ips'][i]), math.ceil(props['right_ips'][i]),props['prominences'][i]] for i,peak in enumerate(peaks)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c077d21-b778-45bc-87a5-3ea29429dee4",
   "metadata": {},
   "source": [
    "## Import ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be6b075-8522-4e67-a394-4759d23c6d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and process ground truth\n",
    "truth_fname = 'Data/labeled_data/ground_truths/fDOM/fDOM_PLP/julian_time/fDOM_PLP_0k-300k.csv'\n",
    "\n",
    "with open(truth_fname, 'r', newline = '') as f:\n",
    "    \n",
    "        reader = csv.reader(f, delimiter = ',')\n",
    "        # truth entries in form: ['timestamp_of_peak', 'value_of_peak','label_of_peak','idx_of_peak']\n",
    "        next(reader)\n",
    "        truths = [[float(row[0]), float(row[1]), row[2], int(row[3])] for row in reader] \n",
    "        f.close()   \n",
    "\n",
    "assert(len(truths) == len(cands))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2326d3f3-7388-420f-b577-949e5a4100df",
   "metadata": {},
   "source": [
    "## Detect turbidity peaks - necessary to distinquish between plummeting peak and interference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fda0af-7e67-48c6-9908-d70a2e8dcd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "turb_peak_params = {'prom' : [6,None],\n",
    "                    'width': [None, None],\n",
    "                    'wlen' : 200,\n",
    "                    'dist' : 1,\n",
    "                    'rel_h': .6}\n",
    "\n",
    "# Get fDOM and turb candiate peaks\n",
    "turb_peaks, _ = get_candidates(turb_data, turb_peak_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4b7f2a-0fc4-4943-b41a-3cb213a64621",
   "metadata": {},
   "source": [
    "## Preprocess candidates to avoid repeated work in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fd6bd9-a205-40bb-81f3-b2bafebe1d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each peak, determine how far it is from the closest adjacent peak\n",
    "proximity_to_adjacent = np.zeros((len(peaks)))\n",
    "\n",
    "for i in range(len(peaks)):\n",
    "    x = y = fDOM_data.shape[0] + 1 \n",
    "    if i > 0: \n",
    "        x = abs(peaks[i] - peaks[i-1])\n",
    "    if i < len(peaks) - 1:\n",
    "        y = abs(peaks[i] - peaks[i+1])\n",
    "        \n",
    "    proximity_to_adjacent[i] = min(x,y)\n",
    "\n",
    "    \n",
    "# For each peak, determine how closely it corresponds to the closest turbidity peak for the interference check \n",
    "\n",
    "# This is not an efficient way to search, but this operation is performed one time... so no sweat\n",
    "proximity_to_interference = np.zeros((len(peaks),2))\n",
    "\n",
    "for i, peak in enumerate(peaks): \n",
    "    x = y = fDOM_data.shape[0] + 1 \n",
    "    for turb_peak in turb_peaks:\n",
    "        if turb_peak <= peak: \n",
    "            x = min(abs(peak-turb_peak),x)\n",
    "        else:\n",
    "            y = min(abs(peak-turb_peak),y)\n",
    "    proximity_to_interference[i,0] = x\n",
    "    proximity_to_interference[i,1] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505c2f11-7b95-44aa-a753-48e94352ed15",
   "metadata": {},
   "source": [
    "## Define helper functions and set of training parameters, create training/testing splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f719765-f78f-4bda-a99a-0941dca8d890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameter ranges and training parameters and helper function\n",
    "\n",
    "iterations = 7000\n",
    "num_splits = 5\n",
    "\n",
    "basewidth_range = (1, 10)\n",
    "prominence_range= (5, 20)\n",
    "\n",
    "peak_proximity_bounds = (1,20)\n",
    "turb_interference_bounds = (0,10)\n",
    "\n",
    "\n",
    "accumulated_test_metrics = {}\n",
    "\n",
    "accumulated_test_results = {}\n",
    "\n",
    "accumulated_best_params = {}\n",
    "\n",
    "accumulated_cfmxs = {}\n",
    "\n",
    "train_test_split_indices = TimeSeriesSplit(num_splits).split(cands)\n",
    "\n",
    "def classify_candidate_peaks(peaks, params):\n",
    "        \n",
    "    results = []\n",
    "    for i,peak in enumerate(peaks):\n",
    "        prominence_condition = peak[3] >= params['min_prominence']\n",
    "        \n",
    "        basewidth_condition = abs(peak[1] - peak[2]) <= params['max_basewidth']\n",
    "        \n",
    "        interference_condition = (proximity_to_interference[i,0] >= params['interference_x_proximity'] \n",
    "            and proximity_to_interference[i,1] >= params['interference_y_proximity'])\n",
    "        \n",
    "        proximity_condition = proximity_to_adjacent[i] >= params['proximity_threshold']\n",
    "        \n",
    "        if prominence_condition and basewidth_condition and interference_condition and proximity_condition:\n",
    "            results.append([peak[0], 'PLP'])\n",
    "        else:\n",
    "            results.append([peak[0], 'NPLP'])\n",
    "    return results\n",
    "\n",
    "def label_positives_negatives(predictions, truths):\n",
    "    TP = TN = FP = FN = 0\n",
    "    results = []\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        prediction = predictions[i][1]\n",
    "        truth = truths[i][2]\n",
    "        if prediction == 'PLP':\n",
    "            if truth == 'NPLP':\n",
    "                FP +=1\n",
    "                results.append(predictions[i].append('FP'))\n",
    "            else: \n",
    "                TP +=1\n",
    "                results.append(predictions[i].append('TP'))\n",
    "        else:\n",
    "            if truth == 'NPLP':\n",
    "                TN +=1\n",
    "                results.append(predictions[i].append('TN'))\n",
    "            else: \n",
    "                FN +=1\n",
    "                results.append(predictions[i].append('FN'))\n",
    "        \n",
    "    return (TP,TN,FP,FN,results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f435be-2919-4a3a-b573-0692f663410b",
   "metadata": {},
   "source": [
    "## Nested Cross Validation Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4667fb20-dff7-453f-9491-9639092154bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_start = datetime.datetime.now()\n",
    "\n",
    "split = 1\n",
    "divide_by_zero_errs = 0\n",
    "for train_val_indices, test_indices in train_test_split_indices:\n",
    "    X_train, y_train = [cands[i] for i in train_val_indices], [truths[i] for i in train_val_indices]\n",
    "    X_test, y_test = [cands[i] for i in test_indices], [truths[i] for i in test_indices]\n",
    "\n",
    "    max_fold_metric = 0 \n",
    "    max_result = None \n",
    "\n",
    "    print(\"Split: \",split)\n",
    "\n",
    "    split_start = datetime.datetime.now()\n",
    "    num_pos_test= len(list(filter(lambda x: x[2] == \"PLP\", y_test)))\n",
    "    num_pos_train= len(list(filter(lambda x: x[2] == \"PLP\", y_train)))\n",
    "\n",
    "    print(f'Num Pos in Test: {num_pos_test}')\n",
    "    print(f'Num Pos in Train: {num_pos_train}\\n')\n",
    "\n",
    "    if num_pos_test >= 1 and num_pos_train >= 1: \n",
    "\n",
    "        for iteration in range(iterations):\n",
    "\n",
    "            # Random grid search for hyperparams \n",
    "            params = {}\n",
    "\n",
    "            params['max_basewidth'] = np.random.randint(basewidth_range[0], basewidth_range[1]+1)\n",
    "            params['min_prominence'] = np.random.uniform(prominence_range[0], prominence_range[1])\n",
    "\n",
    "            params['interference_x_proximity'] = np.random.randint(turb_interference_bounds[0], turb_interference_bounds[1])\n",
    "            params['interference_y_proximity'] = np.random.randint(turb_interference_bounds[0], turb_interference_bounds[1])\n",
    "\n",
    "            params['proximity_threshold']= np.random.randint(peak_proximity_bounds[0], peak_proximity_bounds[1])\n",
    "\n",
    "            predictions = classify_candidate_peaks(X_train, params)\n",
    "\n",
    "            TP,TN,FP,FN,results = label_positives_negatives(predictions, y_train)\n",
    "\n",
    "            TPR = 0 if TP == FN == 0 else TP/(TP + FN)\n",
    "            TNR = TN/(TN + FP)\n",
    "\n",
    "            bal_acc = (TPR + TNR)/2 \n",
    "\n",
    "            f1_score = 0 if TP == FP == FN == 0 else (2 * TP)/((2 * TP) + FP + FN)\n",
    "\n",
    "            acc = bal_acc\n",
    "            if iteration and iteration % int(iterations/10) == 0: print(\" {}/{} \".format(iteration, iterations), end = \"\")\n",
    "            if acc > max_fold_metric: \n",
    "                max_fold_metric = acc\n",
    "                max_result = copy.deepcopy(results)\n",
    "                best_params = copy.deepcopy(params)\n",
    "\n",
    "        # Test best parameters on testing data \n",
    "        test_predictions = classify_candidate_peaks(X_test, best_params)\n",
    "        TP,TN,FP,FN,results = label_positives_negatives(test_predictions, y_test)\n",
    "\n",
    "        cfmx = confusion_matrix([row[2] for row in y_test],[row[1] for row in test_predictions], labels = ['NPLP', 'PLP'])\n",
    "        accumulated_cfmxs[split] = copy.deepcopy(cfmx)\n",
    "\n",
    "        TPR = 0 if TP == FN == 0 else TP/(TP + FN)\n",
    "        TNR = TN/(TN + FP)\n",
    "\n",
    "        precision = 0 if TP == FP == 0 else TP/(TP + FP)\n",
    "\n",
    "        bal_acc = (TPR + TNR)/2 \n",
    "\n",
    "        f1_score = 0 if TP == FP == FN == 0 else (2 * TP)/((2 * TP) + FP + FN)\n",
    "\n",
    "        print('\\nSplit: {}  F1: {:.4f} BA: {:.4f}  Params: {}  TP: {} TN: {} FP: {} FN: {}  Time: {}'.format(split, f1_score, bal_acc, best_params, TP, TN, FP, FN, datetime.datetime.now() - split_start))\n",
    "        accumulated_test_metrics[split] = {'f1': f1_score, 'ba' : bal_acc, 'tpr': TPR, 'tnr' : TNR, 'precision': precision} # Record test metrics of each split\n",
    "        accumulated_test_results[split] = copy.deepcopy(results) # Record test results (FP,FN,TP,TN for each datapoint) for each split\n",
    "        accumulated_best_params[split] = copy.deepcopy(best_params) # Record params uses in testing for each split\n",
    "\n",
    "        split+=1\n",
    "\n",
    "\n",
    "print(divide_by_zero_errs)\n",
    "# Display Resuls \n",
    "mean_f1 = 0\n",
    "mean_ba = 0 \n",
    "mean_sensitivity = 0\n",
    "mean_specificity = 0 \n",
    "mean_precision = 0\n",
    "\n",
    "for key in accumulated_test_metrics:\n",
    "    metrics = accumulated_test_metrics[key]\n",
    "    \n",
    "    mean_f1+=metrics['f1']\n",
    "    mean_ba+=metrics['ba']\n",
    "    mean_sensitivity += metrics['tpr']\n",
    "    mean_specificity += metrics['tnr']\n",
    "    mean_precision += metrics['precision']\n",
    "\n",
    "print(\"Mean Test F1: \", mean_f1/len(accumulated_test_metrics))\n",
    "print(\"Mean Test BA: \", mean_ba/len(accumulated_test_metrics))\n",
    "print(\"Mean Test TPR: \", mean_sensitivity/len(accumulated_test_metrics))\n",
    "print(\"Mean Test TNR: \", mean_specificity/len(accumulated_test_metrics))\n",
    "print(\"Mean Test Precision: \", mean_precision/len(accumulated_test_metrics))\n",
    "\n",
    "print(\"Training time: \", datetime.datetime.now() - overall_start)\n",
    "\n",
    "# Pickle best params\n",
    "with open('Anomaly_Detection/Experimental_Results/fDOM_PLP/best_params.pkl', 'wb') as pck_file:\n",
    "    pickle.dump(accumulated_best_params, pck_file)\n",
    "    pck_file.close()\n",
    "    \n",
    "# Pickle test results\n",
    "with open('Anomaly_Detection/Experimental_Results/fDOM_PLP/test_results.pkl', 'wb') as pck_file:\n",
    "    pickle.dump(accumulated_test_results, pck_file)\n",
    "    pck_file.close()\n",
    "\n",
    "# Pickle test metrics\n",
    "with open('Anomaly_Detection/Experimental_Results/fDOM_PLP/test_metrics.pkl', 'wb') as pck_file:\n",
    "    pickle.dump(accumulated_test_metrics, pck_file)\n",
    "    pck_file.close()\n",
    "    \n",
    "mean_cfmx = np.zeros((2,2))\n",
    "for key in accumulated_cfmxs.keys():\n",
    "    mean_cfmx += accumulated_cfmxs[key]\n",
    "mean_cfmx = mean_cfmx / len(accumulated_cfmxs)\n",
    "print(len(accumulated_cfmxs))\n",
    "print(mean_cfmx)\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.title(label = 'fDOM Plummeting Peak')\n",
    "\n",
    "sn.set(font_scale = 1.5)\n",
    "sn.heatmap( pd.DataFrame(mean_cfmx.astype('float') / mean_cfmx.sum(axis=1)[:, np.newaxis],index = ['Negative', 'Positive'], columns = ['Negative','Positive']), annot = True, annot_kws ={\"size\": 16})\n",
    "plt.xlabel('Ground Truths')\n",
    "plt.ylabel('Predictions')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.title(label = 'fDOM Plummeting Peak')\n",
    "\n",
    "sn.set(font_scale = 1.5)\n",
    "sn.heatmap( pd.DataFrame(mean_cfmx,index = ['Negative', 'Positive'], columns = ['Negative','Positive']), annot = True, annot_kws ={\"size\": 16})\n",
    "plt.xlabel('Ground Truths')\n",
    "plt.ylabel('Predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf216a29-c9b0-41e3-b512-dfef624c1408",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada5b320-08c3-417e-99c7-a38b39887f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "turb_cand_params = {'prom' : [6,None],\n",
    "                    'width': [None, None],\n",
    "                    'wlen' : 200,\n",
    "                    'dist' : 1,\n",
    "                    'rel_h': .6}\n",
    "\n",
    "# Get fDOM and turb candiate peaks\n",
    "turb_peaks, turb_props = get_candidates(turb_data, turb_cand_params)\n",
    "\n",
    "# Iterate through peaks and turn into short 3 point \"events\" by flagging the data point to either side of a peak\n",
    "fDOM_events = []\n",
    "fDOM_lb = []\n",
    "fDOM_rb = []\n",
    "\n",
    "for i,cand in enumerate(cands):\n",
    "            fDOM_events.append(np.array((flipped_fDOM[cand[0]])))\n",
    "            fDOM_lb.append(flipped_fDOM[math.floor(cand[1]),0])\n",
    "            fDOM_rb.append(flipped_fDOM[math.ceil(cand[2]),0])\n",
    "            \n",
    "fDOM_lb = list(set(fDOM_lb))\n",
    "fDOM_lb.sort()\n",
    "fDOM_rb = list(set(fDOM_rb))\n",
    "fDOM_rb.sort()\n",
    "\n",
    "turb_events = []\n",
    "turb_lb = []\n",
    "turb_rb = []\n",
    "\n",
    "for i,peak in enumerate(turb_peaks):\n",
    "            turb_events.append(np.array((turb_data[peak])))\n",
    "            turb_lb.append(turb_data[math.floor(turb_props['left_ips'][i]),0])\n",
    "            turb_rb.append(turb_data[math.ceil(turb_props['right_ips'][i]),0])\n",
    "            \n",
    "turb_lb = list(set(turb_lb))\n",
    "turb_lb.sort()\n",
    "turb_rb = list(set(turb_rb))\n",
    "turb_rb.sort()            \n",
    "\n",
    "fDOM_merged = dp.merge_data(flipped_fDOM, fDOM_events, 'intf', '')\n",
    "turb_merged = dp.merge_data(turb_data, turb_events, 't_opp', '')\n",
    "\n",
    "fDOM_merged = dp.merge_additional_data(fDOM_merged, fDOM_lb, 'left_base')\n",
    "fDOM_merged = dp.merge_additional_data(fDOM_merged, fDOM_rb, 'right_base')\n",
    "\n",
    "turb_merged = dp.merge_additional_data(turb_merged, turb_lb, 'left_base')\n",
    "turb_merged = dp.merge_additional_data(turb_merged, turb_rb, 'right_base')\n",
    "\n",
    "\n",
    "stage_data_merged = dp.merge_data(stage_data, [], '','')\n",
    "\n",
    "dm.write_data_to_trainset(fDOM_merged,\n",
    "                          stage_data_merged,\n",
    "                          turb_merged,\n",
    "                          'Data/temp_plotting/fDOM_plum_200k-300k.csv',\n",
    "                          True,\n",
    "                          True,\n",
    "                          200000,\n",
    "                          300000)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ed753961bdc37ee89b4275051722ceb8ec0b57b8793db9d189305c313070a7d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('srrw')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
