{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61c70487-5047-4d9d-b68f-381d8aa7b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and data \n",
    "import scipy.io as sio\n",
    "import copy\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from os.path import dirname, join as pjoin\n",
    "import datetime\n",
    "import csv\n",
    "import math\n",
    "import sys\n",
    "sys.path.insert(1,'../')\n",
    "import Tools.data_processing as dp\n",
    "import Tools.data_movement as dm \n",
    "from auxiliary_functions import extract_runoff, get_stage_events, detect_edges\n",
    "\n",
    "fDOM_raw_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv')\n",
    "stage_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/stage_10.1.11-1.1.19.csv')\n",
    "turb_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv')\n",
    "stage_data = dp.align_stage_to_fDOM(fDOM_raw_data, stage_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccf4b600-c70a-49c2-b2e8-fb1800251139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process stage rises so that each index displays distance to next stage rise in positive and negative direction\n",
    "s_indices = detect_stage_rises(stage_data[:,1])\n",
    "\n",
    "y = s_indices.shape[0] -1 \n",
    "s_indexed = np.zeros((s_indices.shape[0],2))\n",
    "x_count = -1 \n",
    "y_count = -1\n",
    "for x in range(s_indices.shape[0]):\n",
    "    # X Block \n",
    "    \n",
    "    # When x encounters first stage rise, start x counter\n",
    "    if x_count == -1 and s_indices[x] == 1:\n",
    "        x_count = 0\n",
    "    if x_count != -1:\n",
    "        if s_indices[x] == 1:\n",
    "            x_count = 0\n",
    "            s_indexed[x,0] = x_count\n",
    "        else:\n",
    "            x_count += 1\n",
    "            s_indexed[x,0] = x_count\n",
    "    else:\n",
    "        s_indexed[x,0] = -1\n",
    "            \n",
    "    # Y Block\n",
    "    if y_count == -1 and s_indices[y] == 1:\n",
    "        y_count = 0\n",
    "    if y_count != -1:\n",
    "        if s_indices[y] == 1:\n",
    "            y_count = 0\n",
    "            s_indexed[y,1] = y_count\n",
    "        else:\n",
    "            y_count += 1\n",
    "            s_indexed[y,1] = y_count\n",
    "    else: \n",
    "        s_indexed[y,1] = -1\n",
    "        \n",
    "    y-=1\n",
    "\n",
    "    # Get turb and fDOM peaks\n",
    "fDOM_cand_params = {'prom' : [8,None],\n",
    "                    'width': [5, None],\n",
    "                    'wlen' : 300,\n",
    "                    'dist' : 20,\n",
    "                    'rel_h': .6}\n",
    "\n",
    "turb_cand_params = {'prom' : [6,None],\n",
    "                    'width': [None, None],\n",
    "                    'wlen' : 200,\n",
    "                    'dist' : 1,\n",
    "                    'rel_h': .6}\n",
    "\n",
    "fDOM_peaks, fDOM_props = get_candidates(fDOM_raw_data, fDOM_cand_params)\n",
    "turb_peaks, turb_props = get_candidates(turb_data, turb_cand_params)\n",
    "turb_peaks_org = copy.deepcopy(turb_peaks)\n",
    "turb_props_org = copy.deepcopy(turb_props)\n",
    "\n",
    "# Remove peaks that occur during a flat plateau \n",
    "turb_flat_plat = detect_flat_plat(turb_data, 100, 40)\n",
    "turb_flat_plat_indxs = []\n",
    "for i in range(turb_flat_plat.shape[0]):\n",
    "    if turb_flat_plat[i] == 1:\n",
    "        turb_flat_plat_indxs.append(i)\n",
    "\n",
    "take_indices = []\n",
    "for i,peak in enumerate(turb_peaks):\n",
    "    if peak not in turb_flat_plat_indxs:\n",
    "        take_indices.append(i)\n",
    "\n",
    "turb_peaks = np.take(turb_peaks, take_indices)\n",
    "for key in turb_props:\n",
    "    turb_props[key] = np.take(turb_props[key], take_indices)\n",
    "# Convert peaks and props to useable structure and assign values from s_indexed\n",
    "# Each entry = [index_of_peak, left_ips, right_ips, X, Y, flag]\n",
    "fDOM_cand = [[peak, math.floor(fDOM_props['left_ips'][i]), math.ceil(fDOM_props['right_ips'][i]),s_indexed[peak,0], s_indexed[peak,1] ,None] for i,peak in enumerate(fDOM_peaks)] \n",
    "org_turb_cand = [[peak, math.floor(turb_props['left_ips'][i]), math.ceil(turb_props['right_ips'][i]),s_indexed[peak,0], s_indexed[peak,1] ,None] for i,peak in enumerate(turb_peaks)]\n",
    "\n",
    "def return_turb_cand():\n",
    "    turb_peaks, turb_props = get_candidates(turb_data, turb_cand_params)\n",
    "    \n",
    "    take_indices = []\n",
    "    for i,peak in enumerate(turb_peaks):\n",
    "        if peak not in turb_flat_plat_indxs:\n",
    "            take_indices.append(i)\n",
    "\n",
    "    turb_peaks = np.take(turb_peaks, take_indices)\n",
    "    for key in turb_props:\n",
    "        turb_props[key] = np.take(turb_props[key], take_indices)\n",
    "        \n",
    "    turb_cand = [[peak, math.floor(turb_props['left_ips'][i]), math.ceil(turb_props['right_ips'][i]),s_indexed[peak,0], s_indexed[peak,1] ,None] for i,peak in enumerate(turb_peaks)]\n",
    "    \n",
    "    return turb_cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab88e8aa-4cce-4b38-9827-3a1e448edc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "turb_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b70fd9b3-50b5-4973-a1ab-7c0b81d84b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num PP:  489\n",
      "Num PP:  103\n",
      "Num PP:  42\n",
      "Num PP:  14\n",
      "Num PP:  4\n",
      "Num PP:  1\n"
     ]
    }
   ],
   "source": [
    "x = 10\n",
    "y = 3\n",
    "\n",
    "flag = False\n",
    "count = 0\n",
    "while not flag: \n",
    "    count +=1\n",
    "    # Detect turb candidates \n",
    "    turb_cand = return_turb_cand()\n",
    "\n",
    "    # Label turb candidates as PP or Not PP\n",
    "    PP_count = 0\n",
    "    for peak in turb_cand:\n",
    "        if(peak[3] != -1 and peak[3] <= x) or (peak[4] != -1 and peak[4] <= y):\n",
    "            peak[5] = 'P'\n",
    "        else:\n",
    "            peak[5] = 'F'\n",
    "            PP_count +=1\n",
    "    \n",
    "    # If no PP, then set flag to True, will exit loop after this iteration\n",
    "    if not PP_count or count == 20:\n",
    "        flag = True\n",
    "    else: \n",
    "        print('Num PP: ', PP_count)\n",
    "        \n",
    "    turb_PP_peaks = list(filter(lambda peak: peak[5] == 'F', turb_cand))\n",
    "    \n",
    "    # For PP, interpolate turb_data from left_ips to right_ips\n",
    "    for peak in turb_PP_peaks:\n",
    "        \n",
    "        left_ips = peak[1]\n",
    "        right_ips = peak[2]\n",
    "        left_ips_val = turb_data[left_ips,1]\n",
    "        right_ips_val = turb_data[right_ips,1]\n",
    "        span = right_ips - left_ips\n",
    "        \n",
    "        increment = (right_ips_val - left_ips_val) / (span)\n",
    "        \n",
    "        j = 1\n",
    "        for i in range(left_ips+1, right_ips):\n",
    "            turb_data[i,1] = left_ips_val + (increment * j)\n",
    "            j+=1\n",
    "        \n",
    "# Plot data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc45b1f5-90aa-488f-97db-397505f1ff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stage rises\n",
    "fDOM_cand_params = {'prom' : [1.5,None],\n",
    "                    'width': [None, None],\n",
    "                    'wlen' : 200,\n",
    "                    'dist' : 1,\n",
    "                    'rel_h': .5}\n",
    "\n",
    "# turb_cand_params = {'prom' : [6,None],       These are the params that were used to label turb cand 0-100k\n",
    "#                     'width': [None, None],\n",
    "#                     'wlen' : 200,\n",
    "#                     'dist' : 1,\n",
    "#                     'rel_h': .6}\n",
    "\n",
    "turb_cand_params = {'prom' : [6,None],\n",
    "                    'width': [None, None],\n",
    "                    'wlen' : 200,\n",
    "                    'dist' : 1,\n",
    "                    'rel_h': .6}\n",
    "turb_org_data = dm.read_in_preprocessed_timeseries('../Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv')\n",
    "# Get fDOM and turb candiate peaks\n",
    "fDOM_peaks, fDOM_props = get_candidates(turb_org_data, turb_cand_params)\n",
    "\n",
    "# Remove peaks that occur during a flat plateau \n",
    "turb_flat_plat = detect_flat_plat(turb_data, 100, 40)\n",
    "turb_flat_plat_indxs = []\n",
    "for i in range(turb_flat_plat.shape[0]):\n",
    "    if turb_flat_plat[i] == 1:\n",
    "        turb_flat_plat_indxs.append(i)\n",
    "\n",
    "take_indices = []\n",
    "for i,peak in enumerate(turb_peaks_org):\n",
    "    if peak not in turb_flat_plat_indxs:\n",
    "        take_indices.append(i)\n",
    "\n",
    "turb_peaks_org = np.take(turb_peaks_org, take_indices)\n",
    "for key in turb_props_org:\n",
    "    turb_props_org[key] = np.take(turb_props_org[key], take_indices)\n",
    "\n",
    "# Iterate through peaks and turn into short 3 point \"events\" by flagging the data point to either side of a peak\n",
    "# fDOM_events = []\n",
    "# for peak in fDOM_peaks:\n",
    "#             fDOM_events.append(np.array((fDOM_raw_data[peak-1], fDOM_raw_data[peak], fDOM_raw_data[peak+1])))\n",
    "fDOM_events = []\n",
    "fDOM_lb = []\n",
    "fDOM_rb = []\n",
    "\n",
    "for i,peak in enumerate(fDOM_peaks):\n",
    "            fDOM_events.append(np.array((turb_org_data[peak])))\n",
    "#             fDOM_lb.append(fDOM_raw_data[fDOM_props['left_bases'][i],0])\n",
    "#             fDOM_rb.append(fDOM_raw_data[fDOM_props['right_bases'][i],0])\n",
    "            fDOM_lb.append(turb_org_data[math.floor(fDOM_props['left_ips'][i]),0])\n",
    "            fDOM_rb.append(turb_org_data[math.ceil(fDOM_props['right_ips'][i]),0])\n",
    "            \n",
    "fDOM_lb = list(set(fDOM_lb))\n",
    "fDOM_lb.sort()\n",
    "fDOM_rb = list(set(fDOM_rb))\n",
    "fDOM_rb.sort()\n",
    "\n",
    "turb_events = []\n",
    "turb_lb = []\n",
    "turb_rb = []\n",
    "for i,peak in enumerate(turb_peaks_org):\n",
    "            turb_events.append(np.array((turb_data[peak])))\n",
    "            turb_lb.append(turb_data[math.floor(turb_props_org['left_ips'][i]),0])\n",
    "            turb_rb.append(turb_data[math.ceil(turb_props_org['right_ips'][i]),0])\n",
    "            \n",
    "turb_lb = list(set(turb_lb))\n",
    "turb_lb.sort()\n",
    "turb_rb = list(set(turb_rb))\n",
    "turb_rb.sort()            \n",
    "\n",
    "fDOM_merged = dp.merge_data(turb_org_data, fDOM_events, 't_opp', '')\n",
    "turb_merged = dp.merge_data(turb_data, turb_events, 't_opp', '')\n",
    "\n",
    "fDOM_merged = merge_additional_data(fDOM_merged, fDOM_lb, 'left_base')\n",
    "fDOM_merged = merge_additional_data(fDOM_merged, fDOM_rb, 'right_base')\n",
    "\n",
    "turb_merged = merge_additional_data(turb_merged, turb_lb, 'left_base')\n",
    "turb_merged = merge_additional_data(turb_merged, turb_rb, 'right_base')\n",
    "\n",
    "\n",
    "stage_edge_data = stage_rises_to_data(s_indices, stage_data)\n",
    "stage_data_merged = dp.merge_data(stage_data, stage_edge_data, 'rise','')\n",
    "\n",
    "dm.write_data_to_trainset(fDOM_merged,\n",
    "                          stage_data_merged,\n",
    "                          turb_merged,\n",
    "                          '../Data/anomaly_data/temp_data/turb_corr_test_0k-100k.csv',\n",
    "                          True,\n",
    "                          True,\n",
    "                          0,\n",
    "                          100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0506f0a-efef-445e-8582-10fd91ec605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(data: np.ndarray, params):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return all peaks that should be scanned for out or order peaks(oop)\n",
    "    We don't want to return skyrocketing peaks/local fluctuations - although they are oop, \n",
    "    they will be caught by their respective algorithms\n",
    "    \n",
    "    data   : timeseries to scan for peaks \n",
    "    return : peaks indentified with given hyperparameters and properties of those peaks\n",
    "    \"\"\"\n",
    "    peaks, props = find_peaks(data[:,1],\n",
    "                              height = (None, None),\n",
    "                              threshold = (None,None),\n",
    "                              distance = params['dist'],\n",
    "                              prominence = params['prom'],\n",
    "                              width = params['width'],\n",
    "                              wlen = params['wlen'],\n",
    "                              rel_height = params['rel_h'])\n",
    "    return peaks, props\n",
    "\n",
    "def event_from_props_ips(peaks : np.ndarray, props : dict, data : np.ndarray) -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate a multi-sample \"event\" given a peak and it's left/right interpolated positions\n",
    "    \n",
    "    peaks :  detected peaks indexs\n",
    "    props :  detected peaks properites\n",
    "    data :   timeseries that peaks are from \n",
    "    return : list of events generated from peaks\n",
    "    \"\"\"\n",
    "#     events = []\n",
    "#     prev_end = -1\n",
    "#     for i,index in enumerate(peaks):\n",
    "#         start = int(props['left_ips'][i])\n",
    "#         end = int(props['right_ips'][i])\n",
    "#         if i < len(peaks)-1:\n",
    "#             next_start = int(props['left_ips'][i+1])\n",
    "#             end_2 = end\n",
    "#             bw = end - start\n",
    "#             if end <= next_start:s\n",
    "#                 end = next_start-1\n",
    "#             if start >= end-1:\n",
    "#                 print('\\n\\n', i, bw)\n",
    "#                 print(next_start, start,end_2)\n",
    "#             else:\n",
    "#                 events.append(data[start: end + 1,:])\n",
    "#         else: \n",
    "#             events.append(data[start:end+1,:])\n",
    "#     return events\n",
    "    events = []\n",
    "    for i, index in enumerate(peaks):\n",
    "        events.append(data[int(props['left_ips'][i]):int(props['right_ips'][i])+1,:])\n",
    "    return events\n",
    "\n",
    "def peak_of_event(event : np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Detect the peak sample in an event\n",
    "    \n",
    "    event :  candidate event\n",
    "    return : sample that is peak of event\n",
    "    \"\"\"\n",
    "    max = event[0]\n",
    "    for entry in event:\n",
    "        if entry[1] > max[1]:\n",
    "            max = entry\n",
    "    return max\n",
    "\n",
    "def delete_missing_data_peaks(data, peaks, props, missing_file_path):\n",
    "    \"\"\" \n",
    "    Delete peaks that occur during time periods designated as \"missing data\"\n",
    "    \n",
    "    data:              timeseries that peaks occured in\n",
    "    peaks:             indices of peaks detected\n",
    "    props:             properties associated with each peak\n",
    "    missing_file_path: file path of missing date ranges\n",
    "    return:            filtered peaks and props\n",
    "    \"\"\"\n",
    "    with open(missing_file_path,newline='') as file:\n",
    "        reader = csv.reader(file, delimiter = ',')\n",
    "        time_list = []\n",
    "        for row in reader:\n",
    "            time_list.append([dp.datetime_to_julian(datetime.datetime.strptime(row[0],'%Y-%m-%d %H:%M:%S')),\n",
    "                              dp.datetime_to_julian(datetime.datetime.strptime(row[1],'%Y-%m-%d %H:%M:%S'))])\n",
    "            \n",
    "        # Identify and remove violating peaks \n",
    "        keep_indices = list(np.linspace(0,peaks.shape[0]-1,peaks.shape[0]))\n",
    "        for i,idx in enumerate(peaks): \n",
    "            time = data[idx,0]\n",
    "            for row in time_list: \n",
    "                if time >= row[0] and time <= row[1]:\n",
    "                    keep_indices.remove(i)\n",
    "                    break\n",
    "        \n",
    "        peaks = np.take(peaks,keep_indices,0)  \n",
    "        \n",
    "        # Remove properties for violating peaks\n",
    "        for key in props:\n",
    "            props[key] = np.take(props[key], keep_indices,0)\n",
    "        \n",
    "        return peaks, props\n",
    "    \n",
    "def detect_stage_rises(data : np.ndarray):\n",
    "    \"\"\"\n",
    "    Function detects rising edges in stage based on upon manually selected hyper parameters \n",
    "    \n",
    "    data:   values (no timestamps) of each data point in the timeseries\n",
    "    \n",
    "    return: array, that is length of data, representing whether each data point is\n",
    "            part of a rising edge or not: 1 = rising, 0 = not rising\n",
    "    \"\"\"\n",
    "    \n",
    "    large_window_size = 6\n",
    "    small_window_size = 3\n",
    "    threshold = .02\n",
    "    r = 2\n",
    "    signals = np.zeros(len(data)) \n",
    "    # Detect larger, smoother rising edges \n",
    "    for i in range(len(data) - large_window_size):\n",
    "        if round(round(data[i+large_window_size-1],r) - round(data[i],r),r) >= threshold:\n",
    "            signals[i:i+large_window_size] = 1\n",
    "    # Detect smaller, sharper rising edges \n",
    "    for i in range(len(data) - small_window_size):\n",
    "        if round(round(data[i+small_window_size-1],r) - round(data[i],r),r) >= threshold:\n",
    "            signals[i:i+small_window_size] = 1\n",
    "    # Remove erroneously marked points\n",
    "    for i in range(len(data) - 3):\n",
    "        if data[i+2] < data[i+1] and data[i+1] < data[i]:\n",
    "            signals[i+1] = 0\n",
    "    \n",
    "    # Remove edges that are less than 3 points long\n",
    "    for i in range(1,len(signals)-2):\n",
    "        if signals[i] == 1 and signals[i-1] == 0 and signals[i+1] == 0:\n",
    "            signals[i] = 0 \n",
    "        if signals[i] == 1 and signals[i-1] == 0 and signals[i+1] == 1 and signals[i+2] == 0:\n",
    "            signals[i] = 0\n",
    "    return signals\n",
    "\n",
    "def stage_rises_to_data(signals : np.ndarray, data : np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Givin stage rise signals, correlate these signals to the original data to \n",
    "    extract edges (timestamp, value)\n",
    "    \n",
    "    signals: signal for each data point in timeseries: 1 == rising edge, else 0 \n",
    "    data:    stage data timeseries \n",
    "    \n",
    "    return: 2d array of all rising edge data points\n",
    "    \n",
    "    \"\"\"\n",
    "    take_indices = []\n",
    "    for i in range(len(signals)):\n",
    "        if signals[i] == 1:\n",
    "            take_indices.append(i)\n",
    "    return np.take(data,take_indices, 0)\n",
    "\n",
    "def merge_additional_data(data : list, add_data, add_flag):\n",
    "    \"\"\"\n",
    "    Merge additional data to timeseries data that already has flags\n",
    "    \"\"\"\n",
    "    index = 0 \n",
    "    for i in range(len(data)):\n",
    "        if (index < len(add_data) and data[i][0] == add_data[index]):\n",
    "            data[i][2] = add_flag\n",
    "            index+=1\n",
    "    return data\n",
    "\n",
    "def detect_flat_plat(data: np.ndarray, window: int, threshold: int):\n",
    "    \"\"\" \n",
    "    Detect \"flat plateaus\" in a timeseries: consecutive datapoints as extreme amplitude\n",
    "    \"\"\"\n",
    "    \n",
    "    signals = np.zeros(data.shape[0])\n",
    "    \n",
    "    for i in range(data.shape[0] - window):\n",
    "        flag = True\n",
    "        for j in range(i, i + window):\n",
    "            if data[j,1] < threshold: \n",
    "                flag = False \n",
    "                break\n",
    "        if flag: \n",
    "            signals[i:i+window] = 1 \n",
    "    return signals\n",
    "\n",
    "def detect_pp(peaks : list[float],\n",
    "               stage_peaks : list[float],\n",
    "               hyperparams : dict):\n",
    "    \"\"\"\n",
    "    Use stage stage_peaks and given hyperparams to detect \n",
    "    phantom peak (pp) in the given timeseries \n",
    "    \n",
    "    peaks :       peaks from timeseries to detect pp in \n",
    "    stage_peaks : peaks in stage to compare against \n",
    "    hyperparams : dictionary containing threshold hyperparameters\n",
    "    \n",
    "    return :      list of peaks labeled either accepted (not pp) or rejected (pp)\n",
    "    \"\"\"\n",
    "    stage_before_threshold = hyperparams['stage_before_threshold']\n",
    "    stage_after_threshold = hyperparams['stage_after_threshold']\n",
    "    \n",
    "    labeled_peaks = []\n",
    "    \n",
    "    for peak in peaks:\n",
    "        stage_flag = False\n",
    "        for s_peak in stage_peaks:\n",
    "            if (s_peak[2] >= peak[2] - stage_before_threshold) and (s_peak[2] < peak[2] + stage_after_threshold):\n",
    "                stage_flag = True \n",
    "                break\n",
    "        if stage_flag: \n",
    "            labeled_peaks.append([peak,'accepted'])\n",
    "        else: \n",
    "            labeled_peaks.append([peak,'rejected'])\n",
    "    return labeled_peaks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
