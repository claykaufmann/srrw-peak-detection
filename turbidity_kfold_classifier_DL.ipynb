{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn import preprocessing\n",
    "from Anomaly_Detection.Deep_Learning.resnet import ResNet1D\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from Anomaly_Detection.Deep_Learning.datasets import turbAugOnlyDataset, collate_fn_pad\n",
    "import copy\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "WINDOW_SIZE = 15  # the size of each data segment\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# this is the number of epochs per fold, but because data is already batched,\n",
    "# when larger than 1, training takes a long time, make sure you have cuda for fast training\n",
    "EPOCHS = 5\n",
    "\n",
    "SPLITS = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data files\n",
    "fdom_raw_data = \"Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv\"\n",
    "stage_raw_data = \"Data/converted_data/julian_format/stage_10.1.11-1.1.19.csv\"\n",
    "turb_raw_data = \"Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv\"\n",
    "\n",
    "turb_labeled = \"Data/labeled_data/ground_truths/turb/turb_all_julian_0k-300k.csv\"\n",
    "\n",
    "fdom_raw_augmented = \"Data/augmented_data/turb/unlabeled/unlabeled_fdom.csv\"\n",
    "turb_labeled_augmented = \"Data/augmented_data/turb/labeled/labeled_turb_peaks.csv\"\n",
    "\n",
    "turb_augmented_raw_data = \"Data/augmented_data/turb/unlabeled/unlabeled_turb.csv\"\n",
    "\n",
    "stage_augmented_data_fn = \"Data/augmented_data/turb/unlabeled/unlabeled_stage.csv\"\n",
    "\n",
    "turb_fpt_lookup_path = \"Data/augmented_data/turb/fpt_lookup.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util functions\n",
    "def reset_weights(model):\n",
    "    for layer in model.children():\n",
    "        if hasattr(layer, \"reset_parameters\"):\n",
    "            print(f\"reset trainable params of layer = {layer}\")\n",
    "            layer.reset_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# get device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3169 candidates found in class-balanced augmented dataset.\n"
     ]
    }
   ],
   "source": [
    "classes = [\"NAP\", \"FPT\", \"PP\", \"SKP\"]\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "targets = le.fit_transform(classes)\n",
    "\n",
    "# this is for class balanced testing\n",
    "dataset = turbAugOnlyDataset(\n",
    "    le,\n",
    "    fdom_raw_augmented,\n",
    "    stage_augmented_data_fn,\n",
    "    turb_augmented_raw_data,\n",
    "    turb_labeled_augmented,\n",
    "    turb_fpt_lookup_path,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "results = {}\n",
    "\n",
    "tss = TimeSeriesSplit(SPLITS)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/17 [00:00<?, ?it/s]c:\\Users\\clayk\\Projects\\srrw-peak-detection\\Anomaly_Detection\\Deep_Learning\\datasets.py:40: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  label_list = torch.tensor(label_list, dtype=torch.int64)\n",
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for fold 0 has completed, now testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fold 0: 43 %\n",
      "--------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         FPT       0.43      0.11      0.18       105\n",
      "         NAP       0.73      0.46      0.57       142\n",
      "          PP       0.20      0.11      0.14       109\n",
      "         SKP       0.41      0.82      0.54       172\n",
      "\n",
      "    accuracy                           0.44       528\n",
      "   macro avg       0.44      0.38      0.36       528\n",
      "weighted avg       0.45      0.44      0.39       528\n",
      "\n",
      "FOLD 1\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for fold 1 has completed, now testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fold 1: 45 %\n",
      "--------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         FPT       0.33      0.01      0.02        90\n",
      "         NAP       0.65      0.57      0.61       135\n",
      "          PP       0.36      0.06      0.11       130\n",
      "         SKP       0.40      0.90      0.56       173\n",
      "\n",
      "    accuracy                           0.46       528\n",
      "   macro avg       0.44      0.38      0.32       528\n",
      "weighted avg       0.45      0.46      0.37       528\n",
      "\n",
      "FOLD 2\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for fold 2 has completed, now testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fold 2: 45 %\n",
      "--------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         FPT       0.00      0.00      0.00       112\n",
      "         NAP       0.72      0.62      0.67       133\n",
      "          PP       0.20      0.02      0.03       112\n",
      "         SKP       0.39      0.91      0.55       171\n",
      "\n",
      "    accuracy                           0.46       528\n",
      "   macro avg       0.33      0.39      0.31       528\n",
      "weighted avg       0.35      0.46      0.35       528\n",
      "\n",
      "FOLD 3\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▏         | 1/67 [00:01<01:09,  1.05s/it]"
     ]
    }
   ],
   "source": [
    "# K-fold training\n",
    "conf_matrices = {}\n",
    "accumulated_metrics = {}\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(tss.split(dataset)):\n",
    "    print(f\"FOLD {fold}\")\n",
    "\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampler=train_subsampler,\n",
    "        collate_fn=partial(collate_fn_pad, device=device),\n",
    "    )\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampler=test_subsampler,\n",
    "        collate_fn=partial(collate_fn_pad, device=device),\n",
    "    )\n",
    "\n",
    "    # init model\n",
    "    model = ResNet1D(\n",
    "        in_channels=4,\n",
    "        base_filters=64,\n",
    "        kernel_size=16,\n",
    "        stride=2,\n",
    "        n_block=48,\n",
    "        groups=1,  # check this\n",
    "        n_classes=len(classes),\n",
    "        downsample_gap=6,\n",
    "        increasefilter_gap=12,\n",
    "        verbose=False,\n",
    "    ).to(device)\n",
    "\n",
    "    # set model to use float instead of doubles to prevent errors\n",
    "    model = model.float()\n",
    "\n",
    "    # init optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(0, EPOCHS):\n",
    "        print(f\"Starting epoch {epoch + 1}\")\n",
    "\n",
    "        current_loss = 0\n",
    "\n",
    "        # prog bar\n",
    "        prog_bar = tqdm(trainloader, desc=\"Training\", leave=False)\n",
    "        for i, data in enumerate(prog_bar):\n",
    "            x = data[0].to(device)\n",
    "            y = data[1].squeeze().to(device)\n",
    "\n",
    "            if i == len(prog_bar) - 1:\n",
    "                break\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred = model(x.float())\n",
    "            loss = criterion(pred, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print stats\n",
    "            current_loss += loss.item()\n",
    "            if i % 500 == 499:\n",
    "                print(\"Loss after mini-batch %5d: %.3f\" % (i + 1, current_loss / 500))\n",
    "                current_loss = 0.0\n",
    "\n",
    "    # completed training, now test\n",
    "    print(f\"Training for fold {fold} has completed, now testing\")\n",
    "\n",
    "    # save best params\n",
    "    save_path = f\"Anomaly_Detection/Deep_Learning/results/turb/models/kfold/model-balanced-test-fold={fold}.pth\"\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    total, correct = 0, 0\n",
    "\n",
    "    # for checking correct and incorrect preds\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    prog_bar = tqdm(testloader, desc=\"Testing\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(prog_bar):\n",
    "            x = data[0].to(device)\n",
    "            y = data[1].squeeze().to(device)\n",
    "\n",
    "            outputs = model(x.float())\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for label, prediction in zip(y, preds):\n",
    "                # convert label and prediction to current vals\n",
    "                label = label.to('cpu')\n",
    "                prediction = prediction.to('cpu')\n",
    "                label = le.inverse_transform([label])[0]\n",
    "                prediction = le.inverse_transform([prediction])[0]\n",
    "\n",
    "                # for confusion matrices\n",
    "                y_pred.append(prediction)\n",
    "                y_true.append(label)\n",
    "\n",
    "                if label == prediction:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "        # Print rough general accuracy\n",
    "        print(\"Accuracy for fold %d: %d %%\" % (fold, 100.0 * correct / total))\n",
    "        print(\"--------------------------------\")\n",
    "        results[fold] = 100.0 * (correct / total)\n",
    "\n",
    "        # make classification report\n",
    "        acc_report = classification_report(y_true, y_pred)\n",
    "        print(acc_report)\n",
    "\n",
    "        # get acc score\n",
    "        acc_score = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "        f1 = f1_score(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            average=\"weighted\",\n",
    "        )\n",
    "\n",
    "        precision = precision_score(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            average=\"weighted\",\n",
    "        )\n",
    "\n",
    "        # make conf matrix\n",
    "        matrix = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "\n",
    "        # save conf matrix\n",
    "        conf_matrices[fold] = copy.deepcopy(matrix)\n",
    "\n",
    "        # save accumulated metrics\n",
    "        accumulated_metrics[fold] = {\n",
    "            \"f1\": f1,\n",
    "            \"acc\": acc_score,\n",
    "            \"ba\": bal_acc,\n",
    "            \"precision\": precision,\n",
    "        }\n",
    "\n",
    "# Print fold results\n",
    "print(\"\\n\")\n",
    "print(f\"K-FOLD CROSS VALIDATION RESULTS FOR {SPLITS} FOLDS\")\n",
    "print(\"--------------------------------\")\n",
    "sum = 0.0\n",
    "for key, value in results.items():\n",
    "    print(f\"Fold {key}: {value} %\")\n",
    "    sum += value\n",
    "print(f\"Average: {sum/len(results.items())} %\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save accumulated metrics\n",
    "mean_f1 = 0\n",
    "mean_ba = 0\n",
    "mean_precision = 0\n",
    "mean_acc = 0\n",
    "\n",
    "for key in accumulated_metrics:\n",
    "    metrics = accumulated_metrics[key]\n",
    "\n",
    "    mean_f1 += metrics[\"f1\"]\n",
    "    mean_ba += metrics[\"ba\"]\n",
    "    mean_precision += metrics[\"precision\"]\n",
    "    mean_acc += metrics[\"acc\"]\n",
    "\n",
    "print(\"Mean Test F1: \", mean_f1 / len(accumulated_metrics))\n",
    "print(\"Mean Test BA: \", mean_ba / len(accumulated_metrics))\n",
    "print(\"Mean Test Acc: \", mean_acc / len(accumulated_metrics))\n",
    "print(\"Mean Test Precision: \", mean_precision / len(accumulated_metrics))\n",
    "\n",
    "# make mean confusion matrix\n",
    "mean_cfmx = np.zeros((len(classes), len(classes)))\n",
    "for key in conf_matrices.keys():\n",
    "    mean_cfmx += conf_matrices[key]\n",
    "\n",
    "mean_cfmx = mean_cfmx / len(conf_matrices)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(label=\"Turbidity Peak Detection Totals Confusion Matrix KFold\")\n",
    "\n",
    "sn.set(font_scale=1.5)\n",
    "\n",
    "plot = sn.heatmap(\n",
    "    pd.DataFrame(\n",
    "        mean_cfmx,\n",
    "        index=classes,\n",
    "        columns=classes,\n",
    "    ),\n",
    "    annot=True,\n",
    "    annot_kws={\"size\": 16},\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Ground Truths\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.show()\n",
    "\n",
    "plot.get_figure().savefig(\n",
    "    \"Anomaly_Detection/Deep_Learning/results/turb/graphics/may-11-conf-totals-balanced-test-kfold.png\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(label=\"Turbidity Peak Detection Ratio Confusion Matrix KFold\")\n",
    "\n",
    "sn.set(font_scale=1.5)\n",
    "\n",
    "plot = sn.heatmap(\n",
    "    pd.DataFrame(\n",
    "        mean_cfmx.astype(\"float\") / mean_cfmx.sum(axis=1)[:, np.newaxis],\n",
    "        index=classes,\n",
    "        columns=classes,\n",
    "    ),\n",
    "    annot=True,\n",
    "    annot_kws={\"size\": 16},\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Ground Truths\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.show()\n",
    "\n",
    "plot.get_figure().savefig(\n",
    "    \"Anomaly_Detection/Deep_Learning/results/turb/graphics/may-11-conf-ratio-balanced-test-kfold.png\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "986aac83d3fed427d9eeee9b94ede52c609978b73496d2cfad058b08a24c9de9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('srrw')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
