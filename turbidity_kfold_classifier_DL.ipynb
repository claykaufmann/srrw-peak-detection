{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn import preprocessing\n",
    "from Anomaly_Detection.Deep_Learning.resnet import ResNet1D\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from Anomaly_Detection.Deep_Learning.datasets import turbAugOnlyDataset, collate_fn_pad\n",
    "import copy\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "WINDOW_SIZE = 15  # the size of each data segment\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# this is the number of epochs per fold, but because data is already batched,\n",
    "#   when larger than 1, training takes a long time\n",
    "EPOCHS = 3\n",
    "\n",
    "SPLITS = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data files\n",
    "fdom_raw_data = \"Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv\"\n",
    "stage_raw_data = \"Data/converted_data/julian_format/stage_10.1.11-1.1.19.csv\"\n",
    "turb_raw_data = \"Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv\"\n",
    "\n",
    "turb_labeled = \"Data/labeled_data/ground_truths/turb/turb_all_julian_0k-300k.csv\"\n",
    "\n",
    "fdom_raw_augmented = \"Data/augmented_data/turb/unlabeled/unlabeled_fdom.csv\"\n",
    "turb_labeled_augmented = \"Data/augmented_data/turb/labeled/labeled_turb_peaks.csv\"\n",
    "\n",
    "turb_augmented_raw_data = \"Data/augmented_data/turb/unlabeled/unlabeled_turb.csv\"\n",
    "\n",
    "stage_augmented_data_fn = \"Data/augmented_data/turb/unlabeled/unlabeled_stage.csv\"\n",
    "\n",
    "turb_fpt_lookup_path = \"Data/augmented_data/turb/fpt_lookup.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util functions\n",
    "def reset_weights(model):\n",
    "    for layer in model.children():\n",
    "        if hasattr(layer, \"reset_parameters\"):\n",
    "            print(f\"reset trainable params of layer = {layer}\")\n",
    "            layer.reset_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# get device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3169 candidates found in class-balanced augmented dataset.\n"
     ]
    }
   ],
   "source": [
    "classes = [\"NAP\", \"FPT\", \"PP\", \"SKP\"]\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "targets = le.fit_transform(classes)\n",
    "\n",
    "# this is for class balanced testing\n",
    "dataset = turbAugOnlyDataset(\n",
    "    le,\n",
    "    fdom_raw_augmented,\n",
    "    stage_augmented_data_fn,\n",
    "    turb_augmented_raw_data,\n",
    "    turb_labeled_augmented,\n",
    "    turb_fpt_lookup_path,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "results = {}\n",
    "\n",
    "tss = TimeSeriesSplit(SPLITS)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/17 [00:00<?, ?it/s]c:\\Users\\clayk\\Projects\\srrw-peak-detection\\Anomaly_Detection\\Deep_Learning\\datasets.py:40: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  label_list = torch.tensor(label_list, dtype=torch.int64)\n",
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for fold 0 has completed, now testing\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './results/models/turb/kfold/may-9-model-fold=0.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\clayk\\Projects\\srrw-peak-detection\\turbidity_kfold_classifier_DL.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/clayk/Projects/srrw-peak-detection/turbidity_kfold_classifier_DL.ipynb#ch0000007?line=75'>76</a>\u001b[0m \u001b[39m# save best params\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/clayk/Projects/srrw-peak-detection/turbidity_kfold_classifier_DL.ipynb#ch0000007?line=76'>77</a>\u001b[0m save_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./results/models/turb/kfold/may-9-model-fold=\u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m}\u001b[39;00m\u001b[39m.pth\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/clayk/Projects/srrw-peak-detection/turbidity_kfold_classifier_DL.ipynb#ch0000007?line=77'>78</a>\u001b[0m torch\u001b[39m.\u001b[39;49msave(model\u001b[39m.\u001b[39;49mstate_dict(), save_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/clayk/Projects/srrw-peak-detection/turbidity_kfold_classifier_DL.ipynb#ch0000007?line=79'>80</a>\u001b[0m total, correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/clayk/Projects/srrw-peak-detection/turbidity_kfold_classifier_DL.ipynb#ch0000007?line=81'>82</a>\u001b[0m \u001b[39m# for checking correct and incorrect preds\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\clayk\\miniconda3\\envs\\srrw\\lib\\site-packages\\torch\\serialization.py:377\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/clayk/miniconda3/envs/srrw/lib/site-packages/torch/serialization.py?line=340'>341</a>\u001b[0m \u001b[39m\"\"\"save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/clayk/miniconda3/envs/srrw/lib/site-packages/torch/serialization.py?line=341'>342</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Users/clayk/miniconda3/envs/srrw/lib/site-packages/torch/serialization.py?line=342'>343</a>\u001b[0m \u001b[39mSaves an object to a disk file.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/clayk/miniconda3/envs/srrw/lib/site-packages/torch/serialization.py?line=372'>373</a>\u001b[0m \u001b[39m    >>> torch.save(x, buffer)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/clayk/miniconda3/envs/srrw/lib/site-packages/torch/serialization.py?line=373'>374</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/clayk/miniconda3/envs/srrw/lib/site-packages/torch/serialization.py?line=374'>375</a>\u001b[0m _check_dill_version(pickle_module)\n\u001b[1;32m--> <a href='file:///c%3A/Users/clayk/miniconda3/envs/srrw/lib/site-packages/torch/serialization.py?line=376'>377</a>\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    <a href='file:///c%3A/Users/clayk/miniconda3/envs/srrw/lib/site-packages/torch/serialization.py?line=377'>378</a>\u001b[0m     \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    <a href='file:///c%3A/Users/clayk/miniconda3/envs/srrw/lib/site-packages/torch/serialization.py?line=378'>379</a>\u001b[0m         \u001b[39mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[39mas\u001b[39;00m opened_zipfile:\n",
      "File \u001b[1;32mc:\\Users\\clayk\\miniconda3\\envs\\srrw\\lib\\site-packages\\torch\\serialization.py:231\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/clayk/miniconda3/envs/srrw/lib/site-packages/torch/serialization.py?line=228'>229</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    <a href='file:///c%3A/Users/clayk/miniconda3/envs/srrw/lib/site-packages/torch/serialization.py?line=229'>230</a>\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> <a href='file:///c%3A/Users/clayk/miniconda3/envs/srrw/lib/site-packages/torch/serialization.py?line=230'>231</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    <a href='file:///c%3A/Users/clayk/miniconda3/envs/srrw/lib/site-packages/torch/serialization.py?line=231'>232</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/clayk/miniconda3/envs/srrw/lib/site-packages/torch/serialization.py?line=232'>233</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\clayk\\miniconda3\\envs\\srrw\\lib\\site-packages\\torch\\serialization.py:212\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/clayk/miniconda3/envs/srrw/lib/site-packages/torch/serialization.py?line=210'>211</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> <a href='file:///c%3A/Users/clayk/miniconda3/envs/srrw/lib/site-packages/torch/serialization.py?line=211'>212</a>\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './results/models/turb/kfold/may-9-model-fold=0.pth'"
     ]
    }
   ],
   "source": [
    "# K-fold training\n",
    "conf_matrices = {}\n",
    "accumulated_metrics = {}\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(tss.split(dataset)):\n",
    "    print(f\"FOLD {fold}\")\n",
    "\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampler=train_subsampler,\n",
    "        collate_fn=partial(collate_fn_pad, device=device),\n",
    "    )\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampler=test_subsampler,\n",
    "        collate_fn=partial(collate_fn_pad, device=device),\n",
    "    )\n",
    "\n",
    "    # init model\n",
    "    model = ResNet1D(\n",
    "        in_channels=4,\n",
    "        base_filters=64,\n",
    "        kernel_size=16,\n",
    "        stride=2,\n",
    "        n_block=48,\n",
    "        groups=1,  # check this\n",
    "        n_classes=len(classes),\n",
    "        downsample_gap=6,\n",
    "        increasefilter_gap=12,\n",
    "        verbose=False,\n",
    "    ).to(device)\n",
    "\n",
    "    # set model to use float instead of doubles to prevent errors\n",
    "    model = model.float()\n",
    "\n",
    "    # init optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(0, EPOCHS):\n",
    "        print(f\"Starting epoch {epoch + 1}\")\n",
    "\n",
    "        current_loss = 0\n",
    "\n",
    "        # prog bar\n",
    "        prog_bar = tqdm(trainloader, desc=\"Training\", leave=False)\n",
    "        for i, data in enumerate(prog_bar):\n",
    "            x = data[0].to(device)\n",
    "            y = data[1].squeeze().to(device)\n",
    "\n",
    "            if i == len(prog_bar) - 1:\n",
    "                break\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred = model(x.float())\n",
    "            loss = criterion(pred, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print stats\n",
    "            current_loss += loss.item()\n",
    "            if i % 500 == 499:\n",
    "                print(\"Loss after mini-batch %5d: %.3f\" % (i + 1, current_loss / 500))\n",
    "                current_loss = 0.0\n",
    "\n",
    "    # completed training, now test\n",
    "    print(f\"Training for fold {fold} has completed, now testing\")\n",
    "\n",
    "    # save best params\n",
    "    save_path = f\"Anomaly_Detection/Deep_Learning/results/turb/models/kfold/model-balanced-test-fold={fold}.pth\"\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    total, correct = 0, 0\n",
    "\n",
    "    # for checking correct and incorrect preds\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    prog_bar = tqdm(testloader, desc=\"Testing\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(prog_bar):\n",
    "            x = data[0].to(device)\n",
    "            y = data[1].squeeze().to(device)\n",
    "\n",
    "            outputs = model(x.float())\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for label, prediction in zip(y, preds):\n",
    "                # convert label and prediction to current vals\n",
    "                label = le.inverse_transform([label])[0]\n",
    "                prediction = le.inverse_transform([prediction])[0]\n",
    "\n",
    "                # for confusion matrices\n",
    "                y_pred.append(prediction)\n",
    "                y_true.append(label)\n",
    "\n",
    "                if label == prediction:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "        # Print rough general accuracy\n",
    "        print(\"Accuracy for fold %d: %d %%\" % (fold, 100.0 * correct / total))\n",
    "        print(\"--------------------------------\")\n",
    "        results[fold] = 100.0 * (correct / total)\n",
    "\n",
    "        # make classification report\n",
    "        acc_report = classification_report(y_true, y_pred)\n",
    "        print(acc_report)\n",
    "\n",
    "        # get acc score\n",
    "        acc_score = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "        f1 = f1_score(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            average=\"weighted\",\n",
    "        )\n",
    "\n",
    "        precision = precision_score(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            average=\"weighted\",\n",
    "        )\n",
    "\n",
    "        # make conf matrix\n",
    "        matrix = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "\n",
    "        # save conf matrix\n",
    "        conf_matrices[fold] = copy.deepcopy(matrix)\n",
    "\n",
    "        # save accumulated metrics\n",
    "        accumulated_metrics[fold] = {\n",
    "            \"f1\": f1,\n",
    "            \"acc\": acc_score,\n",
    "            \"ba\": bal_acc,\n",
    "            \"precision\": precision,\n",
    "        }\n",
    "\n",
    "# Print fold results\n",
    "print(\"\\n\")\n",
    "print(f\"K-FOLD CROSS VALIDATION RESULTS FOR {SPLITS} FOLDS\")\n",
    "print(\"--------------------------------\")\n",
    "sum = 0.0\n",
    "for key, value in results.items():\n",
    "    print(f\"Fold {key}: {value} %\")\n",
    "    sum += value\n",
    "print(f\"Average: {sum/len(results.items())} %\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save accumulated metrics\n",
    "mean_f1 = 0\n",
    "mean_ba = 0\n",
    "mean_precision = 0\n",
    "mean_acc = 0\n",
    "\n",
    "for key in accumulated_metrics:\n",
    "    metrics = accumulated_metrics[key]\n",
    "\n",
    "    mean_f1 += metrics[\"f1\"]\n",
    "    mean_ba += metrics[\"ba\"]\n",
    "    mean_precision += metrics[\"precision\"]\n",
    "    mean_acc += metrics[\"acc\"]\n",
    "\n",
    "print(\"Mean Test F1: \", mean_f1 / len(accumulated_metrics))\n",
    "print(\"Mean Test BA: \", mean_ba / len(accumulated_metrics))\n",
    "print(\"Mean Test Acc: \", mean_acc / len(accumulated_metrics))\n",
    "print(\"Mean Test Precision: \", mean_precision / len(accumulated_metrics))\n",
    "\n",
    "# make mean confusion matrix\n",
    "mean_cfmx = np.zeros((len(classes), len(classes)))\n",
    "for key in conf_matrices.keys():\n",
    "    mean_cfmx += conf_matrices[key]\n",
    "\n",
    "mean_cfmx = mean_cfmx / len(conf_matrices)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(label=\"Turbidity Peak Detection Ratio Confusion Matrix KFold\")\n",
    "\n",
    "sn.set(font_scale=1.5)\n",
    "\n",
    "plot = sn.heatmap(\n",
    "    pd.DataFrame(\n",
    "        mean_cfmx.astype(\"float\") / mean_cfmx.sum(axis=1)[:, np.newaxis],\n",
    "        index=classes,\n",
    "        columns=classes,\n",
    "    ),\n",
    "    annot=True,\n",
    "    annot_kws={\"size\": 16},\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Ground Truths\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.show()\n",
    "\n",
    "plot.get_figure().savefig(\n",
    "    \"Anomaly_Detection/Deep_Learning/results/turb/graphics/may-11-conf-ratio-balanced-test-kfold.png\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(label=\"Turbidity Peak Detection Totals Confusion Matrix KFold\")\n",
    "\n",
    "sn.set(font_scale=1.5)\n",
    "\n",
    "plot = sn.heatmap(\n",
    "    pd.DataFrame(\n",
    "        mean_cfmx,\n",
    "        index=classes,\n",
    "        columns=classes,\n",
    "    ),\n",
    "    annot=True,\n",
    "    annot_kws={\"size\": 16},\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Ground Truths\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.show()\n",
    "\n",
    "plot.get_figure().savefig(\n",
    "    \"Anomaly_Detection/Deep_Learning/results/turb/graphics/may-11-conf-totals-balanced-test-kfold.png\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "986aac83d3fed427d9eeee9b94ede52c609978b73496d2cfad058b08a24c9de9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('srrw')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
