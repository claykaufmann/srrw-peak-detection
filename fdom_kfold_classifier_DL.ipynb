{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Classification of fdom Anomaly Peaks with Resnet\n",
    "Using k-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn import preprocessing\n",
    "from Anomaly_Detection.Deep_Learning.resnet import ResNet1D\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from Anomaly_Detection.Deep_Learning.datasets import fdomDataset, collate_fn_pad, fdomAugOnlyDataset\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    ")\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util functions\n",
    "def reset_weights(model):\n",
    "    for layer in model.children():\n",
    "        if hasattr(layer, \"reset_parameters\"):\n",
    "            print(f\"reset trainable params of layer = {layer}\")\n",
    "            layer.reset_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "Adjust as needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "WINDOW_SIZE = 15  # the size of each data segment (NO LONGER RELEVANT WITH FIXED DATASET!)\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# this is the number of epochs per fold, but because data is already batched,\n",
    "# when larger than 1, training takes a long time, make sure you have cuda for fast training\n",
    "EPOCHS = 5\n",
    "\n",
    "SPLITS = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data files\n",
    "fdom_raw_data = (\n",
    "    \"Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv\"\n",
    ")\n",
    "stage_raw_data = \"Data/converted_data/julian_format/stage_10.1.11-1.1.19.csv\"\n",
    "turb_raw_data = (\n",
    "    \"Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv\"\n",
    ")\n",
    "fdom_labeled = \"Data/labeled_data/ground_truths/fDOM/fDOM_all_julian_0k-300k.csv\"\n",
    "\n",
    "# augmented files\n",
    "fdom_raw_augmented = \"Data/augmented_data/fdom/unlabeled/unlabeled_fdom.csv\"\n",
    "fdom_labeled_augmented = \"Data/augmented_data/fdom/labeled/labeled_fdom_peaks.csv\"\n",
    "turb_augmented_raw_data = \"Data/augmented_data/fdom/unlabeled/unlabeled_turb.csv\"\n",
    "stage_augmented_data_fn = \"Data/augmented_data/fdom/unlabeled/unlabeled_stage.csv\"\n",
    "\n",
    "fdom_fpt_lookup_path = \"Data/augmented_data/fdom/fpt_lookup.csv\"\n",
    "fdom_fsk_lookup_path = \"Data/augmented_data/fdom/fsk_lookup.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# get device (cpu or cuda)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset\n",
    "Use the custom dataset class to setup the dataset, classes, and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of classnames\n",
    "classes = [\"NAP\", \"FSK\", \"FPT\", \"PLP\", \"PP\", \"SKP\"]\n",
    "\n",
    "# create label encoder, encode classes into integers\n",
    "le = preprocessing.LabelEncoder()\n",
    "targets = le.fit_transform(classes)\n",
    "\n",
    "# create dataset\n",
    "# dataset = fdomDataset(\n",
    "#     le,\n",
    "#     fdom_raw_data,\n",
    "#     stage_raw_data,\n",
    "#     turb_raw_data,\n",
    "#     fdom_labeled,\n",
    "#     fdom_raw_augmented,\n",
    "#     stage_augmented_data_fn,\n",
    "#     turb_augmented_raw_data,\n",
    "#     fdom_labeled_augmented,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3775 candidates found.\n"
     ]
    }
   ],
   "source": [
    "# this is purely for class balanced TESTING if we want to do that\n",
    "dataset = fdomAugOnlyDataset(\n",
    "    le,\n",
    "    fdom_raw_augmented,\n",
    "    stage_augmented_data_fn,\n",
    "    turb_augmented_raw_data,\n",
    "    fdom_labeled_augmented,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    fpt_lookup_filename=fdom_fpt_lookup_path,\n",
    "    fsk_lookup_filename=fdom_fsk_lookup_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup KFold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "results = {}\n",
    "\n",
    "tss = TimeSeriesSplit(SPLITS)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Train the model, and print out simple accuracy results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/20 [00:00<?, ?it/s]c:\\Users\\clayk\\Projects\\srrw-peak-detection\\Anomaly_Detection\\Deep_Learning\\datasets.py:40: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  label_list = torch.tensor(label_list, dtype=torch.int64)\n",
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 5/20 [00:06<00:19,  1.30s/it]"
     ]
    }
   ],
   "source": [
    "# K-fold training\n",
    "conf_matrices = {}\n",
    "accumulated_metrics = {}\n",
    "accumulated_losses = {}\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(tss.split(dataset)):\n",
    "    print(f\"FOLD {fold}\")\n",
    "    accumulated_losses[fold] = []\n",
    "\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=BATCH_SIZE, sampler=train_subsampler,\n",
    "        collate_fn=partial(collate_fn_pad, device=device),\n",
    "    )\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=BATCH_SIZE, sampler=test_subsampler,\n",
    "        collate_fn=partial(collate_fn_pad, device=device),\n",
    "    )\n",
    "\n",
    "    # init model\n",
    "    model = ResNet1D(\n",
    "        in_channels=3,\n",
    "        base_filters=64,\n",
    "        kernel_size=16,\n",
    "        stride=2,\n",
    "        n_block=48,\n",
    "        groups=1,  # check this\n",
    "        n_classes=len(classes),\n",
    "        downsample_gap=6,\n",
    "        increasefilter_gap=12,\n",
    "        verbose=False,\n",
    "    ).to(device)\n",
    "\n",
    "    # set model to use float instead of doubles to prevent errors\n",
    "    model = model.float()\n",
    "\n",
    "    # init optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(0, EPOCHS):\n",
    "        print(f\"Starting epoch {epoch + 1}\")\n",
    "\n",
    "        current_loss = 0\n",
    "\n",
    "        # prog bar\n",
    "        prog_bar = tqdm(trainloader, desc=\"Training\", leave=False)\n",
    "        for i, data in enumerate(prog_bar):\n",
    "            x = data[0].to(device)\n",
    "            y = data[1].squeeze().to(device)\n",
    "\n",
    "            try:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                pred = model(x.float())\n",
    "                loss = criterion(pred, y)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print stats\n",
    "                current_loss += loss.item()\n",
    "                if i % 500 == 499:\n",
    "                    print(\"Loss after mini-batch %5d: %.3f\" % (i + 1, current_loss / 500))\n",
    "                    current_loss = 0.0\n",
    "                    \n",
    "                # append loss to accumulated\n",
    "                accumulated_losses[fold].append(loss.item())\n",
    "            except:\n",
    "                print('train error')\n",
    "\n",
    "    # completed training, now test\n",
    "    print(f\"Training for fold {fold} has completed, now testing\")\n",
    "\n",
    "    # save best params\n",
    "    save_path = f\"Anomaly_Detection/Deep_Learning/results/fdom/models/kfold/base-test-kfold-no-time={fold}.pth\"\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    total, correct = 0, 0\n",
    "\n",
    "    # for checking correct and incorrect preds\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    prog_bar = tqdm(testloader, desc=\"Testing\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(prog_bar):\n",
    "            x = data[0].to(device)\n",
    "            y = data[1].squeeze().to(device)\n",
    "\n",
    "            outputs = model(x.float())\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for label, prediction in zip(y, preds):\n",
    "                # convert label and prediction to current vals\n",
    "                label = label.to('cpu')\n",
    "                prediction = prediction.to('cpu')\n",
    "                label = le.inverse_transform([label])[0]\n",
    "                prediction = le.inverse_transform([prediction])[0]\n",
    "\n",
    "                # for confusion matrices\n",
    "                y_pred.append(prediction)\n",
    "                y_true.append(label)\n",
    "\n",
    "                if label == prediction:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "        # Print rough general accuracy\n",
    "        print(\"Accuracy for fold %d: %d %%\" % (fold, 100.0 * correct / total))\n",
    "        print(\"--------------------------------\")\n",
    "        results[fold] = 100.0 * (correct / total)\n",
    "\n",
    "        # make classification report\n",
    "        acc_report = classification_report(y_true, y_pred)\n",
    "        print(acc_report)\n",
    "\n",
    "        # get acc score\n",
    "        acc_score = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "        f1 = f1_score(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            average=\"macro\",\n",
    "        )\n",
    "\n",
    "        precision = precision_score(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            average=\"macro\",\n",
    "        )\n",
    "\n",
    "        # make conf matrix\n",
    "        matrix = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "\n",
    "        # save conf matrix\n",
    "        conf_matrices[fold] = copy.deepcopy(matrix)\n",
    "\n",
    "        # save accumulated metrics\n",
    "        accumulated_metrics[fold] = {\n",
    "            \"f1\": f1,\n",
    "            \"acc\": acc_score,\n",
    "            \"ba\": bal_acc,\n",
    "            \"precision\": precision,\n",
    "        }\n",
    "\n",
    "# Print fold results\n",
    "print(\"\\n\")\n",
    "print(f\"K-FOLD CROSS VALIDATION RESULTS FOR {SPLITS} FOLDS\")\n",
    "print(\"--------------------------------\")\n",
    "sum = 0.0\n",
    "for key, value in results.items():\n",
    "    print(f\"Fold {key}: {value} %\")\n",
    "    sum += value\n",
    "print(f\"Average: {sum/len(results.items())} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display training accuracy over iterations\n",
    "# NOTE: we only save the accuracy if it was an improvement, to restrict the crazy jumps as a result of random grid search\n",
    "for key in accumulated_losses:\n",
    "    accumulated_losses[key] = np.array(accumulated_losses[key])\n",
    "    plt.plot(accumulated_losses[key], label=f\"Fold {key}\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Turbidity Training accuracy over iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.savefig(\"Anomaly_Detection/Deep_Learning/results/fdom/graphics/training_loss.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save accumulated metrics\n",
    "mean_f1 = 0\n",
    "mean_ba = 0\n",
    "mean_precision = 0\n",
    "mean_acc = 0\n",
    "\n",
    "for key in accumulated_metrics:\n",
    "    metrics = accumulated_metrics[key]\n",
    "\n",
    "    mean_f1 += metrics[\"f1\"]\n",
    "    mean_ba += metrics[\"ba\"]\n",
    "    mean_precision += metrics[\"precision\"]\n",
    "    mean_acc += metrics[\"acc\"]\n",
    "\n",
    "print(\"Mean Test F1: \", mean_f1 / len(accumulated_metrics))\n",
    "print(\"Mean Test BA: \", mean_ba / len(accumulated_metrics))\n",
    "print(\"Mean Test Acc: \", mean_acc / len(accumulated_metrics))\n",
    "print(\"Mean Test Precision: \", mean_precision / len(accumulated_metrics))\n",
    "\n",
    "# make mean confusion matrix\n",
    "mean_cfmx = np.zeros((len(classes), len(classes)))\n",
    "for key in conf_matrices.keys():\n",
    "    mean_cfmx += conf_matrices[key]\n",
    "\n",
    "mean_cfmx = mean_cfmx / len(conf_matrices)\n",
    "\n",
    "plt.xlabel(\"Predictions\")\n",
    "plt.ylabel(\"Ground Truths\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(label=\"fDOM Peak Detection Confusion Matrix\")\n",
    "\n",
    "sn.set(font_scale=1.5)\n",
    "\n",
    "group_counts = [\"{0:0.0f}\\n\".format(value) for value in mean_cfmx.flatten()]\n",
    "percentages = (mean_cfmx / mean_cfmx.sum(axis=1)[:, np.newaxis]).flatten()\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in percentages]\n",
    "\n",
    "box_labels = [f\"{v2}{v3}\".strip() for v2, v3 in zip(group_counts,group_percentages)]\n",
    "box_labels = np.asarray(box_labels).reshape(mean_cfmx.shape[0],mean_cfmx.shape[1])\n",
    "\n",
    "plot = sn.heatmap(\n",
    "    mean_cfmx.astype(\"float\"),\n",
    "    annot=box_labels,\n",
    "    xticklabels=classes,\n",
    "    yticklabels=classes,\n",
    "    fmt=\"\",\n",
    "    cbar=False,\n",
    "    annot_kws ={\"size\": 16},\n",
    ")\n",
    "\n",
    "plot.patch.set_facecolor('xkcd:white')\n",
    "\n",
    "plt.xlabel(\"Predictions\")\n",
    "plt.ylabel(\"Ground Truths\")\n",
    "plt.show()\n",
    "\n",
    "plot.get_figure().savefig(\n",
    "    \"Anomaly_Detection/Deep_Learning/results/fdom/graphics/base-test-kfold.png\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('srrw')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ed753961bdc37ee89b4275051722ceb8ec0b57b8793db9d189305c313070a7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
