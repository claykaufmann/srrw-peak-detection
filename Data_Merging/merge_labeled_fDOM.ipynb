{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fDOM Data Merging\n",
    "This file's main goal is to merge all labeled timeline data into a single source to allow for easier data augmentation, as well a classifier that can detect all types of anomalies in one, rather than one classifier for each one. This specific file merges fDOM labeled data into a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are helpers for the rest of the merging process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labeled_dataset(filename):\n",
    "    \"\"\" Read in labeled data and return the read in data \"\"\"\n",
    "    with open(filename, 'r', newline='') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        next(reader)\n",
    "        truths = [[float(row[0]), float(row[1]), row[2], int(row[3])] for row in reader] \n",
    "        f.close()  \n",
    "\n",
    "    return truths \n",
    "\n",
    "def print_entire_df(dataframe):\n",
    "    \"\"\"Print out the entire contents of a dataframe, useful if you need to see differences (WARNING: ENTIRE OUTPUT WILL GO INTO GITHUB IF FILE COMMITTED WITH CELL OUTPUT\"\"\"\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        print(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in all of the datasets:\n",
    "fDOM_PLP_path = '../Data/labeled_data/ground_truths/fDOM/fDOM_PLP/julian_time/fDOM_PLP_0k-300k.csv'\n",
    "fDOM_SKP_path = '../Data/labeled_data/ground_truths/fDOM/fDOM_SKP/julian_time/fDOM_SKP_0k-300k.csv'\n",
    "fDOM_PP_path = '../Data/labeled_data/ground_truths/fDOM/fDOM_PP/julian_time/fDOM_PP_0k-300k.csv'\n",
    "\n",
    "# load in dataset from original function (MIGHT BE UNNECESSARY)\n",
    "fDOM_PLP_truths = load_labeled_dataset(fDOM_PLP_path)\n",
    "fDOM_SKP_truths = load_labeled_dataset(fDOM_SKP_path)\n",
    "fDOM_PP_truths = load_labeled_dataset(fDOM_PP_path)\n",
    "\n",
    "# Load in dataframes\n",
    "fDOM_PLP_df = pd.read_csv(fDOM_PLP_path)\n",
    "fDOM_SKP_df = pd.read_csv(fDOM_SKP_path)\n",
    "fDOM_PP_df = pd.read_csv(fDOM_PP_path)\n",
    "\n",
    "# update indices to use timestamp\n",
    "fDOM_PLP_df.set_index('timestamp_of_peak', inplace=True)\n",
    "fDOM_SKP_df.set_index('timestamp_of_peak', inplace=True)\n",
    "fDOM_PP_df.set_index('timestamp_of_peak', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize these dataframes\n",
    "print(\"PLP Head:\")\n",
    "print(fDOM_PLP_df.head())\n",
    "\n",
    "print(\"\\nSKP Head:\")\n",
    "print(fDOM_SKP_df.head())\n",
    "\n",
    "print(\"\\nPP Head:\")\n",
    "print(fDOM_PP_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peak Precendence\n",
    "The following code block sets the order precendence of peaks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET PRECENDENCE OF PEAKS\n",
    "# skyrocketing <- phantom <- plummeting\n",
    "TOP = fDOM_SKP_df\n",
    "TOP_ACRO = \"SKP\"\n",
    "TOP_NO_ACRO = \"NSKP\"\n",
    "\n",
    "SECOND = fDOM_PP_df\n",
    "SECOND_ACRO = \"PP\"\n",
    "SECOND_NO_ACRO = \"NPP\"\n",
    "\n",
    "THIRD = fDOM_PLP_df\n",
    "THIRD_ACRO = \"PLP\"\n",
    "THIRD_NO_ACRO = \"NPLP\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Data\n",
    "We concat all values into a single dataframe, stable sort by timestamp, and then drop all duplicates.\n",
    "Following this, we then rename all labels starting with N to be NAP (not anomaly peaks)\n",
    "\n",
    "Using the stable sorting method keeps the indices in the correct order, so when we drop duplicates our peak precendence is saved should there be any overlapping ones.\n",
    "Following t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all three dataframes\n",
    "df = pd.concat([TOP, SECOND, THIRD])\n",
    "\n",
    "# sort values\n",
    "df = df.sort_values(by=['timestamp_of_peak'], kind='stable')\n",
    "\n",
    "# remove duplicates\n",
    "df = df[~df.index.duplicated(keep='first')]\n",
    "\n",
    "# rename all labels that start with N to be NAP using regex\n",
    "final_data = df.replace(to_replace='N(.*)', value=\"NAP\", regex=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output to CSV\n",
    "The following codeblock exports the newly created timeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here..."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f14ca78382e4dc227f1e51b680612414c34712d66eddea25f1c0d0c728b236b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('anomaly-detection': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
