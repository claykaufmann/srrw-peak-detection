{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Peak Detection in fDOM\n",
    "\n",
    "This file combines all of the fDOM detection scripts into a singular classifier, that detects all peak types. On top of this, it also leverages the augmented data created previously.\n",
    "\n",
    "## Structure\n",
    "\n",
    "The core structure of the project is to have all individual classifiers running, and then when one detects a peak, it alerts the overall classifier \"manager\", which then takes note of the peak that a classifier has detected as an anomaly peak.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, \"../\")\n",
    "import Tools.data_processing as dp\n",
    "import Tools.data_movement as dm\n",
    "from get_cands import get_all_cands_fDOM, get_all_truths_fDOM\n",
    "\n",
    "# import classifiers\n",
    "from fdom_classifiers.fDOM_PLP import fDOM_PLP_Classifier\n",
    "\n",
    "# TODO: uncomment these when the classes are written\n",
    "# from fdom_classifiers.fDOM_FPT import fDOM_FPT_Classifier\n",
    "# from fdom_classifiers.fDOM_FSK import fDOM_FSK_Classifier\n",
    "# from fdom_classifiers.fDOM_PP import fDOM_PP_Classifier\n",
    "# from fdom_classifiers.fDOM_SKP import fDOM_SKP_Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters and helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATIONS = 7000\n",
    "NUM_SPLITS = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: REMOVE THIS WHEN FINISHED MODIFYING\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "fDOM_data = dm.read_in_preprocessed_timeseries(\n",
    "    \"../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv\"\n",
    ")\n",
    "stage_data = dm.read_in_preprocessed_timeseries(\n",
    "    \"../Data/converted_data/julian_format/stage_10.1.11-1.1.19.csv\"\n",
    ")\n",
    "turb_data = dm.read_in_preprocessed_timeseries(\n",
    "    \"../Data/converted_data/julian_format/turbidity_raw_10.1.2011_9.4.2020.csv\"\n",
    ")\n",
    "stage_data = dp.align_stage_to_fDOM(fDOM_data, stage_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Candidates and truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.45604250e+06 2.34162586e+01]\n",
      " [2.45604251e+06 2.33517649e+01]\n",
      " [2.45604252e+06 2.31767076e+01]\n",
      " ...\n",
      " [2.45848448e+06 2.05127300e+01]\n",
      " [2.45848449e+06 2.04898200e+01]\n",
      " [2.45848450e+06 2.05661800e+01]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/claykaufmann/Projects/srrw-anomaly-detection/Multiclass_Detection/../Tools/get_candidates.py:226: PeakPropertyWarning: some peaks have a prominence of 0\n",
      "  peaks, props = find_peaks(\n",
      "/Users/claykaufmann/Projects/srrw-anomaly-detection/Multiclass_Detection/../Tools/get_candidates.py:342: PeakPropertyWarning: some peaks have a prominence of 0\n",
      "  peaks, props = find_peaks(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1636, 4)\n",
      "(1636, 4)\n"
     ]
    }
   ],
   "source": [
    "# get candidates from raw data\n",
    "cands = get_all_cands_fDOM(\n",
    "    \"../Data/converted_data/julian_format/fDOM_raw_10.1.2011-9.4.2020.csv\",\n",
    "    \"../Data/labeled_data/ground_truths/fDOM/fDOM_all_julian_0k-300k.csv\",\n",
    ")\n",
    "\n",
    "print(cands.shape)\n",
    "\n",
    "# get truths from raw data\n",
    "truths = get_all_truths_fDOM(\"../Data/labeled_data/ground_truths/fDOM/fDOM_all_julian_0k-300k.csv\")\n",
    "\n",
    "print(truths.shape)\n",
    "\n",
    "# assert they are the same size\n",
    "assert truths.shape == cands.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136, 4)\n",
      "(136, 4)\n"
     ]
    }
   ],
   "source": [
    "# get candidates from augmented data\n",
    "cands_augmented = get_all_cands_fDOM(\n",
    "    \"../Data/augmented_data/fdom/unlabeled/unlabeled_fdom.csv\",\n",
    "    \"../Data/augmented_data/fdom/labeled/labeled_fdom_peaks.csv\",\n",
    "    True,\n",
    ")\n",
    "print(cands_augmented.shape)\n",
    "\n",
    "truths_augmented = get_all_truths_fDOM(\n",
    "    \"../Data/augmented_data/fdom/labeled/labeled_fdom_peaks.csv\", True\n",
    ")\n",
    "\n",
    "# align the missing augmented data (FPT, NFPT, FSK, NFSK, some others)\n",
    "truths_augmented = truths_augmented[truths_augmented[\"idx_of_peak\"].isin(cands_augmented[\"idx_of_peak\"])]\n",
    "\n",
    "print(truths_augmented.shape)\n",
    "\n",
    "assert truths_augmented.shape == cands_augmented.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate two candidates and truths into single list\n",
    "cands = pd.concat([cands, cands_augmented])\n",
    "truths = pd.concat([truths, truths_augmented])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert cands and truths into lists\n",
    "cands = cands.values.tolist()\n",
    "truths = truths.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_all_cands_fDOM() missing 2 required positional arguments: 'raw_fdom_data_filename' and 'truths_filename'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/claykaufmann/Projects/srrw-anomaly-detection/Multiclass_Detection/multiclass_fDOM.ipynb Cell 14'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/claykaufmann/Projects/srrw-anomaly-detection/Multiclass_Detection/multiclass_fDOM.ipynb#ch0000011?line=0'>1</a>\u001b[0m \u001b[39m# PLP Classifier needs the raw fDOM data and turb data to function correctly\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/claykaufmann/Projects/srrw-anomaly-detection/Multiclass_Detection/multiclass_fDOM.ipynb#ch0000011?line=1'>2</a>\u001b[0m \u001b[39m# TODO: add augmented data here when you do that\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/claykaufmann/Projects/srrw-anomaly-detection/Multiclass_Detection/multiclass_fDOM.ipynb#ch0000011?line=2'>3</a>\u001b[0m plp_classifer \u001b[39m=\u001b[39m fDOM_PLP_Classifier(fDOM_data, turb_data)\n",
      "File \u001b[0;32m~/Projects/srrw-anomaly-detection/Multiclass_Detection/fdom_classifiers/fDOM_PLP.py:62\u001b[0m, in \u001b[0;36mfDOM_PLP_Classifier.__init__\u001b[0;34m(self, fdom_data, turb_data, basewidth_range, prominence_range, peak_prox_bounds, turb_interference_bounds)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/claykaufmann/Projects/srrw-anomaly-detection/Multiclass_Detection/fdom_classifiers/fDOM_PLP.py?line=58'>59</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccumulated_cfmxs \u001b[39m=\u001b[39m {}\n\u001b[1;32m     <a href='file:///Users/claykaufmann/Projects/srrw-anomaly-detection/Multiclass_Detection/fdom_classifiers/fDOM_PLP.py?line=60'>61</a>\u001b[0m \u001b[39m# generate all of the close turb peaks from passed in data\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/claykaufmann/Projects/srrw-anomaly-detection/Multiclass_Detection/fdom_classifiers/fDOM_PLP.py?line=61'>62</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess_turb_interference(fdom_data, turb_data)\n",
      "File \u001b[0;32m~/Projects/srrw-anomaly-detection/Multiclass_Detection/fdom_classifiers/fDOM_PLP.py:232\u001b[0m, in \u001b[0;36mfDOM_PLP_Classifier.preprocess_turb_interference\u001b[0;34m(self, fDOM, turb)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/claykaufmann/Projects/srrw-anomaly-detection/Multiclass_Detection/fdom_classifiers/fDOM_PLP.py?line=228'>229</a>\u001b[0m turb_peaks, _ \u001b[39m=\u001b[39m get_candidates(turb, turb_peak_params)\n\u001b[1;32m    <a href='file:///Users/claykaufmann/Projects/srrw-anomaly-detection/Multiclass_Detection/fdom_classifiers/fDOM_PLP.py?line=230'>231</a>\u001b[0m \u001b[39m# get all fdom candidates, and convert them to a single index list to help find turbidity adjacent peaks\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/claykaufmann/Projects/srrw-anomaly-detection/Multiclass_Detection/fdom_classifiers/fDOM_PLP.py?line=231'>232</a>\u001b[0m fdom_cands \u001b[39m=\u001b[39m get_all_cands_fDOM()\n\u001b[1;32m    <a href='file:///Users/claykaufmann/Projects/srrw-anomaly-detection/Multiclass_Detection/fdom_classifiers/fDOM_PLP.py?line=232'>233</a>\u001b[0m \u001b[39mdel\u001b[39;00m fdom_cands[\u001b[39m\"\u001b[39m\u001b[39mleft_base\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    <a href='file:///Users/claykaufmann/Projects/srrw-anomaly-detection/Multiclass_Detection/fdom_classifiers/fDOM_PLP.py?line=233'>234</a>\u001b[0m \u001b[39mdel\u001b[39;00m fdom_cands[\u001b[39m\"\u001b[39m\u001b[39mright_base\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: get_all_cands_fDOM() missing 2 required positional arguments: 'raw_fdom_data_filename' and 'truths_filename'"
     ]
    }
   ],
   "source": [
    "# PLP Classifier needs the raw fDOM data and turb data to function correctly\n",
    "# TODO: add augmented data here when you do that\n",
    "plp_classifer = fDOM_PLP_Classifier(fDOM_data, turb_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data into testing and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "train_test_split_indices = TimeSeriesSplit(NUM_SPLITS).split(cands)\n",
    "\n",
    "overall_start = datetime.datetime.now()\n",
    "\n",
    "split = 1\n",
    "divide_by_zero_errs = 0\n",
    "\n",
    "for train_val_indices, test_indices in train_test_split_indices:\n",
    "    X_train, y_train = [cands[i] for i in train_val_indices], [truths[i] for i in train_val_indices]\n",
    "    X_test, y_test = [cands[i] for i in test_indices], [truths[i] for i in test_indices]\n",
    "\n",
    "    max_fold_metric = 0\n",
    "    max_result = None\n",
    "    print(\"\\nSplit: \", split)\n",
    "\n",
    "    split_start = datetime.datetime.now()\n",
    "\n",
    "    # TODO: check on these two lines, unsure what they really do\n",
    "    num_pos_test= len(list(filter(lambda x: x[2] != \"NAP\", y_test)))\n",
    "    num_pos_train= len(list(filter(lambda x: x[2] != \"NAP\", y_train)))\n",
    "\n",
    "    print(f'Num Pos in Test: {num_pos_test}')\n",
    "    print(f'Num Pos in Train: {num_pos_train}')\n",
    "\n",
    "    if num_pos_test >= 1 and num_pos_train >= 1:\n",
    "\n",
    "        # main training loop\n",
    "        for iteration in range(ITERATIONS):\n",
    "            # start the iteration for each classifier (resets predictions, generates params)\n",
    "            plp_classifer.start_iteration()\n",
    "\n",
    "            # iterate over list of peaks\n",
    "            for i, peak in enumerate(X_train):\n",
    "                plp_result = plp_classifer.classify_sample(i, peak)\n",
    "                skp_result = skp_classifier.classify_sample(i, peak)\n",
    "\n",
    "            # test classifiers\n",
    "            plp_classifer.test_results(truths, iteration, ITERATIONS)\n",
    "\n",
    "        # increment split\n",
    "        split += 1\n",
    "\n",
    "# print a newline char for better display\n",
    "print(\"\\n\")\n",
    "\n",
    "print(plp_classifer.best_acc)\n",
    "print(plp_classifer.best_f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ed753961bdc37ee89b4275051722ceb8ec0b57b8793db9d189305c313070a7d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('srrw')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
